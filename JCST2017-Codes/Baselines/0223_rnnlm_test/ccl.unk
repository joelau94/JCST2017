0 <rand>
1 it <rand>
2 it shows <rand>
3 it shows that <rand>
4 it shows that the <rand>
5 it shows that the performance <rand>
6 it shows that the performance of <rand>
7 it shows that the performance of models <rand>
8 it shows that the performance of models that <rand>
9 it shows that the performance of models that consider <rand>
10 it shows that the performance of models that consider the <rand>
11 it shows that the performance of models that consider the relation <rand>
12 it shows that the performance of models that consider the relation between <rand>
13 it shows that the performance of models that consider the relation between labels <rand>
14 it shows that the performance of models that consider the relation between labels is <rand>
15 it shows that the performance of models that consider the relation between labels is better <rand>
16 it shows that the performance of models that consider the relation between labels is better than <rand>
17 it shows that the performance of models that consider the relation between labels is better than that <rand>
18 it shows that the performance of models that consider the relation between labels is better than that without <rand>
19 it shows that the performance of models that consider the relation between labels is better than that without considering <rand>
20 it shows that the performance of models that consider the relation between labels is better than that without considering the <rand>
21 it shows that the performance of models that consider the relation between labels is better than that without considering the relations <rand>
22 it shows that the performance of models that consider the relation between labels is better than that without considering the relations between <rand>
23 it shows that the performance of models that consider the relation between labels is better than that without considering the relations between labels <rand>
24 <rand>
25 furthermore <rand>
26 furthermore , <rand>
27 furthermore , we <rand>
28 furthermore , we can <rand>
29 furthermore , we can also <rand>
30 furthermore , we can also see <rand>
31 furthermore , we can also see that <rand>
32 furthermore , we can also see that the <rand>
33 furthermore , we can also see that the perplexities <rand>
34 furthermore , we can also see that the perplexities of <rand>
35 furthermore , we can also see that the perplexities of ehllda <rand>
36 furthermore , we can also see that the perplexities of ehllda are <rand>
37 furthermore , we can also see that the perplexities of ehllda are lower <rand>
38 furthermore , we can also see that the perplexities of ehllda are lower than <rand>
39 furthermore , we can also see that the perplexities of ehllda are lower than that <rand>
40 furthermore , we can also see that the perplexities of ehllda are lower than that of <rand>
41 furthermore , we can also see that the perplexities of ehllda are lower than that of hllda <rand>
42 furthermore , we can also see that the perplexities of ehllda are lower than that of hllda over <rand>
43 furthermore , we can also see that the perplexities of ehllda are lower than that of hllda over all <rand>
44 furthermore , we can also see that the perplexities of ehllda are lower than that of hllda over all four <rand>
45 furthermore , we can also see that the perplexities of ehllda are lower than that of hllda over all four datasets <rand>
46 <rand>
47 ehllda <rand>
48 ehllda , <rand>
49 ehllda , which <rand>
50 ehllda , which incorporated <rand>
51 ehllda , which incorporated prior <rand>
52 ehllda , which incorporated prior information <rand>
53 ehllda , which incorporated prior information of <rand>
54 ehllda , which incorporated prior information of paths <rand>
55 ehllda , which incorporated prior information of paths and <rand>
56 ehllda , which incorporated prior information of paths and relaxed <rand>
57 ehllda , which incorporated prior information of paths and relaxed the <rand>
58 ehllda , which incorporated prior information of paths and relaxed the assumption <rand>
59 ehllda , which incorporated prior information of paths and relaxed the assumption of <rand>
60 ehllda , which incorporated prior information of paths and relaxed the assumption of hllda <rand>
61 <rand>
62 this <rand>
63 this paper <rand>
64 this paper presents <rand>
65 this paper presents a <rand>
66 this paper presents a construction <rand>
67 this paper presents a construction method <rand>
68 this paper presents a construction method that <rand>
69 this paper presents a construction method that utilize <rand>
70 this paper presents a construction method that utilize transition <rand>
71 this paper presents a construction method that utilize transition words <rand>
72 this paper presents a construction method that utilize transition words and <rand>
73 this paper presents a construction method that utilize transition words and negative <rand>
74 this paper presents a construction method that utilize transition words and negative words <rand>
75 this paper presents a construction method that utilize transition words and negative words to <rand>
76 this paper presents a construction method that utilize transition words and negative words to classify <rand>
77 this paper presents a construction method that utilize transition words and negative words to classify the <rand>
78 this paper presents a construction method that utilize transition words and negative words to classify the sentiment <rand>
79 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words <rand>
80 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in <rand>
81 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus <rand>
82 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , <rand>
83 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as <rand>
84 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these <rand>
85 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these words <rand>
86 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these words can <rand>
87 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these words can reverse <rand>
88 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these words can reverse the <rand>
89 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these words can reverse the polarity <rand>
90 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these words can reverse the polarity of <rand>
91 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these words can reverse the polarity of text <rand>
92 <rand>
93 parameters <rand>
94 parameters and <rand>
95 parameters and policy <rand>
96 parameters and policy of <rand>
97 parameters and policy of pomdp-based <rand>
98 parameters and policy of pomdp-based dms <rand>
99 parameters and policy of pomdp-based dms is <rand>
100 parameters and policy of pomdp-based dms is estimated <rand>
101 parameters and policy of pomdp-based dms is estimated from <rand>
102 parameters and policy of pomdp-based dms is estimated from data <rand>
103 <rand>
104 taking <rand>
105 taking the <rand>
106 taking the teach-and-learn <rand>
107 taking the teach-and-learn task <rand>
108 taking the teach-and-learn task for <rand>
109 taking the teach-and-learn task for example <rand>
110 taking the teach-and-learn task for example , <rand>
111 taking the teach-and-learn task for example , even <rand>
112 taking the teach-and-learn task for example , even if <rand>
113 taking the teach-and-learn task for example , even if objects <rand>
114 taking the teach-and-learn task for example , even if objects mentioned <rand>
115 taking the teach-and-learn task for example , even if objects mentioned in <rand>
116 taking the teach-and-learn task for example , even if objects mentioned in this <rand>
117 taking the teach-and-learn task for example , even if objects mentioned in this task <rand>
118 taking the teach-and-learn task for example , even if objects mentioned in this task have <rand>
119 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized <rand>
120 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots <rand>
121 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , <rand>
122 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the <rand>
123 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible <rand>
124 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values <rand>
125 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of <rand>
126 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots <rand>
127 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots are <rand>
128 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots are infinite <rand>
129 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots are infinite because <rand>
130 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots are infinite because of <rand>
131 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots are infinite because of infinity <rand>
132 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots are infinite because of infinity of <rand>
133 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots are infinite because of infinity of objects <rand>
134 <rand>
135 in <rand>
136 in this <rand>
137 in this way <rand>
138 in this way , <rand>
139 in this way , dm <rand>
140 in this way , dm obtains <rand>
141 in this way , dm obtains more <rand>
142 in this way , dm obtains more information <rand>
143 in this way , dm obtains more information with <rand>
144 in this way , dm obtains more information with which <rand>
145 in this way , dm obtains more information with which it <rand>
146 in this way , dm obtains more information with which it can <rand>
147 in this way , dm obtains more information with which it can guide <rand>
148 in this way , dm obtains more information with which it can guide the <rand>
149 in this way , dm obtains more information with which it can guide the dialogue <rand>
150 in this way , dm obtains more information with which it can guide the dialogue in <rand>
151 in this way , dm obtains more information with which it can guide the dialogue in a <rand>
152 in this way , dm obtains more information with which it can guide the dialogue in a more <rand>
153 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate <rand>
154 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction <rand>
155 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when <rand>
156 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when 1-best <rand>
157 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when 1-best result <rand>
158 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when 1-best result of <rand>
159 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when 1-best result of speech <rand>
160 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when 1-best result of speech recognition <rand>
161 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when 1-best result of speech recognition is <rand>
162 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when 1-best result of speech recognition is inaccurate <rand>
163 <rand>
164 in <rand>
165 in slot-filling <rand>
166 in slot-filling tasks <rand>
167 in slot-filling tasks based <rand>
168 in slot-filling tasks based on <rand>
169 in slot-filling tasks based on sds-pomdp <rand>
170 in slot-filling tasks based on sds-pomdp , <rand>
171 in slot-filling tasks based on sds-pomdp , set <rand>
172 in slot-filling tasks based on sds-pomdp , set of <rand>
173 in slot-filling tasks based on sds-pomdp , set of user <rand>
174 in slot-filling tasks based on sds-pomdp , set of user goals <rand>
175 in slot-filling tasks based on sds-pomdp , set of user goals su <rand>
176 in slot-filling tasks based on sds-pomdp , set of user goals su is <rand>
177 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian <rand>
178 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product <rand>
179 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of <rand>
180 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all <rand>
181 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots <rand>
182 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots , <rand>
183 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots , which <rand>
184 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots , which needs <rand>
185 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots , which needs to <rand>
186 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots , which needs to be <rand>
187 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots , which needs to be defined <rand>
188 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots , which needs to be defined in <rand>
189 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots , which needs to be defined in advance <rand>
190 <rand>
191 system <rand>
192 system performs <rand>
193 system performs language <rand>
194 system performs language understanding <rand>
195 system performs language understanding when <rand>
196 system performs language understanding when user <rand>
197 system performs language understanding when user inputs <rand>
198 system performs language understanding when user inputs an <rand>
199 system performs language understanding when user inputs an utterance <rand>
200 system performs language understanding when user inputs an utterance , <rand>
201 system performs language understanding when user inputs an utterance , which <rand>
202 system performs language understanding when user inputs an utterance , which includes <rand>
203 system performs language understanding when user inputs an utterance , which includes intention <rand>
204 system performs language understanding when user inputs an utterance , which includes intention recognition <rand>
205 system performs language understanding when user inputs an utterance , which includes intention recognition and <rand>
206 system performs language understanding when user inputs an utterance , which includes intention recognition and slot <rand>
207 system performs language understanding when user inputs an utterance , which includes intention recognition and slot value <rand>
208 system performs language understanding when user inputs an utterance , which includes intention recognition and slot value extraction <rand>
209 <rand>
210 in <rand>
211 in algorithm <rand>
212 in algorithm 1 <rand>
213 in algorithm 1 , <rand>
214 in algorithm 1 , we <rand>
215 in algorithm 1 , we give <rand>
216 in algorithm 1 , we give out <rand>
217 in algorithm 1 , we give out algorithm <rand>
218 in algorithm 1 , we give out algorithm for <rand>
219 in algorithm 1 , we give out algorithm for the <rand>
220 in algorithm 1 , we give out algorithm for the improved <rand>
221 in algorithm 1 , we give out algorithm for the improved sdspomdp <rand>
222 in algorithm 1 , we give out algorithm for the improved sdspomdp , <rand>
223 in algorithm 1 , we give out algorithm for the improved sdspomdp , which <rand>
224 in algorithm 1 , we give out algorithm for the improved sdspomdp , which is <rand>
225 in algorithm 1 , we give out algorithm for the improved sdspomdp , which is consistent <rand>
226 in algorithm 1 , we give out algorithm for the improved sdspomdp , which is consistent to <rand>
227 in algorithm 1 , we give out algorithm for the improved sdspomdp , which is consistent to algorithm <rand>
228 in algorithm 1 , we give out algorithm for the improved sdspomdp , which is consistent to algorithm for <rand>
229 in algorithm 1 , we give out algorithm for the improved sdspomdp , which is consistent to algorithm for sds-pomdp <rand>
230 <rand>
231 our <rand>
232 our modification <rand>
233 our modification to <rand>
234 our modification to observation <rand>
235 our modification to observation model <rand>
236 our modification to observation model is <rand>
237 our modification to observation model is mainly <rand>
238 our modification to observation model is mainly reflected <rand>
239 our modification to observation model is mainly reflected in <rand>
240 our modification to observation model is mainly reflected in line <rand>
241 our modification to observation model is mainly reflected in line 6 <rand>
242 our modification to observation model is mainly reflected in line 6 and <rand>
243 our modification to observation model is mainly reflected in line 6 and line <rand>
244 our modification to observation model is mainly reflected in line 6 and line 17 <rand>
245 our modification to observation model is mainly reflected in line 6 and line 17 to <rand>
246 our modification to observation model is mainly reflected in line 6 and line 17 to 20 <rand>
247 <rand>
248 the <rand>
249 the procedure <rand>
250 the procedure of <rand>
251 the procedure of dynamic <rand>
252 the procedure of dynamic binding <rand>
253 the procedure of dynamic binding is <rand>
254 the procedure of dynamic binding is shown <rand>
255 the procedure of dynamic binding is shown between <rand>
256 the procedure of dynamic binding is shown between lines <rand>
257 the procedure of dynamic binding is shown between lines 7 <rand>
258 the procedure of dynamic binding is shown between lines 7 to <rand>
259 the procedure of dynamic binding is shown between lines 7 to 16 <rand>
260 <rand>
261 results <rand>
262 results of <rand>
263 results of experiments <rand>
264 results of experiments indicate <rand>
265 results of experiments indicate that <rand>
266 results of experiments indicate that our <rand>
267 results of experiments indicate that our method <rand>
268 results of experiments indicate that our method can <rand>
269 results of experiments indicate that our method can effectively <rand>
270 results of experiments indicate that our method can effectively overcome <rand>
271 results of experiments indicate that our method can effectively overcome the <rand>
272 results of experiments indicate that our method can effectively overcome the infinite <rand>
273 results of experiments indicate that our method can effectively overcome the infinite teaching <rand>
274 results of experiments indicate that our method can effectively overcome the infinite teaching objects <rand>
275 results of experiments indicate that our method can effectively overcome the infinite teaching objects problem <rand>
276 results of experiments indicate that our method can effectively overcome the infinite teaching objects problem in <rand>
277 results of experiments indicate that our method can effectively overcome the infinite teaching objects problem in the <rand>
278 results of experiments indicate that our method can effectively overcome the infinite teaching objects problem in the teach-andlearn <rand>
279 results of experiments indicate that our method can effectively overcome the infinite teaching objects problem in the teach-andlearn task <rand>
280 <rand>
281 mongolian <rand>
282 mongolian is <rand>
283 mongolian is one <rand>
284 mongolian is one of <rand>
285 mongolian is one of the <rand>
286 mongolian is one of the less <rand>
287 mongolian is one of the less studied <rand>
288 mongolian is one of the less studied languages <rand>
289 mongolian is one of the less studied languages for <rand>
290 mongolian is one of the less studied languages for speech <rand>
291 mongolian is one of the less studied languages for speech recognition <rand>
292 <rand>
293 and <rand>
294 and the <rand>
295 and the feature <rand>
296 and the feature is <rand>
297 and the feature is same <rand>
298 and the feature is same as <rand>
299 and the feature is same as the <rand>
300 and the feature is same as the mono1 <rand>
301 <rand>
302 first <rand>
303 first , <rand>
304 first , according <rand>
305 first , according to <rand>
306 first , according to how <rand>
307 first , according to how two <rand>
308 first , according to how two morpheme <rand>
309 first , according to how two morpheme meanings <rand>
310 first , according to how two morpheme meanings turn <rand>
311 first , according to how two morpheme meanings turn into <rand>
312 first , according to how two morpheme meanings turn into word <rand>
313 first , according to how two morpheme meanings turn into word meanings <rand>
314 first , according to how two morpheme meanings turn into word meanings by <rand>
315 first , according to how two morpheme meanings turn into word meanings by metonymy <rand>
316 first , according to how two morpheme meanings turn into word meanings by metonymy or <rand>
317 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor <rand>
318 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , <rand>
319 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the <rand>
320 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the undirected <rand>
321 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the undirected word <rand>
322 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the undirected word was <rand>
323 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the undirected word was classified <rand>
324 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the undirected word was classified into <rand>
325 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the undirected word was classified into 8 <rand>
326 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the undirected word was classified into 8 categories <rand>
327 <rand>
328 digital <rand>
329 digital libraries <rand>
330 digital libraries suffer <rand>
331 digital libraries suffer from <rand>
332 digital libraries suffer from the <rand>
333 digital libraries suffer from the overload <rand>
334 digital libraries suffer from the overload problem <rand>
335 digital libraries suffer from the overload problem , <rand>
336 digital libraries suffer from the overload problem , which <rand>
337 digital libraries suffer from the overload problem , which makes <rand>
338 digital libraries suffer from the overload problem , which makes the <rand>
339 digital libraries suffer from the overload problem , which makes the researchers <rand>
340 digital libraries suffer from the overload problem , which makes the researchers have <rand>
341 digital libraries suffer from the overload problem , which makes the researchers have to <rand>
342 digital libraries suffer from the overload problem , which makes the researchers have to spend <rand>
343 digital libraries suffer from the overload problem , which makes the researchers have to spend much <rand>
344 digital libraries suffer from the overload problem , which makes the researchers have to spend much time <rand>
345 digital libraries suffer from the overload problem , which makes the researchers have to spend much time to <rand>
346 digital libraries suffer from the overload problem , which makes the researchers have to spend much time to find <rand>
347 digital libraries suffer from the overload problem , which makes the researchers have to spend much time to find relevant <rand>
348 digital libraries suffer from the overload problem , which makes the researchers have to spend much time to find relevant papers <rand>
349 <rand>
350 previous <rand>
351 previous paper <rand>
352 previous paper recommendation <rand>
353 previous paper recommendation methods <rand>
354 previous paper recommendation methods are <rand>
355 previous paper recommendation methods are either <rand>
356 previous paper recommendation methods are either citation-based <rand>
357 previous paper recommendation methods are either citation-based or <rand>
358 previous paper recommendation methods are either citation-based or contentbased <rand>
359 <rand>
360 co-coupling <rand>
361 co-coupling occurred <rand>
362 co-coupling occurred when <rand>
363 co-coupling occurred when two <rand>
364 co-coupling occurred when two papers <rand>
365 co-coupling occurred when two papers referred <rand>
366 co-coupling occurred when two papers referred a <rand>
367 co-coupling occurred when two papers referred a common <rand>
368 co-coupling occurred when two papers referred a common third <rand>
369 co-coupling occurred when two papers referred a common third paper <rand>
370 co-coupling occurred when two papers referred a common third paper , <rand>
371 co-coupling occurred when two papers referred a common third paper , then <rand>
372 co-coupling occurred when two papers referred a common third paper , then that <rand>
373 co-coupling occurred when two papers referred a common third paper , then that two <rand>
374 co-coupling occurred when two papers referred a common third paper , then that two papers <rand>
375 co-coupling occurred when two papers referred a common third paper , then that two papers were <rand>
376 co-coupling occurred when two papers referred a common third paper , then that two papers were co-coupling <rand>
377 <rand>
378 the <rand>
379 the co-coupling <rand>
380 the co-coupling strength <rand>
381 the co-coupling strength of <rand>
382 the co-coupling strength of two <rand>
383 the co-coupling strength of two given <rand>
384 the co-coupling strength of two given papers <rand>
385 the co-coupling strength of two given papers was <rand>
386 the co-coupling strength of two given papers was higher <rand>
387 the co-coupling strength of two given papers was higher if <rand>
388 the co-coupling strength of two given papers was higher if they <rand>
389 the co-coupling strength of two given papers was higher if they referred <rand>
390 the co-coupling strength of two given papers was higher if they referred more <rand>
391 the co-coupling strength of two given papers was higher if they referred more common <rand>
392 the co-coupling strength of two given papers was higher if they referred more common papers <rand>
393 <rand>
394 then <rand>
395 then , <rand>
396 then , they <rand>
397 then , they applied <rand>
398 then , they applied the <rand>
399 then , they applied the hits <rand>
400 then , they applied the hits algorithm <rand>
401 then , they applied the hits algorithm to <rand>
402 then , they applied the hits algorithm to assign <rand>
403 then , they applied the hits algorithm to assign each <rand>
404 then , they applied the hits algorithm to assign each candidate <rand>
405 then , they applied the hits algorithm to assign each candidate paper <rand>
406 then , they applied the hits algorithm to assign each candidate paper a <rand>
407 then , they applied the hits algorithm to assign each candidate paper a hub <rand>
408 then , they applied the hits algorithm to assign each candidate paper a hub score <rand>
409 then , they applied the hits algorithm to assign each candidate paper a hub score , <rand>
410 then , they applied the hits algorithm to assign each candidate paper a hub score , and <rand>
411 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally <rand>
412 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according <rand>
413 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to <rand>
414 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the <rand>
415 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub <rand>
416 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores <rand>
417 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores , <rand>
418 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores , the <rand>
419 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores , the top-n <rand>
420 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores , the top-n papers <rand>
421 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores , the top-n papers would <rand>
422 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores , the top-n papers would be <rand>
423 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores , the top-n papers would be recommended <rand>
424 <rand>
425 and <rand>
426 and if <rand>
427 and if one <rand>
428 and if one paper <rand>
429 and if one paper had <rand>
430 and if one paper had citation <rand>
431 and if one paper had citation relation <rand>
432 and if one paper had citation relation to <rand>
433 and if one paper had citation relation to another <rand>
434 and if one paper had citation relation to another paper <rand>
435 and if one paper had citation relation to another paper , <rand>
436 and if one paper had citation relation to another paper , the <rand>
437 and if one paper had citation relation to another paper , the two <rand>
438 and if one paper had citation relation to another paper , the two papers <rand>
439 and if one paper had citation relation to another paper , the two papers were <rand>
440 and if one paper had citation relation to another paper , the two papers were also <rand>
441 and if one paper had citation relation to another paper , the two papers were also considered <rand>
442 and if one paper had citation relation to another paper , the two papers were also considered similar <rand>
443 <rand>
444 the <rand>
445 the key-terms <rand>
446 the key-terms are <rand>
447 the key-terms are extracted <rand>
448 the key-terms are extracted based <rand>
449 the key-terms are extracted based on <rand>
450 the key-terms are extracted based on the <rand>
451 the key-terms are extracted based on the tfidf <rand>
452 the key-terms are extracted based on the tfidf score <rand>
453 the key-terms are extracted based on the tfidf score , <rand>
454 the key-terms are extracted based on the tfidf score , the <rand>
455 the key-terms are extracted based on the tfidf score , the weights <rand>
456 the key-terms are extracted based on the tfidf score , the weights of <rand>
457 the key-terms are extracted based on the tfidf score , the weights of these <rand>
458 the key-terms are extracted based on the tfidf score , the weights of these key-terms <rand>
459 the key-terms are extracted based on the tfidf score , the weights of these key-terms are <rand>
460 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed <rand>
461 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by <rand>
462 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet <rand>
463 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , <rand>
464 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and <rand>
465 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and if <rand>
466 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and if key-terms <rand>
467 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and if key-terms tj <rand>
468 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and if key-terms tj appears <rand>
469 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and if key-terms tj appears in <rand>
470 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and if key-terms tj appears in paper <rand>
471 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and if key-terms tj appears in paper pi <rand>
472 <rand>
473 the <rand>
474 the main <rand>
475 the main idea <rand>
476 the main idea of <rand>
477 the main idea of graph-based <rand>
478 the main idea of graph-based semi-supervised <rand>
479 the main idea of graph-based semi-supervised classification <rand>
480 the main idea of graph-based semi-supervised classification was <rand>
481 the main idea of graph-based semi-supervised classification was that <rand>
482 the main idea of graph-based semi-supervised classification was that similar <rand>
483 the main idea of graph-based semi-supervised classification was that similar instances <rand>
484 the main idea of graph-based semi-supervised classification was that similar instances tended <rand>
485 the main idea of graph-based semi-supervised classification was that similar instances tended to <rand>
486 the main idea of graph-based semi-supervised classification was that similar instances tended to have <rand>
487 the main idea of graph-based semi-supervised classification was that similar instances tended to have similar <rand>
488 the main idea of graph-based semi-supervised classification was that similar instances tended to have similar categories <rand>
489 <rand>
490 general <rand>
491 general recommendation <rand>
492 general recommendation system <rand>
493 general recommendation system will <rand>
494 general recommendation system will recommend <rand>
495 general recommendation system will recommend some <rand>
496 general recommendation system will recommend some items <rand>
497 general recommendation system will recommend some items for <rand>
498 general recommendation system will recommend some items for the <rand>
499 general recommendation system will recommend some items for the user <rand>
500 <rand>
501 mean <rand>
502 mean reciprocal <rand>
503 mean reciprocal rank <rand>
504 mean reciprocal rank is <rand>
505 mean reciprocal rank is only <rand>
506 mean reciprocal rank is only concerned <rand>
507 mean reciprocal rank is only concerned about <rand>
508 mean reciprocal rank is only concerned about the <rand>
509 mean reciprocal rank is only concerned about the ranking <rand>
510 mean reciprocal rank is only concerned about the ranking of <rand>
511 mean reciprocal rank is only concerned about the ranking of the <rand>
512 mean reciprocal rank is only concerned about the ranking of the first <rand>
513 mean reciprocal rank is only concerned about the ranking of the first relevant <rand>
514 mean reciprocal rank is only concerned about the ranking of the first relevant term <rand>
515 mean reciprocal rank is only concerned about the ranking of the first relevant term which <rand>
516 mean reciprocal rank is only concerned about the ranking of the first relevant term which is <rand>
517 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned <rand>
518 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by <rand>
519 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the <rand>
520 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the system <rand>
521 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the system , <rand>
522 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the system , average <rand>
523 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the system , average over <rand>
524 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the system , average over all <rand>
525 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the system , average over all target <rand>
526 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the system , average over all target papers <rand>
527 <rand>
528 in <rand>
529 in order <rand>
530 in order to <rand>
531 in order to examine <rand>
532 in order to examine the <rand>
533 in order to examine the sensitive <rand>
534 in order to examine the sensitive of <rand>
535 in order to examine the sensitive of the <rand>
536 in order to examine the sensitive of the initial <rand>
537 in order to examine the sensitive of the initial weights <rand>
538 in order to examine the sensitive of the initial weights of <rand>
539 in order to examine the sensitive of the initial weights of the <rand>
540 in order to examine the sensitive of the initial weights of the edges <rand>
541 in order to examine the sensitive of the initial weights of the edges , <rand>
542 in order to examine the sensitive of the initial weights of the edges , we <rand>
543 in order to examine the sensitive of the initial weights of the edges , we do <rand>
544 in order to examine the sensitive of the initial weights of the edges , we do the <rand>
545 in order to examine the sensitive of the initial weights of the edges , we do the other <rand>
546 in order to examine the sensitive of the initial weights of the edges , we do the other experiment <rand>
547 <rand>
548 table <rand>
549 table 2 <rand>
550 table 2 is <rand>
551 table 2 is the <rand>
552 table 2 is the results <rand>
553 table 2 is the results of <rand>
554 table 2 is the results of five <rand>
555 table 2 is the results of five different <rand>
556 table 2 is the results of five different weights <rand>
557 table 2 is the results of five different weights of <rand>
558 table 2 is the results of five different weights of edges <rand>
559 table 2 is the results of five different weights of edges combinations <rand>
560 table 2 is the results of five different weights of edges combinations in <rand>
561 table 2 is the results of five different weights of edges combinations in the <rand>
562 table 2 is the results of five different weights of edges combinations in the heterogeneous <rand>
563 table 2 is the results of five different weights of edges combinations in the heterogeneous graph <rand>
564 <rand>
565 frame <rand>
566 frame semantics <rand>
567 frame semantics has <rand>
568 frame semantics has a <rand>
569 frame semantics has a complete <rand>
570 frame semantics has a complete system <rand>
571 frame semantics has a complete system , <rand>
572 frame semantics has a complete system , is <rand>
573 frame semantics has a complete system , is an <rand>
574 frame semantics has a complete system , is an effective <rand>
575 frame semantics has a complete system , is an effective knowledge <rand>
576 frame semantics has a complete system , is an effective knowledge representation <rand>
577 frame semantics has a complete system , is an effective knowledge representation with <rand>
578 frame semantics has a complete system , is an effective knowledge representation with empirical <rand>
579 frame semantics has a complete system , is an effective knowledge representation with empirical semantic <rand>
580 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties <rand>
581 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against <rand>
582 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the <rand>
583 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background <rand>
584 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of <rand>
585 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive <rand>
586 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism <rand>
587 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism , <rand>
588 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism , and <rand>
589 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism , and directly <rand>
590 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism , and directly oriented <rand>
591 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism , and directly oriented to <rand>
592 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism , and directly oriented to the <rand>
593 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism , and directly oriented to the application <rand>
594 <rand>
595 it <rand>
596 it is <rand>
597 it is of <rand>
598 it is of great <rand>
599 it is of great importance <rand>
600 it is of great importance to <rand>
601 it is of great importance to frame <rand>
602 it is of great importance to frame construction <rand>
603 it is of great importance to frame construction , <rand>
604 it is of great importance to frame construction , sentence <rand>
605 it is of great importance to frame construction , sentence annotation <rand>
606 it is of great importance to frame construction , sentence annotation and <rand>
607 it is of great importance to frame construction , sentence annotation and the <rand>
608 it is of great importance to frame construction , sentence annotation and the valence <rand>
609 it is of great importance to frame construction , sentence annotation and the valence patterns <rand>
610 it is of great importance to frame construction , sentence annotation and the valence patterns statistics <rand>
611 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of <rand>
612 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of lexical <rand>
613 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of lexical unit <rand>
614 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of lexical unit , <rand>
615 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of lexical unit , providing <rand>
616 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of lexical unit , providing precondition <rand>
617 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of lexical unit , providing precondition for <rand>
618 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of lexical unit , providing precondition for domlfsr <rand>
619 <rand>
620 the <rand>
621 the tagset <rand>
622 the tagset we <rand>
623 the tagset we used <rand>
624 the tagset we used in <rand>
625 the tagset we used in annotating <rand>
626 the tagset we used in annotating divided <rand>
627 the tagset we used in annotating divided into <rand>
628 the tagset we used in annotating divided into two <rand>
629 the tagset we used in annotating divided into two kinds <rand>
630 the tagset we used in annotating divided into two kinds of <rand>
631 the tagset we used in annotating divided into two kinds of english <rand>
632 the tagset we used in annotating divided into two kinds of english and <rand>
633 the tagset we used in annotating divided into two kinds of english and chinese <rand>
634 <rand>
635 mapping <rand>
636 mapping the <rand>
637 mapping the structure <rand>
638 mapping the structure and <rand>
639 mapping the structure and meaning <rand>
640 mapping the structure and meaning of <rand>
641 mapping the structure and meaning of language <rand>
642 mapping the structure and meaning of language has <rand>
643 mapping the structure and meaning of language has been <rand>
644 mapping the structure and meaning of language has been considered <rand>
645 mapping the structure and meaning of language has been considered as <rand>
646 mapping the structure and meaning of language has been considered as one <rand>
647 mapping the structure and meaning of language has been considered as one of <rand>
648 mapping the structure and meaning of language has been considered as one of the <rand>
649 mapping the structure and meaning of language has been considered as one of the basic <rand>
650 mapping the structure and meaning of language has been considered as one of the basic principles <rand>
651 mapping the structure and meaning of language has been considered as one of the basic principles of <rand>
652 mapping the structure and meaning of language has been considered as one of the basic principles of reearches <rand>
653 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in <rand>
654 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational <rand>
655 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics <rand>
656 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and <rand>
657 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language <rand>
658 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information <rand>
659 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing <rand>
660 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , <rand>
661 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting <rand>
662 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from <rand>
663 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the <rand>
664 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom <rand>
665 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of <rand>
666 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the <rand>
667 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language <rand>
668 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law <rand>
669 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , <rand>
670 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the <rand>
671 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching <rand>
672 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event <rand>
673 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction <rand>
674 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method <rand>
675 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has <rand>
676 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important <rand>
677 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important significance <rand>
678 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important significance for <rand>
679 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important significance for domain-oriented <rand>
680 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important significance for domain-oriented multi-language <rand>
681 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important significance for domain-oriented multi-language news <rand>
682 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important significance for domain-oriented multi-language news events <rand>
683 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important significance for domain-oriented multi-language news events extraction <rand>
684 <rand>
685 in <rand>
686 in addition <rand>
687 in addition , <rand>
688 in addition , we <rand>
689 in addition , we will <rand>
690 in addition , we will transfer <rand>
691 in addition , we will transfer above <rand>
692 in addition , we will transfer above research <rand>
693 in addition , we will transfer above research productions <rand>
694 in addition , we will transfer above research productions to <rand>
695 in addition , we will transfer above research productions to other <rand>
696 in addition , we will transfer above research productions to other suitable <rand>
697 in addition , we will transfer above research productions to other suitable oriental <rand>
698 in addition , we will transfer above research productions to other suitable oriental languages <rand>
699 in addition , we will transfer above research productions to other suitable oriental languages like <rand>
700 in addition , we will transfer above research productions to other suitable oriental languages like malay <rand>
701 in addition , we will transfer above research productions to other suitable oriental languages like malay , <rand>
702 in addition , we will transfer above research productions to other suitable oriental languages like malay , thai <rand>
703 in addition , we will transfer above research productions to other suitable oriental languages like malay , thai , <rand>
704 in addition , we will transfer above research productions to other suitable oriental languages like malay , thai , japanese <rand>
705 in addition , we will transfer above research productions to other suitable oriental languages like malay , thai , japanese , <rand>
706 in addition , we will transfer above research productions to other suitable oriental languages like malay , thai , japanese , and <rand>
707 in addition , we will transfer above research productions to other suitable oriental languages like malay , thai , japanese , and so <rand>
708 in addition , we will transfer above research productions to other suitable oriental languages like malay , thai , japanese , and so on <rand>
709 <rand>
710 by <rand>
711 by observing <rand>
712 by observing the <rand>
713 by observing the sentimental <rand>
714 by observing the sentimental trend <rand>
715 by observing the sentimental trend of <rand>
716 by observing the sentimental trend of the <rand>
717 by observing the sentimental trend of the different <rand>
718 by observing the sentimental trend of the different topics <rand>
719 by observing the sentimental trend of the different topics beneath <rand>
720 by observing the sentimental trend of the different topics beneath this <rand>
721 by observing the sentimental trend of the different topics beneath this event <rand>
722 by observing the sentimental trend of the different topics beneath this event , <rand>
723 by observing the sentimental trend of the different topics beneath this event , we <rand>
724 by observing the sentimental trend of the different topics beneath this event , we attempt <rand>
725 by observing the sentimental trend of the different topics beneath this event , we attempt to <rand>
726 by observing the sentimental trend of the different topics beneath this event , we attempt to offer <rand>
727 by observing the sentimental trend of the different topics beneath this event , we attempt to offer feasible <rand>
728 by observing the sentimental trend of the different topics beneath this event , we attempt to offer feasible suggestions <rand>
729 by observing the sentimental trend of the different topics beneath this event , we attempt to offer feasible suggestions for <rand>
730 by observing the sentimental trend of the different topics beneath this event , we attempt to offer feasible suggestions for public <rand>
731 by observing the sentimental trend of the different topics beneath this event , we attempt to offer feasible suggestions for public sentiment <rand>
732 by observing the sentimental trend of the different topics beneath this event , we attempt to offer feasible suggestions for public sentiment monitoring <rand>
733 <rand>
734 dependency <rand>
735 dependency and <rand>
736 dependency and relational <rand>
737 dependency and relational structure <rand>
738 dependency and relational structure in <rand>
739 dependency and relational structure in treebank <rand>
740 dependency and relational structure in treebank annotation <rand>
741 <rand>
742 the <rand>
743 the rules <rand>
744 the rules of <rand>
745 the rules of hedges <rand>
746 the rules of hedges scope <rand>
747 the rules of hedges scope were <rand>
748 the rules of hedges scope were mainly <rand>
749 the rules of hedges scope were mainly due <rand>
750 the rules of hedges scope were mainly due to <rand>
751 the rules of hedges scope were mainly due to the <rand>
752 the rules of hedges scope were mainly due to the different <rand>
753 the rules of hedges scope were mainly due to the different syntactic <rand>
754 the rules of hedges scope were mainly due to the different syntactic constituents <rand>
755 the rules of hedges scope were mainly due to the different syntactic constituents which <rand>
756 the rules of hedges scope were mainly due to the different syntactic constituents which caused <rand>
757 the rules of hedges scope were mainly due to the different syntactic constituents which caused by <rand>
758 the rules of hedges scope were mainly due to the different syntactic constituents which caused by its <rand>
759 the rules of hedges scope were mainly due to the different syntactic constituents which caused by its parts <rand>
760 the rules of hedges scope were mainly due to the different syntactic constituents which caused by its parts of <rand>
761 the rules of hedges scope were mainly due to the different syntactic constituents which caused by its parts of speech <rand>
762 <rand>
763 the <rand>
764 the paper <rand>
765 the paper proposed <rand>
766 the paper proposed a <rand>
767 the paper proposed a local <rand>
768 the paper proposed a local community <rand>
769 the paper proposed a local community detection <rand>
770 the paper proposed a local community detection method <rand>
771 the paper proposed a local community detection method , <rand>
772 the paper proposed a local community detection method , this <rand>
773 the paper proposed a local community detection method , this method <rand>
774 the paper proposed a local community detection method , this method does <rand>
775 the paper proposed a local community detection method , this method does not <rand>
776 the paper proposed a local community detection method , this method does not need <rand>
777 the paper proposed a local community detection method , this method does not need to <rand>
778 the paper proposed a local community detection method , this method does not need to know <rand>
779 the paper proposed a local community detection method , this method does not need to know the <rand>
780 the paper proposed a local community detection method , this method does not need to know the whole <rand>
781 the paper proposed a local community detection method , this method does not need to know the whole complex <rand>
782 the paper proposed a local community detection method , this method does not need to know the whole complex network <rand>
783 the paper proposed a local community detection method , this method does not need to know the whole complex network information <rand>
784 the paper proposed a local community detection method , this method does not need to know the whole complex network information , <rand>
785 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it <rand>
786 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just <rand>
787 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting <rand>
788 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from <rand>
789 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an <rand>
790 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial <rand>
791 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node <rand>
792 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and <rand>
793 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating <rand>
794 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the <rand>
795 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness <rand>
796 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between <rand>
797 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between the <rand>
798 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between the initial <rand>
799 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between the initial node <rand>
800 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between the initial node and <rand>
801 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between the initial node and the <rand>
802 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between the initial node and the adjacent <rand>
803 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between the initial node and the adjacent nodes <rand>
804 <rand>
805 and <rand>
806 and gradually <rand>
807 and gradually add <rand>
808 and gradually add the <rand>
809 and gradually add the adjacent <rand>
810 and gradually add the adjacent nodes <rand>
811 and gradually add the adjacent nodes to <rand>
812 and gradually add the adjacent nodes to the <rand>
813 and gradually add the adjacent nodes to the community <rand>
814 and gradually add the adjacent nodes to the community ; <rand>
815 and gradually add the adjacent nodes to the community ; finally <rand>
816 and gradually add the adjacent nodes to the community ; finally get <rand>
817 and gradually add the adjacent nodes to the community ; finally get this <rand>
818 and gradually add the adjacent nodes to the community ; finally get this node <rand>
819 and gradually add the adjacent nodes to the community ; finally get this node community <rand>
820 and gradually add the adjacent nodes to the community ; finally get this node community structure <rand>
821 <rand>
822 identifying <rand>
823 identifying the <rand>
824 identifying the role <rand>
825 identifying the role that <rand>
826 identifying the role that animals <rand>
827 identifying the role that animals play <rand>
828 identifying the role that animals play in <rand>
829 identifying the role that animals play in their <rand>
830 identifying the role that animals play in their social <rand>
831 identifying the role that animals play in their social networks <rand>
832 <rand>
833 and <rand>
834 and then <rand>
835 and then the <rand>
836 and then the authors <rand>
837 and then the authors compare <rand>
838 and then the authors compare several <rand>
839 and then the authors compare several pos <rand>
840 and then the authors compare several pos tagging <rand>
841 and then the authors compare several pos tagging methods <rand>
842 and then the authors compare several pos tagging methods , <rand>
843 and then the authors compare several pos tagging methods , the <rand>
844 and then the authors compare several pos tagging methods , the results <rand>
845 and then the authors compare several pos tagging methods , the results prove <rand>
846 and then the authors compare several pos tagging methods , the results prove that <rand>
847 and then the authors compare several pos tagging methods , the results prove that train <rand>
848 and then the authors compare several pos tagging methods , the results prove that train data <rand>
849 and then the authors compare several pos tagging methods , the results prove that train data with <rand>
850 and then the authors compare several pos tagging methods , the results prove that train data with the <rand>
851 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level <rand>
852 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation <rand>
853 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation can <rand>
854 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation can enhance <rand>
855 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation can enhance the <rand>
856 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation can enhance the effects <rand>
857 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation can enhance the effects of <rand>
858 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation can enhance the effects of pos <rand>
859 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation can enhance the effects of pos tagging <rand>
860 <rand>
861 by <rand>
862 by considering <rand>
863 by considering the <rand>
864 by considering the characteristics <rand>
865 by considering the characteristics of <rand>
866 by considering the characteristics of both <rand>
867 by considering the characteristics of both chinese <rand>
868 by considering the characteristics of both chinese language <rand>
869 by considering the characteristics of both chinese language and <rand>
870 by considering the characteristics of both chinese language and social <rand>
871 by considering the characteristics of both chinese language and social networks <rand>
872 by considering the characteristics of both chinese language and social networks , <rand>
873 by considering the characteristics of both chinese language and social networks , we <rand>
874 by considering the characteristics of both chinese language and social networks , we build <rand>
875 by considering the characteristics of both chinese language and social networks , we build a <rand>
876 by considering the characteristics of both chinese language and social networks , we build a set <rand>
877 by considering the characteristics of both chinese language and social networks , we build a set of <rand>
878 by considering the characteristics of both chinese language and social networks , we build a set of discriminating <rand>
879 by considering the characteristics of both chinese language and social networks , we build a set of discriminating features <rand>
880 by considering the characteristics of both chinese language and social networks , we build a set of discriminating features for <rand>
881 by considering the characteristics of both chinese language and social networks , we build a set of discriminating features for chinese <rand>
882 by considering the characteristics of both chinese language and social networks , we build a set of discriminating features for chinese irony <rand>
883 by considering the characteristics of both chinese language and social networks , we build a set of discriminating features for chinese irony detection <rand>
884 <rand>
885 for <rand>
886 for these <rand>
887 for these features <rand>
888 for these features , <rand>
889 for these features , information <rand>
890 for these features , information gain <rand>
891 for these features , information gain is <rand>
892 for these features , information gain is applied <rand>
893 for these features , information gain is applied to <rand>
894 for these features , information gain is applied to compare <rand>
895 for these features , information gain is applied to compare their <rand>
896 for these features , information gain is applied to compare their efficiency <rand>
897 for these features , information gain is applied to compare their efficiency , <rand>
898 for these features , information gain is applied to compare their efficiency , and <rand>
899 for these features , information gain is applied to compare their efficiency , and several <rand>
900 for these features , information gain is applied to compare their efficiency , and several classifiers <rand>
901 for these features , information gain is applied to compare their efficiency , and several classifiers are <rand>
902 for these features , information gain is applied to compare their efficiency , and several classifiers are also <rand>
903 for these features , information gain is applied to compare their efficiency , and several classifiers are also applied <rand>
904 for these features , information gain is applied to compare their efficiency , and several classifiers are also applied to <rand>
905 for these features , information gain is applied to compare their efficiency , and several classifiers are also applied to test <rand>
906 for these features , information gain is applied to compare their efficiency , and several classifiers are also applied to test their <rand>
907 for these features , information gain is applied to compare their efficiency , and several classifiers are also applied to test their stability <rand>
908 <rand>
909 in <rand>
910 in our <rand>
911 in our work <rand>
912 in our work , <rand>
913 in our work , lexical <rand>
914 in our work , lexical level <rand>
915 in our work , lexical level features <rand>
916 in our work , lexical level features include <rand>
917 in our work , lexical level features include the <rand>
918 in our work , lexical level features include the two <rand>
919 in our work , lexical level features include the two entities <rand>
920 in our work , lexical level features include the two entities , <rand>
921 in our work , lexical level features include the two entities , their <rand>
922 in our work , lexical level features include the two entities , their ner <rand>
923 in our work , lexical level features include the two entities , their ner tags <rand>
924 in our work , lexical level features include the two entities , their ner tags , <rand>
925 in our work , lexical level features include the two entities , their ner tags , and <rand>
926 in our work , lexical level features include the two entities , their ner tags , and the <rand>
927 in our work , lexical level features include the two entities , their ner tags , and the neighbor <rand>
928 in our work , lexical level features include the two entities , their ner tags , and the neighbor tokens <rand>
929 in our work , lexical level features include the two entities , their ner tags , and the neighbor tokens of <rand>
930 in our work , lexical level features include the two entities , their ner tags , and the neighbor tokens of these <rand>
931 in our work , lexical level features include the two entities , their ner tags , and the neighbor tokens of these two <rand>
932 in our work , lexical level features include the two entities , their ner tags , and the neighbor tokens of these two entities <rand>
933 <rand>
934 in <rand>
935 in our <rand>
936 in our work <rand>
937 in our work , <rand>
938 in our work , we <rand>
939 in our work , we adopt <rand>
940 in our work , we adopt neural <rand>
941 in our work , we adopt neural tensor <rand>
942 in our work , we adopt neural tensor layer <rand>
943 in our work , we adopt neural tensor layer to <rand>
944 in our work , we adopt neural tensor layer to model <rand>
945 in our work , we adopt neural tensor layer to model the <rand>
946 in our work , we adopt neural tensor layer to model the interactions <rand>
947 in our work , we adopt neural tensor layer to model the interactions of <rand>
948 in our work , we adopt neural tensor layer to model the interactions of above <rand>
949 in our work , we adopt neural tensor layer to model the interactions of above extracted <rand>
950 in our work , we adopt neural tensor layer to model the interactions of above extracted features <rand>
951 in our work , we adopt neural tensor layer to model the interactions of above extracted features in <rand>
952 in our work , we adopt neural tensor layer to model the interactions of above extracted features in a <rand>
953 in our work , we adopt neural tensor layer to model the interactions of above extracted features in a mention <rand>
954 <rand>
955 we <rand>
956 we can <rand>
957 we can think <rand>
958 we can think of <rand>
959 we can think of this <rand>
960 we can think of this procedure <rand>
961 we can think of this procedure from <rand>
962 we can think of this procedure from a <rand>
963 we can think of this procedure from a joint <rand>
964 we can think of this procedure from a joint learning <rand>
965 we can think of this procedure from a joint learning perspective <rand>
966 <rand>
967 in <rand>
968 in another <rand>
969 in another word <rand>
970 in another word , <rand>
971 in another word , the <rand>
972 in another word , the representation <rand>
973 in another word , the representation learned <rand>
974 in another word , the representation learned contains <rand>
975 in another word , the representation learned contains information <rand>
976 in another word , the representation learned contains information from <rand>
977 in another word , the representation learned contains information from all <rand>
978 in another word , the representation learned contains information from all of <rand>
979 in another word , the representation learned contains information from all of these <rand>
980 in another word , the representation learned contains information from all of these relations <rand>
981 in another word , the representation learned contains information from all of these relations , <rand>
982 in another word , the representation learned contains information from all of these relations , hence <rand>
983 in another word , the representation learned contains information from all of these relations , hence it <rand>
984 in another word , the representation learned contains information from all of these relations , hence it can <rand>
985 in another word , the representation learned contains information from all of these relations , hence it can capture <rand>
986 in another word , the representation learned contains information from all of these relations , hence it can capture the <rand>
987 in another word , the representation learned contains information from all of these relations , hence it can capture the correlations <rand>
988 in another word , the representation learned contains information from all of these relations , hence it can capture the correlations among <rand>
989 in another word , the representation learned contains information from all of these relations , hence it can capture the correlations among different <rand>
990 in another word , the representation learned contains information from all of these relations , hence it can capture the correlations among different relations <rand>
991 <rand>
992 on <rand>
993 on the <rand>
994 on the one <rand>
995 on the one hand <rand>
996 on the one hand , <rand>
997 on the one hand , comparing <rand>
998 on the one hand , comparing to <rand>
999 on the one hand , comparing to traditional <rand>
1000 on the one hand , comparing to traditional shallow <rand>
1001 on the one hand , comparing to traditional shallow models <rand>
1002 on the one hand , comparing to traditional shallow models , <rand>
1003 on the one hand , comparing to traditional shallow models , our <rand>
1004 on the one hand , comparing to traditional shallow models , our model <rand>
1005 on the one hand , comparing to traditional shallow models , our model does <rand>
1006 on the one hand , comparing to traditional shallow models , our model does not <rand>
1007 on the one hand , comparing to traditional shallow models , our model does not dependent <rand>
1008 on the one hand , comparing to traditional shallow models , our model does not dependent on <rand>
1009 on the one hand , comparing to traditional shallow models , our model does not dependent on lots <rand>
1010 on the one hand , comparing to traditional shallow models , our model does not dependent on lots of <rand>
1011 on the one hand , comparing to traditional shallow models , our model does not dependent on lots of rich <rand>
1012 on the one hand , comparing to traditional shallow models , our model does not dependent on lots of rich handdesigned <rand>
1013 on the one hand , comparing to traditional shallow models , our model does not dependent on lots of rich handdesigned features <rand>
1014 <rand>
1015 further <rand>
1016 further more <rand>
1017 further more , <rand>
1018 further more , our <rand>
1019 further more , our model <rand>
1020 further more , our model uses <rand>
1021 further more , our model uses global <rand>
1022 further more , our model uses global structure <rand>
1023 further more , our model uses global structure information <rand>
1024 further more , our model uses global structure information of <rand>
1025 further more , our model uses global structure information of parse <rand>
1026 further more , our model uses global structure information of parse trees <rand>
1027 <rand>
1028 one <rand>
1029 one of <rand>
1030 one of drawbacks <rand>
1031 one of drawbacks of <rand>
1032 one of drawbacks of these <rand>
1033 one of drawbacks of these traditional <rand>
1034 one of drawbacks of these traditional models <rand>
1035 one of drawbacks of these traditional models is <rand>
1036 one of drawbacks of these traditional models is that <rand>
1037 one of drawbacks of these traditional models is that they <rand>
1038 one of drawbacks of these traditional models is that they need <rand>
1039 one of drawbacks of these traditional models is that they need a <rand>
1040 one of drawbacks of these traditional models is that they need a lot <rand>
1041 one of drawbacks of these traditional models is that they need a lot of <rand>
1042 one of drawbacks of these traditional models is that they need a lot of hand-designed <rand>
1043 one of drawbacks of these traditional models is that they need a lot of hand-designed features <rand>
1044 one of drawbacks of these traditional models is that they need a lot of hand-designed features and <rand>
1045 one of drawbacks of these traditional models is that they need a lot of hand-designed features and rich <rand>
1046 one of drawbacks of these traditional models is that they need a lot of hand-designed features and rich resources <rand>
1047 one of drawbacks of these traditional models is that they need a lot of hand-designed features and rich resources to <rand>
1048 one of drawbacks of these traditional models is that they need a lot of hand-designed features and rich resources to reach <rand>
1049 one of drawbacks of these traditional models is that they need a lot of hand-designed features and rich resources to reach high <rand>
1050 one of drawbacks of these traditional models is that they need a lot of hand-designed features and rich resources to reach high performance <rand>
1051 <rand>
1052 they <rand>
1053 they model <rand>
1054 they model srl <rand>
1055 they model srl task <rand>
1056 they model srl task using <rand>
1057 they model srl task using time <rand>
1058 they model srl task using time delay <rand>
1059 they model srl task using time delay neural <rand>
1060 they model srl task using time delay neural networks <rand>
1061 they model srl task using time delay neural networks and <rand>
1062 they model srl task using time delay neural networks and got <rand>
1063 they model srl task using time delay neural networks and got competitive <rand>
1064 they model srl task using time delay neural networks and got competitive performance <rand>
1065 they model srl task using time delay neural networks and got competitive performance compared <rand>
1066 they model srl task using time delay neural networks and got competitive performance compared to <rand>
1067 they model srl task using time delay neural networks and got competitive performance compared to the <rand>
1068 they model srl task using time delay neural networks and got competitive performance compared to the state-of-art <rand>
1069 they model srl task using time delay neural networks and got competitive performance compared to the state-of-art traditional <rand>
1070 they model srl task using time delay neural networks and got competitive performance compared to the state-of-art traditional model <rand>
1071 <rand>
1072 consider <rand>
1073 consider the <rand>
1074 consider the expandability <rand>
1075 consider the expandability of <rand>
1076 consider the expandability of the <rand>
1077 consider the expandability of the network <rand>
1078 consider the expandability of the network , <rand>
1079 consider the expandability of the network , we <rand>
1080 consider the expandability of the network , we also <rand>
1081 consider the expandability of the network , we also add <rand>
1082 consider the expandability of the network , we also add a <rand>
1083 consider the expandability of the network , we also add a hidden <rand>
1084 consider the expandability of the network , we also add a hidden layer <rand>
1085 consider the expandability of the network , we also add a hidden layer below <rand>
1086 consider the expandability of the network , we also add a hidden layer below output <rand>
1087 consider the expandability of the network , we also add a hidden layer below output layer <rand>
1088 <rand>
1089 so <rand>
1090 so we <rand>
1091 so we need <rand>
1092 so we need not <rand>
1093 so we need not take <rand>
1094 so we need not take too <rand>
1095 so we need not take too much <rand>
1096 so we need not take too much attention <rand>
1097 so we need not take too much attention on <rand>
1098 so we need not take too much attention on the <rand>
1099 so we need not take too much attention on the roles <rand>
1100 so we need not take too much attention on the roles of <rand>
1101 so we need not take too much attention on the roles of other <rand>
1102 so we need not take too much attention on the roles of other clauses <rand>
1103 <rand>
1104 although <rand>
1105 although the <rand>
1106 although the purpose <rand>
1107 although the purpose of <rand>
1108 although the purpose of sharing <rand>
1109 although the purpose of sharing weight <rand>
1110 although the purpose of sharing weight is <rand>
1111 although the purpose of sharing weight is that <rand>
1112 although the purpose of sharing weight is that the <rand>
1113 although the purpose of sharing weight is that the network <rand>
1114 although the purpose of sharing weight is that the network can <rand>
1115 although the purpose of sharing weight is that the network can learning <rand>
1116 although the purpose of sharing weight is that the network can learning some <rand>
1117 although the purpose of sharing weight is that the network can learning some general <rand>
1118 although the purpose of sharing weight is that the network can learning some general features <rand>
1119 although the purpose of sharing weight is that the network can learning some general features from <rand>
1120 although the purpose of sharing weight is that the network can learning some general features from each <rand>
1121 although the purpose of sharing weight is that the network can learning some general features from each recursive <rand>
1122 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure <rand>
1123 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , <rand>
1124 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our <rand>
1125 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal <rand>
1126 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is <rand>
1127 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is predict <rand>
1128 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is predict different <rand>
1129 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is predict different semantic <rand>
1130 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is predict different semantic roles <rand>
1131 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is predict different semantic roles in <rand>
1132 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is predict different semantic roles in one <rand>
1133 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is predict different semantic roles in one sentence <rand>
1134 <rand>
1135 so <rand>
1136 so our <rand>
1137 so our model <rand>
1138 so our model not <rand>
1139 so our model not only <rand>
1140 so our model not only needs <rand>
1141 so our model not only needs the <rand>
1142 so our model not only needs the global <rand>
1143 so our model not only needs the global features <rand>
1144 so our model not only needs the global features but <rand>
1145 so our model not only needs the global features but local <rand>
1146 so our model not only needs the global features but local features <rand>
1147 so our model not only needs the global features but local features as <rand>
1148 so our model not only needs the global features but local features as well <rand>
1149 <rand>
1150 the <rand>
1151 the loss <rand>
1152 the loss is <rand>
1153 the loss is proportional <rand>
1154 the loss is proportional to <rand>
1155 the loss is proportional to the <rand>
1156 the loss is proportional to the number <rand>
1157 the loss is proportional to the number of <rand>
1158 the loss is proportional to the number of nodes <rand>
1159 the loss is proportional to the number of nodes with <rand>
1160 the loss is proportional to the number of nodes with an <rand>
1161 the loss is proportional to the number of nodes with an incorrect <rand>
1162 the loss is proportional to the number of nodes with an incorrect role <rand>
1163 the loss is proportional to the number of nodes with an incorrect role in <rand>
1164 the loss is proportional to the number of nodes with an incorrect role in the <rand>
1165 the loss is proportional to the number of nodes with an incorrect role in the proposed <rand>
1166 the loss is proportional to the number of nodes with an incorrect role in the proposed role <rand>
1167 the loss is proportional to the number of nodes with an incorrect role in the proposed role set <rand>
1168 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , <rand>
1169 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which <rand>
1170 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases <rand>
1171 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the <rand>
1172 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the more <rand>
1173 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the more incorrect <rand>
1174 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the more incorrect the <rand>
1175 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the more incorrect the proposed <rand>
1176 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the more incorrect the proposed role <rand>
1177 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the more incorrect the proposed role set <rand>
1178 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the more incorrect the proposed role set is <rand>
1179 <rand>
1180 although <rand>
1181 although our <rand>
1182 although our system <rand>
1183 although our system is <rand>
1184 although our system is a <rand>
1185 although our system is a recursive <rand>
1186 although our system is a recursive network <rand>
1187 although our system is a recursive network based <rand>
1188 although our system is a recursive network based on <rand>
1189 although our system is a recursive network based on parse <rand>
1190 although our system is a recursive network based on parse tree <rand>
1191 although our system is a recursive network based on parse tree , <rand>
1192 although our system is a recursive network based on parse tree , an <rand>
1193 although our system is a recursive network based on parse tree , an average <rand>
1194 although our system is a recursive network based on parse tree , an average parse <rand>
1195 although our system is a recursive network based on parse tree , an average parse tree <rand>
1196 although our system is a recursive network based on parse tree , an average parse tree level <rand>
1197 although our system is a recursive network based on parse tree , an average parse tree level is <rand>
1198 although our system is a recursive network based on parse tree , an average parse tree level is about <rand>
1199 although our system is a recursive network based on parse tree , an average parse tree level is about ten <rand>
1200 although our system is a recursive network based on parse tree , an average parse tree level is about ten and <rand>
1201 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the <rand>
1202 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length <rand>
1203 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of <rand>
1204 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence <rand>
1205 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is <rand>
1206 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about <rand>
1207 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen <rand>
1208 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , <rand>
1209 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the <rand>
1210 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost <rand>
1211 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of <rand>
1212 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing <rand>
1213 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on <rand>
1214 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the <rand>
1215 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive <rand>
1216 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse <rand>
1217 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree <rand>
1218 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is <rand>
1219 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar <rand>
1220 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with <rand>
1221 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with the <rand>
1222 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with the cost <rand>
1223 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with the cost of <rand>
1224 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with the cost of convolution <rand>
1225 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with the cost of convolution layer <rand>
1226 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with the cost of convolution layer in <rand>
1227 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with the cost of convolution layer in cnn <rand>
1228 <rand>
1229 our <rand>
1230 our model <rand>
1231 our model is <rand>
1232 our model is also <rand>
1233 our model is also extensible <rand>
1234 our model is also extensible for <rand>
1235 our model is also extensible for add <rand>
1236 our model is also extensible for add more <rand>
1237 our model is also extensible for add more features <rand>
1238 our model is also extensible for add more features and <rand>
1239 our model is also extensible for add more features and resources <rand>
1240 our model is also extensible for add more features and resources as <rand>
1241 our model is also extensible for add more features and resources as talked <rand>
1242 our model is also extensible for add more features and resources as talked in <rand>
1243 our model is also extensible for add more features and resources as talked in section <rand>
1244 our model is also extensible for add more features and resources as talked in section 3 <rand>
1245 <rand>
1246 svm-crf <rand>
1247 svm-crf modeled <rand>
1248 svm-crf modeled the <rand>
1249 svm-crf modeled the task <rand>
1250 svm-crf modeled the task by <rand>
1251 svm-crf modeled the task by sequence <rand>
1252 svm-crf modeled the task by sequence labeling <rand>
1253 svm-crf modeled the task by sequence labeling model <rand>
1254 svm-crf modeled the task by sequence labeling model , <rand>
1255 svm-crf modeled the task by sequence labeling model , however <rand>
1256 svm-crf modeled the task by sequence labeling model , however this <rand>
1257 svm-crf modeled the task by sequence labeling model , however this method <rand>
1258 svm-crf modeled the task by sequence labeling model , however this method performs <rand>
1259 svm-crf modeled the task by sequence labeling model , however this method performs badly <rand>
1260 svm-crf modeled the task by sequence labeling model , however this method performs badly and <rand>
1261 svm-crf modeled the task by sequence labeling model , however this method performs badly and may <rand>
1262 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be <rand>
1263 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because <rand>
1264 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the <rand>
1265 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf <rand>
1266 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model <rand>
1267 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs <rand>
1268 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a <rand>
1269 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large <rand>
1270 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number <rand>
1271 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of <rand>
1272 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data <rand>
1273 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to <rand>
1274 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn <rand>
1275 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the <rand>
1276 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model <rand>
1277 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters <rand>
1278 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and <rand>
1279 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and the <rand>
1280 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and the imbalanced <rand>
1281 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and the imbalanced learning <rand>
1282 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and the imbalanced learning problem <rand>
1283 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and the imbalanced learning problem is <rand>
1284 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and the imbalanced learning problem is not <rand>
1285 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and the imbalanced learning problem is not concerned <rand>
1286 <rand>
1287 lcr <rand>
1288 lcr in <rand>
1289 lcr in the <rand>
1290 lcr in the right <rand>
1291 lcr in the right section <rand>
1292 lcr in the right section shares <rand>
1293 lcr in the right section shares lookup <rand>
1294 lcr in the right section shares lookup table <rand>
1295 lcr in the right section shares lookup table layer <rand>
1296 lcr in the right section shares lookup table layer and <rand>
1297 lcr in the right section shares lookup table layer and convolutional <rand>
1298 lcr in the right section shares lookup table layer and convolutional layer <rand>
1299 lcr in the right section shares lookup table layer and convolutional layer with <rand>
1300 lcr in the right section shares lookup table layer and convolutional layer with ldr <rand>
1301 lcr in the right section shares lookup table layer and convolutional layer with ldr the <rand>
1302 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn <rand>
1303 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture <rand>
1304 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically <rand>
1305 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns <rand>
1306 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features <rand>
1307 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for <rand>
1308 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification <rand>
1309 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task <rand>
1310 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task in <rand>
1311 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task in the <rand>
1312 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task in the deep <rand>
1313 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task in the deep layers <rand>
1314 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task in the deep layers of <rand>
1315 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task in the deep layers of its <rand>
1316 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task in the deep layers of its architecture <rand>
1317 <rand>
1318 learning <rand>
1319 learning continuous <rand>
1320 learning continuous representation <rand>
1321 learning continuous representation of <rand>
1322 learning continuous representation of the <rand>
1323 learning continuous representation of the context <rand>
1324 learning continuous representation of the context and <rand>
1325 learning continuous representation of the context and its <rand>
1326 learning continuous representation of the context and its category <rand>
1327 learning continuous representation of the context and its category are <rand>
1328 learning continuous representation of the context and its category are two <rand>
1329 learning continuous representation of the context and its category are two highly <rand>
1330 learning continuous representation of the context and its category are two highly related <rand>
1331 learning continuous representation of the context and its category are two highly related task <rand>
1332 learning continuous representation of the context and its category are two highly related task , <rand>
1333 learning continuous representation of the context and its category are two highly related task , and <rand>
1334 learning continuous representation of the context and its category are two highly related task , and it <rand>
1335 learning continuous representation of the context and its category are two highly related task , and it make <rand>
1336 learning continuous representation of the context and its category are two highly related task , and it make sense <rand>
1337 learning continuous representation of the context and its category are two highly related task , and it make sense that <rand>
1338 learning continuous representation of the context and its category are two highly related task , and it make sense that features <rand>
1339 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful <rand>
1340 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for <rand>
1341 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one <rand>
1342 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one task <rand>
1343 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one task might <rand>
1344 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one task might be <rand>
1345 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one task might be useful <rand>
1346 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one task might be useful for <rand>
1347 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one task might be useful for another <rand>
1348 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one task might be useful for another one <rand>
1349 <rand>
1350 compared <rand>
1351 compared to <rand>
1352 compared to traditional <rand>
1353 compared to traditional models <rand>
1354 compared to traditional models , <rand>
1355 compared to traditional models , we <rand>
1356 compared to traditional models , we incorporates <rand>
1357 compared to traditional models , we incorporates much <rand>
1358 compared to traditional models , we incorporates much larger <rand>
1359 compared to traditional models , we incorporates much larger context <rand>
1360 compared to traditional models , we incorporates much larger context and <rand>
1361 compared to traditional models , we incorporates much larger context and more <rand>
1362 compared to traditional models , we incorporates much larger context and more patterns <rand>
1363 compared to traditional models , we incorporates much larger context and more patterns of <rand>
1364 compared to traditional models , we incorporates much larger context and more patterns of interactions <rand>
1365 <rand>
1366 we <rand>
1367 we adapt <rand>
1368 we adapt this <rand>
1369 we adapt this network <rand>
1370 we adapt this network for <rand>
1371 we adapt this network for dependency <rand>
1372 we adapt this network for dependency trees <rand>
1373 we adapt this network for dependency trees and <rand>
1374 we adapt this network for dependency trees and propose <rand>
1375 we adapt this network for dependency trees and propose the <rand>
1376 we adapt this network for dependency trees and propose the content-context <rand>
1377 we adapt this network for dependency trees and propose the content-context vectorial <rand>
1378 we adapt this network for dependency trees and propose the content-context vectorial representation <rand>
1379 we adapt this network for dependency trees and propose the content-context vectorial representation for <rand>
1380 we adapt this network for dependency trees and propose the content-context vectorial representation for each <rand>
1381 we adapt this network for dependency trees and propose the content-context vectorial representation for each node <rand>
1382 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , <rand>
1383 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this <rand>
1384 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node <rand>
1385 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation <rand>
1386 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses <rand>
1387 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses rich <rand>
1388 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses rich context <rand>
1389 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses rich context of <rand>
1390 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses rich context of the <rand>
1391 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses rich context of the whole <rand>
1392 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses rich context of the whole sub-tree <rand>
1393 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses rich context of the whole sub-tree information <rand>
1394 <rand>
1395 the <rand>
1396 the only <rand>
1397 the only difference <rand>
1398 the only difference is <rand>
1399 the only difference is that <rand>
1400 the only difference is that in <rand>
1401 the only difference is that in each <rand>
1402 the only difference is that in each chart <rand>
1403 the only difference is that in each chart cell <rand>
1404 the only difference is that in each chart cell , <rand>
1405 the only difference is that in each chart cell , we <rand>
1406 the only difference is that in each chart cell , we keep <rand>
1407 the only difference is that in each chart cell , we keep b <rand>
1408 the only difference is that in each chart cell , we keep b highest <rand>
1409 the only difference is that in each chart cell , we keep b highest scored <rand>
1410 the only difference is that in each chart cell , we keep b highest scored candidates <rand>
1411 the only difference is that in each chart cell , we keep b highest scored candidates instead <rand>
1412 the only difference is that in each chart cell , we keep b highest scored candidates instead of <rand>
1413 the only difference is that in each chart cell , we keep b highest scored candidates instead of just <rand>
1414 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one <rand>
1415 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one , <rand>
1416 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one , and <rand>
1417 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one , and call <rand>
1418 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one , and call 7 <rand>
1419 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one , and call 7 this <rand>
1420 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one , and call 7 this list <rand>
1421 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one , and call 7 this list an <rand>
1422 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one , and call 7 this list an agenda <rand>
1423 <rand>
1424 this <rand>
1425 this will <rand>
1426 this will be <rand>
1427 this will be time-consuming <rand>
1428 this will be time-consuming in <rand>
1429 this will be time-consuming in practise <rand>
1430 <rand>
1431 the <rand>
1432 the minibatch <rand>
1433 the minibatch size <rand>
1434 the minibatch size was <rand>
1435 the minibatch size was set <rand>
1436 the minibatch size was set as <rand>
1437 the minibatch size was set as 20 <rand>
1438 <rand>
1439 we <rand>
1440 we then <rand>
1441 we then directly <rand>
1442 we then directly applies <rand>
1443 we then directly applies this <rand>
1444 we then directly applies this trained <rand>
1445 we then directly applies this trained model <rand>
1446 we then directly applies this trained model in <rand>
1447 we then directly applies this trained model in a <rand>
1448 we then directly applies this trained model in a reranking <rand>
1449 we then directly applies this trained model in a reranking framework <rand>
1450 <rand>
1451 compared <rand>
1452 compared with <rand>
1453 compared with these <rand>
1454 compared with these models <rand>
1455 compared with these models , <rand>
1456 compared with these models , our <rand>
1457 compared with these models , our system <rand>
1458 compared with these models , our system considers <rand>
1459 compared with these models , our system considers richer <rand>
1460 compared with these models , our system considers richer more <rand>
1461 compared with these models , our system considers richer more structural <rand>
1462 compared with these models , our system considers richer more structural context <rand>
1463 compared with these models , our system considers richer more structural context information <rand>
1464 compared with these models , our system considers richer more structural context information , <rand>
1465 compared with these models , our system considers richer more structural context information , and <rand>
1466 compared with these models , our system considers richer more structural context information , and we <rand>
1467 compared with these models , our system considers richer more structural context information , and we do <rand>
1468 compared with these models , our system considers richer more structural context information , and we do not <rand>
1469 compared with these models , our system considers richer more structural context information , and we do not rely <rand>
1470 compared with these models , our system considers richer more structural context information , and we do not rely on <rand>
1471 compared with these models , our system considers richer more structural context information , and we do not rely on hand-crafted <rand>
1472 compared with these models , our system considers richer more structural context information , and we do not rely on hand-crafted feature <rand>
1473 compared with these models , our system considers richer more structural context information , and we do not rely on hand-crafted feature templates <rand>
1474 <rand>
1475 such <rand>
1476 such as <rand>
1477 such as treatment <rand>
1478 such as treatment customization <rand>
1479 such as treatment customization based <rand>
1480 such as treatment customization based on <rand>
1481 such as treatment customization based on the <rand>
1482 such as treatment customization based on the risk <rand>
1483 such as treatment customization based on the risk level <rand>
1484 such as treatment customization based on the risk level of <rand>
1485 such as treatment customization based on the risk level of the <rand>
1486 such as treatment customization based on the risk level of the specific <rand>
1487 such as treatment customization based on the risk level of the specific patient <rand>
1488 <rand>
1489 in <rand>
1490 in this <rand>
1491 in this way <rand>
1492 in this way , <rand>
1493 in this way , each <rand>
1494 in this way , each report <rand>
1495 in this way , each report can <rand>
1496 in this way , each report can be <rand>
1497 in this way , each report can be represented <rand>
1498 in this way , each report can be represented by <rand>
1499 in this way , each report can be represented by a <rand>
1500 in this way , each report can be represented by a vector <rand>
1501 in this way , each report can be represented by a vector containing <rand>
1502 in this way , each report can be represented by a vector containing values <rand>
1503 in this way , each report can be represented by a vector containing values of <rand>
1504 in this way , each report can be represented by a vector containing values of each <rand>
1505 in this way , each report can be represented by a vector containing values of each attributes <rand>
1506 <rand>
1507 while <rand>
1508 while the <rand>
1509 while the classifier <rand>
1510 while the classifier trained <rand>
1511 while the classifier trained by <rand>
1512 while the classifier trained by machine <rand>
1513 while the classifier trained by machine learning <rand>
1514 while the classifier trained by machine learning is <rand>
1515 while the classifier trained by machine learning is validated <rand>
1516 while the classifier trained by machine learning is validated through <rand>
1517 while the classifier trained by machine learning is validated through a <rand>
1518 while the classifier trained by machine learning is validated through a 10-fold <rand>
1519 while the classifier trained by machine learning is validated through a 10-fold cross-validation <rand>
1520 while the classifier trained by machine learning is validated through a 10-fold cross-validation approach <rand>
1521 <rand>
1522 after <rand>
1523 after the <rand>
1524 after the tagging <rand>
1525 after the tagging process <rand>
1526 after the tagging process was <rand>
1527 after the tagging process was done <rand>
1528 after the tagging process was done , <rand>
1529 after the tagging process was done , adopting <rand>
1530 after the tagging process was done , adopting maximum <rand>
1531 after the tagging process was done , adopting maximum entropy <rand>
1532 after the tagging process was done , adopting maximum entropy model <rand>
1533 after the tagging process was done , adopting maximum entropy model three <rand>
1534 after the tagging process was done , adopting maximum entropy model three subtasks <rand>
1535 after the tagging process was done , adopting maximum entropy model three subtasks of <rand>
1536 after the tagging process was done , adopting maximum entropy model three subtasks of chinese <rand>
1537 after the tagging process was done , adopting maximum entropy model three subtasks of chinese lexical <rand>
1538 after the tagging process was done , adopting maximum entropy model three subtasks of chinese lexical analysis <rand>
1539 after the tagging process was done , adopting maximum entropy model three subtasks of chinese lexical analysis could <rand>
1540 after the tagging process was done , adopting maximum entropy model three subtasks of chinese lexical analysis could be <rand>
1541 after the tagging process was done , adopting maximum entropy model three subtasks of chinese lexical analysis could be completed <rand>
1542 after the tagging process was done , adopting maximum entropy model three subtasks of chinese lexical analysis could be completed at <rand>
1543 after the tagging process was done , adopting maximum entropy model three subtasks of chinese lexical analysis could be completed at once <rand>
1544 <rand>
1545 the <rand>
1546 the raising <rand>
1547 the raising performance <rand>
1548 the raising performance indicates <rand>
1549 the raising performance indicates that <rand>
1550 the raising performance indicates that the <rand>
1551 the raising performance indicates that the trinity <rand>
1552 the raising performance indicates that the trinity character-based <rand>
1553 the raising performance indicates that the trinity character-based tagging <rand>
1554 the raising performance indicates that the trinity character-based tagging method <rand>
1555 the raising performance indicates that the trinity character-based tagging method is <rand>
1556 the raising performance indicates that the trinity character-based tagging method is much <rand>
1557 the raising performance indicates that the trinity character-based tagging method is much better <rand>
1558 the raising performance indicates that the trinity character-based tagging method is much better than <rand>
1559 the raising performance indicates that the trinity character-based tagging method is much better than traditional <rand>
1560 the raising performance indicates that the trinity character-based tagging method is much better than traditional method <rand>
1561 <rand>
1562 the <rand>
1563 the corpus <rand>
1564 the corpus is <rand>
1565 the corpus is based <rand>
1566 the corpus is based on <rand>
1567 the corpus is based on connective-driven <rand>
1568 the corpus is based on connective-driven dependence <rand>
1569 the corpus is based on connective-driven dependence tree <rand>
1570 the corpus is based on connective-driven dependence tree representation <rand>
1571 the corpus is based on connective-driven dependence tree representation system <rand>
1572 the corpus is based on connective-driven dependence tree representation system , <rand>
1573 the corpus is based on connective-driven dependence tree representation system , which <rand>
1574 the corpus is based on connective-driven dependence tree representation system , which contains <rand>
1575 the corpus is based on connective-driven dependence tree representation system , which contains clause <rand>
1576 the corpus is based on connective-driven dependence tree representation system , which contains clause , <rand>
1577 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective <rand>
1578 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective , <rand>
1579 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective , discourse <rand>
1580 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective , discourse relation <rand>
1581 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective , discourse relation , <rand>
1582 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective , discourse relation , discourse <rand>
1583 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective , discourse relation , discourse structure <rand>
1584 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective , discourse relation , discourse structure et <rand>
1585 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective , discourse relation , discourse structure et al <rand>
1586 <rand>
1587 firstly <rand>
1588 firstly , <rand>
1589 firstly , answer <rand>
1590 firstly , answer features <rand>
1591 firstly , answer features bind <rand>
1592 firstly , answer features bind question <rand>
1593 firstly , answer features bind question features <rand>
1594 firstly , answer features bind question features to <rand>
1595 firstly , answer features bind question features to realize <rand>
1596 firstly , answer features bind question features to realize samples <rand>
1597 firstly , answer features bind question features to realize samples said <rand>
1598 firstly , answer features bind question features to realize samples said ; <rand>
1599 firstly , answer features bind question features to realize samples said ; secondly <rand>
1600 firstly , answer features bind question features to realize samples said ; secondly , <rand>
1601 firstly , answer features bind question features to realize samples said ; secondly , a <rand>
1602 firstly , answer features bind question features to realize samples said ; secondly , a question <rand>
1603 firstly , answer features bind question features to realize samples said ; secondly , a question classifier <rand>
1604 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will <rand>
1605 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be <rand>
1606 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained <rand>
1607 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on <rand>
1608 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled <rand>
1609 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions <rand>
1610 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using <rand>
1611 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label <rand>
1612 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation <rand>
1613 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm <rand>
1614 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to <rand>
1615 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to annotate <rand>
1616 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to annotate the <rand>
1617 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to annotate the category <rand>
1618 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to annotate the category of <rand>
1619 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to annotate the category of unlabeled <rand>
1620 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to annotate the category of unlabeled questions <rand>
1621 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to annotate the category of unlabeled questions automatically <rand>
1622 <rand>
1623 topics <rand>
1624 topics are <rand>
1625 topics are then <rand>
1626 topics are then represented <rand>
1627 topics are then represented with <rand>
1628 topics are then represented with multi-word <rand>
1629 topics are then represented with multi-word expression <rand>
1630 <rand>
1631 label <rand>
1632 label examples <rand>
1633 label examples for <rand>
1634 label examples for news <rand>
1635 label examples for news , <rand>
1636 label examples for news , twitter <rand>
1637 label examples for news , twitter and <rand>
1638 label examples for news , twitter and nips <rand>
1639 label examples for news , twitter and nips topics <rand>
1640 <rand>
1641 unbiased <rand>
1642 unbiased , <rand>
1643 unbiased , which <rand>
1644 unbiased , which means <rand>
1645 unbiased , which means that <rand>
1646 unbiased , which means that either <rand>
1647 unbiased , which means that either of <rand>
1648 unbiased , which means that either of two <rand>
1649 unbiased , which means that either of two characters <rand>
1650 unbiased , which means that either of two characters semantic <rand>
1651 unbiased , which means that either of two characters semantic distance <rand>
1652 unbiased , which means that either of two characters semantic distance to <rand>
1653 unbiased , which means that either of two characters semantic distance to the <rand>
1654 unbiased , which means that either of two characters semantic distance to the word <rand>
1655 unbiased , which means that either of two characters semantic distance to the word is <rand>
1656 unbiased , which means that either of two characters semantic distance to the word is approximate <rand>
1657 unbiased , which means that either of two characters semantic distance to the word is approximate or <rand>
1658 unbiased , which means that either of two characters semantic distance to the word is approximate or the <rand>
1659 unbiased , which means that either of two characters semantic distance to the word is approximate or the word <rand>
1660 unbiased , which means that either of two characters semantic distance to the word is approximate or the word is <rand>
1661 unbiased , which means that either of two characters semantic distance to the word is approximate or the word is non-compositional <rand>
1662 <rand>
1663 generally <rand>
1664 generally speaking <rand>
1665 generally speaking , <rand>
1666 generally speaking , when <rand>
1667 generally speaking , when machine <rand>
1668 generally speaking , when machine learning <rand>
1669 generally speaking , when machine learning models <rand>
1670 generally speaking , when machine learning models such <rand>
1671 generally speaking , when machine learning models such as <rand>
1672 generally speaking , when machine learning models such as me <rand>
1673 generally speaking , when machine learning models such as me and <rand>
1674 generally speaking , when machine learning models such as me and crfs <rand>
1675 generally speaking , when machine learning models such as me and crfs are <rand>
1676 generally speaking , when machine learning models such as me and crfs are used <rand>
1677 generally speaking , when machine learning models such as me and crfs are used to <rand>
1678 generally speaking , when machine learning models such as me and crfs are used to train <rand>
1679 generally speaking , when machine learning models such as me and crfs are used to train a <rand>
1680 generally speaking , when machine learning models such as me and crfs are used to train a tibetan <rand>
1681 generally speaking , when machine learning models such as me and crfs are used to train a tibetan word <rand>
1682 generally speaking , when machine learning models such as me and crfs are used to train a tibetan word segmenter <rand>
1683 <rand>
1684 these <rand>
1685 these features <rand>
1686 these features are <rand>
1687 these features are usually <rand>
1688 these features are usually acquired <rand>
1689 these features are usually acquired by <rand>
1690 these features are usually acquired by the <rand>
1691 these features are usually acquired by the help <rand>
1692 these features are usually acquired by the help of <rand>
1693 these features are usually acquired by the help of external <rand>
1694 these features are usually acquired by the help of external linguistic <rand>
1695 these features are usually acquired by the help of external linguistic resources <rand>
1696 <rand>
1697 the <rand>
1698 the traditional <rand>
1699 the traditional methods <rand>
1700 the traditional methods introduced <rand>
1701 the traditional methods introduced above <rand>
1702 the traditional methods introduced above for <rand>
1703 the traditional methods introduced above for implicit <rand>
1704 the traditional methods introduced above for implicit relation <rand>
1705 the traditional methods introduced above for implicit relation classification <rand>
1706 the traditional methods introduced above for implicit relation classification face <rand>
1707 the traditional methods introduced above for implicit relation classification face two <rand>
1708 the traditional methods introduced above for implicit relation classification face two major <rand>
1709 the traditional methods introduced above for implicit relation classification face two major challenges <rand>
1710 <rand>
1711 since <rand>
1712 since sentences <rand>
1713 since sentences in <rand>
1714 since sentences in different <rand>
1715 since sentences in different length <rand>
1716 since sentences in different length contain <rand>
1717 since sentences in different length contain different <rand>
1718 since sentences in different length contain different number <rand>
1719 since sentences in different length contain different number of <rand>
1720 since sentences in different length contain different number of word <rand>
1721 since sentences in different length contain different number of word pairs <rand>
1722 since sentences in different length contain different number of word pairs , <rand>
1723 since sentences in different length contain different number of word pairs , concatenation <rand>
1724 since sentences in different length contain different number of word pairs , concatenation of <rand>
1725 since sentences in different length contain different number of word pairs , concatenation of word <rand>
1726 since sentences in different length contain different number of word pairs , concatenation of word embeddings <rand>
1727 since sentences in different length contain different number of word pairs , concatenation of word embeddings can <rand>
1728 since sentences in different length contain different number of word pairs , concatenation of word embeddings can not <rand>
1729 since sentences in different length contain different number of word pairs , concatenation of word embeddings can not lead <rand>
1730 since sentences in different length contain different number of word pairs , concatenation of word embeddings can not lead to <rand>
1731 since sentences in different length contain different number of word pairs , concatenation of word embeddings can not lead to a <rand>
1732 since sentences in different length contain different number of word pairs , concatenation of word embeddings can not lead to a fixed-length <rand>
1733 since sentences in different length contain different number of word pairs , concatenation of word embeddings can not lead to a fixed-length vector <rand>
1734 since sentences in different length contain different number of word pairs , concatenation of word embeddings can not lead to a fixed-length vector representation <rand>
1735 <rand>
1736 additionally <rand>
1737 additionally , <rand>
1738 additionally , when <rand>
1739 additionally , when we <rand>
1740 additionally , when we are <rand>
1741 additionally , when we are training <rand>
1742 additionally , when we are training with <rand>
1743 additionally , when we are training with discrete <rand>
1744 additionally , when we are training with discrete features <rand>
1745 additionally , when we are training with discrete features , <rand>
1746 additionally , when we are training with discrete features , we <rand>
1747 additionally , when we are training with discrete features , we follow <rand>
1748 additionally , when we are training with discrete features , we follow the <rand>
1749 additionally , when we are training with discrete features , we follow the previous <rand>
1750 additionally , when we are training with discrete features , we follow the previous methods <rand>
1751 additionally , when we are training with discrete features , we follow the previous methods and <rand>
1752 additionally , when we are training with discrete features , we follow the previous methods and remove <rand>
1753 additionally , when we are training with discrete features , we follow the previous methods and remove the <rand>
1754 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete <rand>
1755 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features <rand>
1756 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose <rand>
1757 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence <rand>
1758 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number <rand>
1759 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower <rand>
1760 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than <rand>
1761 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a <rand>
1762 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off <rand>
1763 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , <rand>
1764 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which <rand>
1765 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is <rand>
1766 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 <rand>
1767 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , <rand>
1768 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 <rand>
1769 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , <rand>
1770 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and <rand>
1771 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 <rand>
1772 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for <rand>
1773 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3 <rand>
1774 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3 , <rand>
1775 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3 , words <rand>
1776 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3 , words pairs <rand>
1777 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3 , words pairs and <rand>
1778 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3 , words pairs and production <rand>
1779 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3 , words pairs and production rules <rand>
1780 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3 , words pairs and production rules respectively <rand>
1781 <rand>
1782 our <rand>
1783 our final <rand>
1784 our final system <rand>
1785 our final system obtain <rand>
1786 our final system obtain the <rand>
1787 our final system obtain the best <rand>
1788 our final system obtain the best performance <rand>
1789 our final system obtain the best performance for <rand>
1790 our final system obtain the best performance for contingency <rand>
1791 our final system obtain the best performance for contingency , <rand>
1792 our final system obtain the best performance for contingency , expansion <rand>
1793 our final system obtain the best performance for contingency , expansion and <rand>
1794 our final system obtain the best performance for contingency , expansion and temporal <rand>
1795 our final system obtain the best performance for contingency , expansion and temporal relations <rand>
1796 our final system obtain the best performance for contingency , expansion and temporal relations , <rand>
1797 our final system obtain the best performance for contingency , expansion and temporal relations , while <rand>
1798 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps <rand>
1799 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive <rand>
1800 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in <rand>
1801 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in comparison <rand>
1802 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in comparison relation <rand>
1803 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in comparison relation compared <rand>
1804 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in comparison relation compared to <rand>
1805 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in comparison relation compared to the <rand>
1806 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in comparison relation compared to the state-of-the-art <rand>
1807 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in comparison relation compared to the state-of-the-art methods <rand>
1808 <rand>
1809 in <rand>
1810 in the <rand>
1811 in the future <rand>
1812 in the future , <rand>
1813 in the future , we <rand>
1814 in the future , we will <rand>
1815 in the future , we will devote <rand>
1816 in the future , we will devote ourselves <rand>
1817 in the future , we will devote ourselves to <rand>
1818 in the future , we will devote ourselves to encode <rand>
1819 in the future , we will devote ourselves to encode more <rand>
1820 in the future , we will devote ourselves to encode more linguistically <rand>
1821 in the future , we will devote ourselves to encode more linguistically features <rand>
1822 in the future , we will devote ourselves to encode more linguistically features into <rand>
1823 in the future , we will devote ourselves to encode more linguistically features into distributed <rand>
1824 in the future , we will devote ourselves to encode more linguistically features into distributed representation <rand>
1825 in the future , we will devote ourselves to encode more linguistically features into distributed representation and <rand>
1826 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore <rand>
1827 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more <rand>
1828 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective <rand>
1829 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method <rand>
1830 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to <rand>
1831 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn <rand>
1832 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn long-distance <rand>
1833 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn long-distance dependency <rand>
1834 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn long-distance dependency of <rand>
1835 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn long-distance dependency of the <rand>
1836 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn long-distance dependency of the syntax <rand>
1837 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn long-distance dependency of the syntax and <rand>
1838 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn long-distance dependency of the syntax and semantics <rand>
1839 <rand>
1840 we <rand>
1841 we add <rand>
1842 we add this <rand>
1843 we add this ratio <rand>
1844 we add this ratio in <rand>
1845 we add this ratio in a <rand>
1846 we add this ratio in a logarithm <rand>
1847 we add this ratio in a logarithm function <rand>
1848 we add this ratio in a logarithm function as <rand>
1849 we add this ratio in a logarithm function as the <rand>
1850 we add this ratio in a logarithm function as the weight <rand>
1851 we add this ratio in a logarithm function as the weight of <rand>
1852 we add this ratio in a logarithm function as the weight of the <rand>
1853 we add this ratio in a logarithm function as the weight of the matching <rand>
1854 we add this ratio in a logarithm function as the weight of the matching word <rand>
1855 <rand>
1856 performances <rand>
1857 performances of <rand>
1858 performances of meteor <rand>
1859 performances of meteor based <rand>
1860 performances of meteor based on <rand>
1861 performances of meteor based on different <rand>
1862 performances of meteor based on different score <rand>
1863 performances of meteor based on different score selections <rand>
1864 performances of meteor based on different score selections on <rand>
1865 performances of meteor based on different score selections on fluency <rand>
1866 performances of meteor based on different score selections on fluency and <rand>
1867 performances of meteor based on different score selections on fluency and accuracy <rand>
1868 performances of meteor based on different score selections on fluency and accuracy with <rand>
1869 performances of meteor based on different score selections on fluency and accuracy with and <rand>
1870 performances of meteor based on different score selections on fluency and accuracy with and without <rand>
1871 performances of meteor based on different score selections on fluency and accuracy with and without synonym <rand>
1872 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching <rand>
1873 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at <rand>
1874 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level <rand>
1875 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in <rand>
1876 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms <rand>
1877 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of <rand>
1878 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of pearson <rand>
1879 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of pearson correlation <rand>
1880 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of pearson correlation are <rand>
1881 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of pearson correlation are shown <rand>
1882 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of pearson correlation are shown in <rand>
1883 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of pearson correlation are shown in tables <rand>
1884 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of pearson correlation are shown in tables 1-2 <rand>
1885 <rand>
1886 as <rand>
1887 as the <rand>
1888 as the architecture <rand>
1889 as the architecture is <rand>
1890 as the architecture is given <rand>
1891 as the architecture is given in <rand>
1892 as the architecture is given in the <rand>
1893 as the architecture is given in the figure <rand>
1894 as the architecture is given in the figure 2 <rand>
1895 as the architecture is given in the figure 2 , <rand>
1896 as the architecture is given in the figure 2 , scnn <rand>
1897 as the architecture is given in the figure 2 , scnn model <rand>
1898 as the architecture is given in the figure 2 , scnn model consists <rand>
1899 as the architecture is given in the figure 2 , scnn model consists of <rand>
1900 as the architecture is given in the figure 2 , scnn model consists of two <rand>
1901 as the architecture is given in the figure 2 , scnn model consists of two convolutional <rand>
1902 as the architecture is given in the figure 2 , scnn model consists of two convolutional layers <rand>
1903 as the architecture is given in the figure 2 , scnn model consists of two convolutional layers to <rand>
1904 as the architecture is given in the figure 2 , scnn model consists of two convolutional layers to do <rand>
1905 as the architecture is given in the figure 2 , scnn model consists of two convolutional layers to do the <rand>
1906 as the architecture is given in the figure 2 , scnn model consists of two convolutional layers to do the composition <rand>
1907 <rand>
1908 the <rand>
1909 the our <rand>
1910 the our scnn <rand>
1911 the our scnn model <rand>
1912 the our scnn model for <rand>
1913 the our scnn model for learning <rand>
1914 the our scnn model for learning sentence <rand>
1915 the our scnn model for learning sentence representation <rand>
1916 <rand>
1917 through <rand>
1918 through the <rand>
1919 through the pooling <rand>
1920 through the pooling layer <rand>
1921 through the pooling layer , <rand>
1922 through the pooling layer , the <rand>
1923 through the pooling layer , the sentence <rand>
1924 through the pooling layer , the sentence vectors <rand>
1925 through the pooling layer , the sentence vectors transform <rand>
1926 through the pooling layer , the sentence vectors transform into <rand>
1927 through the pooling layer , the sentence vectors transform into a <rand>
1928 through the pooling layer , the sentence vectors transform into a document <rand>
1929 through the pooling layer , the sentence vectors transform into a document vector <rand>
1930 through the pooling layer , the sentence vectors transform into a document vector by <rand>
1931 through the pooling layer , the sentence vectors transform into a document vector by a <rand>
1932 through the pooling layer , the sentence vectors transform into a document vector by a weighted-average <rand>
1933 through the pooling layer , the sentence vectors transform into a document vector by a weighted-average operation <rand>
1934 <rand>
1935 we <rand>
1936 we do <rand>
1937 we do two <rand>
1938 we do two comparison <rand>
1939 we do two comparison experiments <rand>
1940 we do two comparison experiments to <rand>
1941 we do two comparison experiments to show <rand>
1942 we do two comparison experiments to show the <rand>
1943 we do two comparison experiments to show the effectiveness <rand>
1944 we do two comparison experiments to show the effectiveness our <rand>
1945 we do two comparison experiments to show the effectiveness our model <rand>
1946 <rand>
1947 the <rand>
1948 the basic <rand>
1949 the basic cnn <rand>
1950 the basic cnn is <rand>
1951 the basic cnn is the <rand>
1952 the basic cnn is the basic <rand>
1953 the basic cnn is the basic convolutional <rand>
1954 the basic cnn is the basic convolutional neural <rand>
1955 the basic cnn is the basic convolutional neural network <rand>
1956 the basic cnn is the basic convolutional neural network model <rand>
1957 the basic cnn is the basic convolutional neural network model which <rand>
1958 the basic cnn is the basic convolutional neural network model which sentences <rand>
1959 the basic cnn is the basic convolutional neural network model which sentences are <rand>
1960 the basic cnn is the basic convolutional neural network model which sentences are representing <rand>
1961 the basic cnn is the basic convolutional neural network model which sentences are representing through <rand>
1962 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional <rand>
1963 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer <rand>
1964 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and <rand>
1965 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform <rand>
1966 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into <rand>
1967 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into a <rand>
1968 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into a document <rand>
1969 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into a document vector <rand>
1970 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into a document vector by <rand>
1971 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into a document vector by the <rand>
1972 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into a document vector by the average <rand>
1973 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into a document vector by the average operation <rand>
1974 <rand>
1975 the <rand>
1976 the parameters <rand>
1977 the parameters of <rand>
1978 the parameters of swnnmodel <rand>
1979 the parameters of swnnmodel used <rand>
1980 the parameters of swnnmodel used in <rand>
1981 the parameters of swnnmodel used in the <rand>
1982 the parameters of swnnmodel used in the deceptive <rand>
1983 the parameters of swnnmodel used in the deceptive opinion <rand>
1984 the parameters of swnnmodel used in the deceptive opinion spam <rand>
1985 the parameters of swnnmodel used in the deceptive opinion spam detection <rand>
1986 the parameters of swnnmodel used in the deceptive opinion spam detection experiment <rand>
1987 the parameters of swnnmodel used in the deceptive opinion spam detection experiment is <rand>
1988 the parameters of swnnmodel used in the deceptive opinion spam detection experiment is listing <rand>
1989 the parameters of swnnmodel used in the deceptive opinion spam detection experiment is listing followed <rand>
1990 <rand>
1991 this <rand>
1992 this phenomenon <rand>
1993 this phenomenon attracted <rand>
1994 this phenomenon attracted researchers <rand>
1995 this phenomenon attracted researchers attention <rand>
1996 <rand>
1997 sentences <rand>
1998 sentences play <rand>
1999 sentences play different <rand>
2000 sentences play different important <rand>
2001 sentences play different important role <rand>
2002 sentences play different important role in <rand>
2003 sentences play different important role in the <rand>
2004 sentences play different important role in the document <rand>
2005 <rand>
2006 we <rand>
2007 we extend <rand>
2008 we extend it <rand>
2009 we extend it based <rand>
2010 we extend it based tree <rand>
2011 we extend it based tree structure <rand>
2012 we extend it based tree structure to <rand>
2013 we extend it based tree structure to recursive <rand>
2014 we extend it based tree structure to recursive neural <rand>
2015 we extend it based tree structure to recursive neural network <rand>
2016 we extend it based tree structure to recursive neural network to <rand>
2017 we extend it based tree structure to recursive neural network to capture <rand>
2018 we extend it based tree structure to recursive neural network to capture more <rand>
2019 we extend it based tree structure to recursive neural network to capture more syntactic <rand>
2020 we extend it based tree structure to recursive neural network to capture more syntactic and <rand>
2021 we extend it based tree structure to recursive neural network to capture more syntactic and semantic <rand>
2022 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information <rand>
2023 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information , <rand>
2024 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information , and <rand>
2025 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information , and sentiment <rand>
2026 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information , and sentiment polarity <rand>
2027 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information , and sentiment polarity shifting <rand>
2028 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information , and sentiment polarity shifting model <rand>
2029 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information , and sentiment polarity shifting model is <rand>
2030 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information , and sentiment polarity shifting model is introduced <rand>
2031 <rand>
2032 since <rand>
2033 since the <rand>
2034 since the professional <rand>
2035 since the professional technical <rand>
2036 since the professional technical literature <rand>
2037 since the professional technical literature include <rand>
2038 since the professional technical literature include amounts <rand>
2039 since the professional technical literature include amounts of <rand>
2040 since the professional technical literature include amounts of complex <rand>
2041 since the professional technical literature include amounts of complex noun <rand>
2042 since the professional technical literature include amounts of complex noun phrases <rand>
2043 since the professional technical literature include amounts of complex noun phrases , <rand>
2044 since the professional technical literature include amounts of complex noun phrases , identifying <rand>
2045 since the professional technical literature include amounts of complex noun phrases , identifying those <rand>
2046 since the professional technical literature include amounts of complex noun phrases , identifying those phrases <rand>
2047 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has <rand>
2048 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an <rand>
2049 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important <rand>
2050 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical <rand>
2051 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical value <rand>
2052 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical value for <rand>
2053 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical value for such <rand>
2054 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical value for such tasks <rand>
2055 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical value for such tasks as <rand>
2056 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical value for such tasks as machine <rand>
2057 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical value for such tasks as machine translation <rand>
2058 <rand>
2059 as <rand>
2060 as the <rand>
2061 as the development <rand>
2062 as the development of <rand>
2063 as the development of digital <rand>
2064 as the development of digital libraries <rand>
2065 as the development of digital libraries and <rand>
2066 as the development of digital libraries and publication <rand>
2067 as the development of digital libraries and publication , <rand>
2068 as the development of digital libraries and publication , there <rand>
2069 as the development of digital libraries and publication , there is <rand>
2070 as the development of digital libraries and publication , there is a <rand>
2071 as the development of digital libraries and publication , there is a need <rand>
2072 as the development of digital libraries and publication , there is a need to <rand>
2073 as the development of digital libraries and publication , there is a need to assemble <rand>
2074 as the development of digital libraries and publication , there is a need to assemble new <rand>
2075 as the development of digital libraries and publication , there is a need to assemble new books <rand>
2076 as the development of digital libraries and publication , there is a need to assemble new books or <rand>
2077 as the development of digital libraries and publication , there is a need to assemble new books or resources <rand>
2078 as the development of digital libraries and publication , there is a need to assemble new books or resources by <rand>
2079 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking <rand>
2080 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage <rand>
2081 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of <rand>
2082 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books <rand>
2083 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which <rand>
2084 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having <rand>
2085 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having been <rand>
2086 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having been published <rand>
2087 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having been published and <rand>
2088 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having been published and stored <rand>
2089 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having been published and stored in <rand>
2090 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having been published and stored in digital <rand>
2091 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having been published and stored in digital libraries <rand>
2092 <rand>
2093 so <rand>
2094 so an <rand>
2095 so an automatic <rand>
2096 so an automatic keyword <rand>
2097 so an automatic keyword recommendation <rand>
2098 so an automatic keyword recommendation mechanism <rand>
2099 so an automatic keyword recommendation mechanism is <rand>
2100 so an automatic keyword recommendation mechanism is needed <rand>
2101 so an automatic keyword recommendation mechanism is needed to <rand>
2102 so an automatic keyword recommendation mechanism is needed to faster <rand>
2103 so an automatic keyword recommendation mechanism is needed to faster the <rand>
2104 so an automatic keyword recommendation mechanism is needed to faster the process <rand>
2105 so an automatic keyword recommendation mechanism is needed to faster the process of <rand>
2106 so an automatic keyword recommendation mechanism is needed to faster the process of making <rand>
2107 so an automatic keyword recommendation mechanism is needed to faster the process of making items <rand>
2108 so an automatic keyword recommendation mechanism is needed to faster the process of making items of <rand>
2109 so an automatic keyword recommendation mechanism is needed to faster the process of making items of books <rand>
2110 <rand>
2111 we <rand>
2112 we did <rand>
2113 we did not <rand>
2114 we did not use <rand>
2115 we did not use the <rand>
2116 we did not use the user <rand>
2117 we did not use the user study <rand>
2118 we did not use the user study valuation <rand>
2119 we did not use the user study valuation method <rand>
2120 we did not use the user study valuation method for <rand>
2121 we did not use the user study valuation method for that <rand>
2122 we did not use the user study valuation method for that we <rand>
2123 we did not use the user study valuation method for that we have <rand>
2124 we did not use the user study valuation method for that we have enough <rand>
2125 we did not use the user study valuation method for that we have enough annotated <rand>
2126 we did not use the user study valuation method for that we have enough annotated items <rand>
2127 we did not use the user study valuation method for that we have enough annotated items to <rand>
2128 we did not use the user study valuation method for that we have enough annotated items to test <rand>
2129 we did not use the user study valuation method for that we have enough annotated items to test and <rand>
2130 we did not use the user study valuation method for that we have enough annotated items to test and the <rand>
2131 we did not use the user study valuation method for that we have enough annotated items to test and the annotated <rand>
2132 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items <rand>
2133 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were <rand>
2134 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated <rand>
2135 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by <rand>
2136 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert <rand>
2137 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors <rand>
2138 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who <rand>
2139 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have <rand>
2140 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough <rand>
2141 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority <rand>
2142 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in <rand>
2143 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging <rand>
2144 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work <rand>
2145 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work , <rand>
2146 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work , and <rand>
2147 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work , and it <rand>
2148 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work , and it also <rand>
2149 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work , and it also saves <rand>
2150 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work , and it also saves lots <rand>
2151 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work , and it also saves lots of <rand>
2152 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work , and it also saves lots of time <rand>
2153 <rand>
2154 but <rand>
2155 but it <rand>
2156 but it does <rand>
2157 but it does not <rand>
2158 but it does not means <rand>
2159 but it does not means that <rand>
2160 but it does not means that we <rand>
2161 but it does not means that we would <rand>
2162 but it does not means that we would abandon <rand>
2163 but it does not means that we would abandon the <rand>
2164 but it does not means that we would abandon the extraction <rand>
2165 but it does not means that we would abandon the extraction method <rand>
2166 but it does not means that we would abandon the extraction method because <rand>
2167 but it does not means that we would abandon the extraction method because there <rand>
2168 but it does not means that we would abandon the extraction method because there are <rand>
2169 but it does not means that we would abandon the extraction method because there are cases <rand>
2170 but it does not means that we would abandon the extraction method because there are cases that <rand>
2171 but it does not means that we would abandon the extraction method because there are cases that the <rand>
2172 but it does not means that we would abandon the extraction method because there are cases that the coming <rand>
2173 but it does not means that we would abandon the extraction method because there are cases that the coming new <rand>
2174 but it does not means that we would abandon the extraction method because there are cases that the coming new item <rand>
2175 but it does not means that we would abandon the extraction method because there are cases that the coming new item is <rand>
2176 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite <rand>
2177 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different <rand>
2178 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from <rand>
2179 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the <rand>
2180 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training <rand>
2181 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set <rand>
2182 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and <rand>
2183 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the <rand>
2184 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended <rand>
2185 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords <rand>
2186 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from <rand>
2187 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical <rand>
2188 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information <rand>
2189 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may <rand>
2190 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not <rand>
2191 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not cover <rand>
2192 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not cover the <rand>
2193 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not cover the main <rand>
2194 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not cover the main idea <rand>
2195 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not cover the main idea of <rand>
2196 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not cover the main idea of the <rand>
2197 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not cover the main idea of the item <rand>
2198 <rand>
2199 however <rand>
2200 however , <rand>
2201 however , traditional <rand>
2202 however , traditional ner <rand>
2203 however , traditional ner systems <rand>
2204 however , traditional ner systems are <rand>
2205 however , traditional ner systems are mainly <rand>
2206 however , traditional ner systems are mainly based <rand>
2207 however , traditional ner systems are mainly based on <rand>
2208 however , traditional ner systems are mainly based on complex <rand>
2209 however , traditional ner systems are mainly based on complex hand-designed <rand>
2210 however , traditional ner systems are mainly based on complex hand-designed features <rand>
2211 however , traditional ner systems are mainly based on complex hand-designed features which <rand>
2212 however , traditional ner systems are mainly based on complex hand-designed features which are <rand>
2213 however , traditional ner systems are mainly based on complex hand-designed features which are derived <rand>
2214 however , traditional ner systems are mainly based on complex hand-designed features which are derived from <rand>
2215 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various <rand>
2216 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic <rand>
2217 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses <rand>
2218 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses and <rand>
2219 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses and maybe <rand>
2220 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses and maybe only <rand>
2221 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses and maybe only adapted <rand>
2222 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses and maybe only adapted to <rand>
2223 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses and maybe only adapted to specified <rand>
2224 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses and maybe only adapted to specified area <rand>
2225 <rand>
2226 as <rand>
2227 as the <rand>
2228 as the shallow <rand>
2229 as the shallow machine <rand>
2230 as the shallow machine learning <rand>
2231 as the shallow machine learning methods <rand>
2232 as the shallow machine learning methods described <rand>
2233 as the shallow machine learning methods described above <rand>
2234 as the shallow machine learning methods described above have <rand>
2235 as the shallow machine learning methods described above have strong <rand>
2236 as the shallow machine learning methods described above have strong dependency <rand>
2237 as the shallow machine learning methods described above have strong dependency on <rand>
2238 as the shallow machine learning methods described above have strong dependency on the <rand>
2239 as the shallow machine learning methods described above have strong dependency on the artificial <rand>
2240 as the shallow machine learning methods described above have strong dependency on the artificial features <rand>
2241 as the shallow machine learning methods described above have strong dependency on the artificial features and <rand>
2242 as the shallow machine learning methods described above have strong dependency on the artificial features and are <rand>
2243 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard <rand>
2244 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to <rand>
2245 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent <rand>
2246 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the <rand>
2247 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex <rand>
2248 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models <rand>
2249 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , <rand>
2250 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep <rand>
2251 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning <rand>
2252 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has <rand>
2253 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has been <rand>
2254 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has been applied <rand>
2255 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has been applied on <rand>
2256 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has been applied on ner <rand>
2257 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has been applied on ner in <rand>
2258 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has been applied on ner in recent <rand>
2259 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has been applied on ner in recent years <rand>
2260 <rand>
2261 the <rand>
2262 the outputs <rand>
2263 the outputs of <rand>
2264 the outputs of hidden <rand>
2265 the outputs of hidden layer <rand>
2266 the outputs of hidden layer and <rand>
2267 the outputs of hidden layer and output <rand>
2268 the outputs of hidden layer and output layer <rand>
2269 the outputs of hidden layer and output layer are <rand>
2270 the outputs of hidden layer and output layer are saved <rand>
2271 the outputs of hidden layer and output layer are saved and <rand>
2272 the outputs of hidden layer and output layer are saved and inputted <rand>
2273 the outputs of hidden layer and output layer are saved and inputted into <rand>
2274 the outputs of hidden layer and output layer are saved and inputted into the <rand>
2275 the outputs of hidden layer and output layer are saved and inputted into the next <rand>
2276 the outputs of hidden layer and output layer are saved and inputted into the next node <rand>
2277 <rand>
2278 secondly <rand>
2279 secondly , <rand>
2280 secondly , the <rand>
2281 secondly , the affective <rand>
2282 secondly , the affective features <rand>
2283 secondly , the affective features , <rand>
2284 secondly , the affective features , which <rand>
2285 secondly , the affective features , which are <rand>
2286 secondly , the affective features , which are extracted <rand>
2287 secondly , the affective features , which are extracted with <rand>
2288 secondly , the affective features , which are extracted with the <rand>
2289 secondly , the affective features , which are extracted with the sentiment <rand>
2290 secondly , the affective features , which are extracted with the sentiment lexicon <rand>
2291 secondly , the affective features , which are extracted with the sentiment lexicon , <rand>
2292 secondly , the affective features , which are extracted with the sentiment lexicon , are <rand>
2293 secondly , the affective features , which are extracted with the sentiment lexicon , are merge <rand>
2294 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into <rand>
2295 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature <rand>
2296 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters <rand>
2297 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based <rand>
2298 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on <rand>
2299 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the <rand>
2300 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method <rand>
2301 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of <rand>
2302 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing <rand>
2303 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the <rand>
2304 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word <rand>
2305 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity <rand>
2306 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with <rand>
2307 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the <rand>
2308 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term <rand>
2309 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector <rand>
2310 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , <rand>
2311 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and <rand>
2312 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then <rand>
2313 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these <rand>
2314 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature <rand>
2315 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters <rand>
2316 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are <rand>
2317 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are used <rand>
2318 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are used construct <rand>
2319 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are used construct the <rand>
2320 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are used construct the text <rand>
2321 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are used construct the text vector <rand>
2322 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are used construct the text vector with <rand>
2323 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are used construct the text vector with low-dimension <rand>
2324 <rand>
2325 we <rand>
2326 we build <rand>
2327 we build sentence <rand>
2328 we build sentence weights <rand>
2329 we build sentence weights model <rand>
2330 we build sentence weights model by <rand>
2331 we build sentence weights model by word <rand>
2332 we build sentence weights model by word distribution <rand>
2333 we build sentence weights model by word distribution which <rand>
2334 we build sentence weights model by word distribution which make <rand>
2335 we build sentence weights model by word distribution which make the <rand>
2336 we build sentence weights model by word distribution which make the calculation <rand>
2337 we build sentence weights model by word distribution which make the calculation efficient <rand>
2338 we build sentence weights model by word distribution which make the calculation efficient and <rand>
2339 we build sentence weights model by word distribution which make the calculation efficient and simple <rand>
2340 <rand>
2341 our <rand>
2342 our likelihood <rand>
2343 our likelihood estimation <rand>
2344 our likelihood estimation of <rand>
2345 our likelihood estimation of weights <rand>
2346 our likelihood estimation of weights shows <rand>
2347 our likelihood estimation of weights shows as <rand>
2348 our likelihood estimation of weights shows as formula <rand>
2349 our likelihood estimation of weights shows as formula 2 <rand>
2350 <rand>
2351 and <rand>
2352 and the <rand>
2353 and the meaning <rand>
2354 and the meaning of <rand>
2355 and the meaning of the <rand>
2356 and the meaning of the translation <rand>
2357 and the meaning of the translation of <rand>
2358 and the meaning of the translation of our <rand>
2359 and the meaning of the translation of our system <rand>
2360 and the meaning of the translation of our system is <rand>
2361 and the meaning of the translation of our system is the <rand>
2362 and the meaning of the translation of our system is the same <rand>
2363 and the meaning of the translation of our system is the same to <rand>
2364 and the meaning of the translation of our system is the same to that <rand>
2365 and the meaning of the translation of our system is the same to that of <rand>
2366 and the meaning of the translation of our system is the same to that of the <rand>
2367 and the meaning of the translation of our system is the same to that of the news <rand>
2368 and the meaning of the translation of our system is the same to that of the news domain <rand>
2369 and the meaning of the translation of our system is the same to that of the news domain translation <rand>
2370 and the meaning of the translation of our system is the same to that of the news domain translation system <rand>
2371 <rand>
2372 in <rand>
2373 in the <rand>
2374 in the sentence <rand>
2375 in the sentence 1 <rand>
2376 in the sentence 1 , <rand>
2377 in the sentence 1 , the <rand>
2378 in the sentence 1 , the meaning <rand>
2379 in the sentence 1 , the meaning of <rand>
2380 in the sentence 1 , the meaning of source <rand>
2381 in the sentence 1 , the meaning of source sentence <rand>
2382 in the sentence 1 , the meaning of source sentence is <rand>
2383 in the sentence 1 , the meaning of source sentence is that <rand>
2384 in the sentence 1 , the meaning of source sentence is that the <rand>
2385 in the sentence 1 , the meaning of source sentence is that the economic <rand>
2386 in the sentence 1 , the meaning of source sentence is that the economic growth <rand>
2387 in the sentence 1 , the meaning of source sentence is that the economic growth may <rand>
2388 in the sentence 1 , the meaning of source sentence is that the economic growth may slow <rand>
2389 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down <rand>
2390 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , <rand>
2391 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so <rand>
2392 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , <rand>
2393 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the <rand>
2394 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation <rand>
2395 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of <rand>
2396 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of baseline <rand>
2397 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of baseline is <rand>
2398 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of baseline is wrong <rand>
2399 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of baseline is wrong and <rand>
2400 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of baseline is wrong and ours <rand>
2401 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of baseline is wrong and ours is <rand>
2402 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of baseline is wrong and ours is accurately <rand>
2403 <rand>
2404 in <rand>
2405 in order <rand>
2406 in order to <rand>
2407 in order to better <rand>
2408 in order to better describe <rand>
2409 in order to better describe the <rand>
2410 in order to better describe the corresponding <rand>
2411 in order to better describe the corresponding relationship <rand>
2412 in order to better describe the corresponding relationship between <rand>
2413 in order to better describe the corresponding relationship between the <rand>
2414 in order to better describe the corresponding relationship between the source <rand>
2415 in order to better describe the corresponding relationship between the source domain <rand>
2416 in order to better describe the corresponding relationship between the source domain and <rand>
2417 in order to better describe the corresponding relationship between the source domain and the <rand>
2418 in order to better describe the corresponding relationship between the source domain and the attributes <rand>
2419 in order to better describe the corresponding relationship between the source domain and the attributes , <rand>
2420 in order to better describe the corresponding relationship between the source domain and the attributes , the <rand>
2421 in order to better describe the corresponding relationship between the source domain and the attributes , the paper <rand>
2422 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists <rand>
2423 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all <rand>
2424 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the <rand>
2425 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source <rand>
2426 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source domains <rand>
2427 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source domains , <rand>
2428 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source domains , mapped <rand>
2429 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source domains , mapped more <rand>
2430 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source domains , mapped more than <rand>
2431 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source domains , mapped more than 10 <rand>
2432 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source domains , mapped more than 10 times <rand>
2433 <rand>
2434 meanwhile <rand>
2435 meanwhile , <rand>
2436 meanwhile , the <rand>
2437 meanwhile , the ranking <rand>
2438 meanwhile , the ranking of <rand>
2439 meanwhile , the ranking of the <rand>
2440 meanwhile , the ranking of the 5 <rand>
2441 meanwhile , the ranking of the 5 categories <rand>
2442 meanwhile , the ranking of the 5 categories has <rand>
2443 meanwhile , the ranking of the 5 categories has also <rand>
2444 meanwhile , the ranking of the 5 categories has also changed <rand>
2445 meanwhile , the ranking of the 5 categories has also changed , <rand>
2446 meanwhile , the ranking of the 5 categories has also changed , the <rand>
2447 meanwhile , the ranking of the 5 categories has also changed , the source <rand>
2448 meanwhile , the ranking of the 5 categories has also changed , the source domain <rand>
2449 meanwhile , the ranking of the 5 categories has also changed , the source domain of <rand>
2450 meanwhile , the ranking of the 5 categories has also changed , the source domain of the <rand>
2451 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second <rand>
2452 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place <rand>
2453 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ <rand>
2454 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people <rand>
2455 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply <rand>
2456 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined <rand>
2457 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , <rand>
2458 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing <rand>
2459 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only <rand>
2460 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 <rand>
2461 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words <rand>
2462 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and <rand>
2463 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked <rand>
2464 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third <rand>
2465 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , <rand>
2466 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while <rand>
2467 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the <rand>
2468 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number <rand>
2469 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of <rand>
2470 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of d_ <rand>
2471 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of d_ abstract <rand>
2472 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of d_ abstract things <rand>
2473 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of d_ abstract things arranged <rand>
2474 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of d_ abstract things arranged in <rand>
2475 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of d_ abstract things arranged in second <rand>
2476 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of d_ abstract things arranged in second place <rand>
2477 <rand>
2478 im <rand>
2479 im reminded <rand>
2480 im reminded of <rand>
2481 im reminded of a <rand>
2482 im reminded of a counselor <rand>
2483 im reminded of a counselor who <rand>
2484 im reminded of a counselor who would <rand>
2485 im reminded of a counselor who would often <rand>
2486 im reminded of a counselor who would often state <rand>
2487 im reminded of a counselor who would often state no <rand>
2488 im reminded of a counselor who would often state no one <rand>
2489 im reminded of a counselor who would often state no one can <rand>
2490 im reminded of a counselor who would often state no one can drive <rand>
2491 im reminded of a counselor who would often state no one can drive your <rand>
2492 im reminded of a counselor who would often state no one can drive your car <rand>
2493 im reminded of a counselor who would often state no one can drive your car unless <rand>
2494 im reminded of a counselor who would often state no one can drive your car unless you <rand>
2495 im reminded of a counselor who would often state no one can drive your car unless you give <rand>
2496 im reminded of a counselor who would often state no one can drive your car unless you give them <rand>
2497 im reminded of a counselor who would often state no one can drive your car unless you give them the <rand>
2498 im reminded of a counselor who would often state no one can drive your car unless you give them the keys <rand>
2499 <rand>
2500 you <rand>
2501 you can <rand>
2502 you can not <rand>
2503 you can not control <rand>
2504 you can not control others <rand>
2505 you can not control others actions <rand>
2506 you can not control others actions , <rand>
2507 you can not control others actions , but <rand>
2508 you can not control others actions , but you <rand>
2509 you can not control others actions , but you can <rand>
2510 you can not control others actions , but you can be <rand>
2511 you can not control others actions , but you can be responsible <rand>
2512 you can not control others actions , but you can be responsible for <rand>
2513 you can not control others actions , but you can be responsible for your <rand>
2514 you can not control others actions , but you can be responsible for your reactions <rand>
2515 <rand>
2516 for <rand>
2517 for every <rand>
2518 for every candidate <rand>
2519 for every candidate answer <rand>
2520 for every candidate answer of <rand>
2521 for every candidate answer of the <rand>
2522 for every candidate answer of the question <rand>
2523 for every candidate answer of the question we <rand>
2524 for every candidate answer of the question we are <rand>
2525 for every candidate answer of the question we are considering <rand>
2526 for every candidate answer of the question we are considering , <rand>
2527 for every candidate answer of the question we are considering , we <rand>
2528 for every candidate answer of the question we are considering , we score <rand>
2529 for every candidate answer of the question we are considering , we score it <rand>
2530 for every candidate answer of the question we are considering , we score it base <rand>
2531 for every candidate answer of the question we are considering , we score it base on <rand>
2532 for every candidate answer of the question we are considering , we score it base on its <rand>
2533 for every candidate answer of the question we are considering , we score it base on its content <rand>
2534 for every candidate answer of the question we are considering , we score it base on its content and <rand>
2535 for every candidate answer of the question we are considering , we score it base on its content and structure <rand>
2536 for every candidate answer of the question we are considering , we score it base on its content and structure similarity <rand>
2537 for every candidate answer of the question we are considering , we score it base on its content and structure similarity with <rand>
2538 for every candidate answer of the question we are considering , we score it base on its content and structure similarity with the <rand>
2539 for every candidate answer of the question we are considering , we score it base on its content and structure similarity with the support <rand>
2540 for every candidate answer of the question we are considering , we score it base on its content and structure similarity with the support sets <rand>
2541 <rand>
2542 notice <rand>
2543 notice that <rand>
2544 notice that , <rand>
2545 notice that , a <rand>
2546 notice that , a smaller <rand>
2547 notice that , a smaller distance <rand>
2548 notice that , a smaller distance means <rand>
2549 notice that , a smaller distance means the <rand>
2550 notice that , a smaller distance means the feature <rand>
2551 notice that , a smaller distance means the feature of <rand>
2552 notice that , a smaller distance means the feature of the <rand>
2553 notice that , a smaller distance means the feature of the candidate <rand>
2554 notice that , a smaller distance means the feature of the candidate answer <rand>
2555 notice that , a smaller distance means the feature of the candidate answer is <rand>
2556 notice that , a smaller distance means the feature of the candidate answer is more <rand>
2557 notice that , a smaller distance means the feature of the candidate answer is more close <rand>
2558 notice that , a smaller distance means the feature of the candidate answer is more close with <rand>
2559 notice that , a smaller distance means the feature of the candidate answer is more close with the <rand>
2560 notice that , a smaller distance means the feature of the candidate answer is more close with the representative <rand>
2561 <rand>
2562 so <rand>
2563 so far <rand>
2564 so far , <rand>
2565 so far , we <rand>
2566 so far , we made <rand>
2567 so far , we made some <rand>
2568 so far , we made some rules <rand>
2569 so far , we made some rules solve <rand>
2570 so far , we made some rules solve this <rand>
2571 so far , we made some rules solve this reasoning <rand>
2572 so far , we made some rules solve this reasoning problem <rand>
2573 <rand>
2574 the <rand>
2575 the evaluation <rand>
2576 the evaluation of <rand>
2577 the evaluation of the <rand>
2578 the evaluation of the system <rand>
2579 the evaluation of the system is <rand>
2580 the evaluation of the system is performed <rand>
2581 the evaluation of the system is performed by <rand>
2582 the evaluation of the system is performed by applying <rand>
2583 the evaluation of the system is performed by applying it <rand>
2584 the evaluation of the system is performed by applying it on <rand>
2585 the evaluation of the system is performed by applying it on recognizing <rand>
2586 the evaluation of the system is performed by applying it on recognizing entailment <rand>
2587 the evaluation of the system is performed by applying it on recognizing entailment relation <rand>
2588 the evaluation of the system is performed by applying it on recognizing entailment relation of <rand>
2589 the evaluation of the system is performed by applying it on recognizing entailment relation of test <rand>
2590 the evaluation of the system is performed by applying it on recognizing entailment relation of test text <rand>
2591 the evaluation of the system is performed by applying it on recognizing entailment relation of test text pairs <rand>
2592 <rand>
2593 it <rand>
2594 it seeks <rand>
2595 it seeks to <rand>
2596 it seeks to generalize <rand>
2597 it seeks to generalize a <rand>
2598 it seeks to generalize a model <rand>
2599 it seeks to generalize a model , <rand>
2600 it seeks to generalize a model , which <rand>
2601 it seeks to generalize a model , which is <rand>
2602 it seeks to generalize a model , which is trained <rand>
2603 it seeks to generalize a model , which is trained on <rand>
2604 it seeks to generalize a model , which is trained on a <rand>
2605 it seeks to generalize a model , which is trained on a source <rand>
2606 it seeks to generalize a model , which is trained on a source domain <rand>
2607 it seeks to generalize a model , which is trained on a source domain and <rand>
2608 it seeks to generalize a model , which is trained on a source domain and using <rand>
2609 it seeks to generalize a model , which is trained on a source domain and using it <rand>
2610 it seeks to generalize a model , which is trained on a source domain and using it to <rand>
2611 it seeks to generalize a model , which is trained on a source domain and using it to label <rand>
2612 it seeks to generalize a model , which is trained on a source domain and using it to label samples <rand>
2613 it seeks to generalize a model , which is trained on a source domain and using it to label samples in <rand>
2614 it seeks to generalize a model , which is trained on a source domain and using it to label samples in the <rand>
2615 it seeks to generalize a model , which is trained on a source domain and using it to label samples in the target <rand>
2616 it seeks to generalize a model , which is trained on a source domain and using it to label samples in the target domain <rand>
2617 <rand>
2618 moreover <rand>
2619 moreover , <rand>
2620 moreover , ss-lda <rand>
2621 moreover , ss-lda bases <rand>
2622 moreover , ss-lda bases on <rand>
2623 moreover , ss-lda bases on sentence <rand>
2624 moreover , ss-lda bases on sentence level <rand>
2625 moreover , ss-lda bases on sentence level while <rand>
2626 moreover , ss-lda bases on sentence level while jst <rand>
2627 moreover , ss-lda bases on sentence level while jst depends <rand>
2628 moreover , ss-lda bases on sentence level while jst depends on <rand>
2629 moreover , ss-lda bases on sentence level while jst depends on document <rand>
2630 moreover , ss-lda bases on sentence level while jst depends on document level <rand>
2631 <rand>
2632 furthermore <rand>
2633 furthermore , <rand>
2634 furthermore , prior <rand>
2635 furthermore , prior knowledge <rand>
2636 furthermore , prior knowledge that <rand>
2637 furthermore , prior knowledge that obtained <rand>
2638 furthermore , prior knowledge that obtained from <rand>
2639 furthermore , prior knowledge that obtained from sentiment <rand>
2640 furthermore , prior knowledge that obtained from sentiment lexicons <rand>
2641 furthermore , prior knowledge that obtained from sentiment lexicons is <rand>
2642 furthermore , prior knowledge that obtained from sentiment lexicons is utilized <rand>
2643 furthermore , prior knowledge that obtained from sentiment lexicons is utilized during <rand>
2644 furthermore , prior knowledge that obtained from sentiment lexicons is utilized during the <rand>
2645 furthermore , prior knowledge that obtained from sentiment lexicons is utilized during the initialization <rand>
2646 furthermore , prior knowledge that obtained from sentiment lexicons is utilized during the initialization for <rand>
2647 furthermore , prior knowledge that obtained from sentiment lexicons is utilized during the initialization for gibbs <rand>
2648 furthermore , prior knowledge that obtained from sentiment lexicons is utilized during the initialization for gibbs sampling <rand>
2649 <rand>
2650 my <rand>
2651 my friends <rand>
2652 my friends were <rand>
2653 my friends were disappointed <rand>
2654 my friends were disappointed to <rand>
2655 my friends were disappointed to the <rand>
2656 my friends were disappointed to the sound <rand>
2657 <rand>
2658 besides <rand>
2659 besides , <rand>
2660 besides , in <rand>
2661 besides , in order <rand>
2662 besides , in order to <rand>
2663 besides , in order to improving <rand>
2664 besides , in order to improving the <rand>
2665 besides , in order to improving the sentiment <rand>
2666 besides , in order to improving the sentiment detection <rand>
2667 besides , in order to improving the sentiment detection , <rand>
2668 besides , in order to improving the sentiment detection , prior <rand>
2669 besides , in order to improving the sentiment detection , prior knowledge <rand>
2670 besides , in order to improving the sentiment detection , prior knowledge is <rand>
2671 besides , in order to improving the sentiment detection , prior knowledge is incorporated <rand>
2672 besides , in order to improving the sentiment detection , prior knowledge is incorporated during <rand>
2673 besides , in order to improving the sentiment detection , prior knowledge is incorporated during the <rand>
2674 besides , in order to improving the sentiment detection , prior knowledge is incorporated during the initialization <rand>
2675 besides , in order to improving the sentiment detection , prior knowledge is incorporated during the initialization of <rand>
2676 besides , in order to improving the sentiment detection , prior knowledge is incorporated during the initialization of gibbs <rand>
2677 besides , in order to improving the sentiment detection , prior knowledge is incorporated during the initialization of gibbs sampling <rand>
2678 <rand>
2679 in <rand>
2680 in summary <rand>
2681 in summary , <rand>
2682 in summary , filtering <rand>
2683 in summary , filtering the <rand>
2684 in summary , filtering the sentences <rand>
2685 in summary , filtering the sentences whose <rand>
2686 in summary , filtering the sentences whose polarities <rand>
2687 in summary , filtering the sentences whose polarities opposite <rand>
2688 in summary , filtering the sentences whose polarities opposite to <rand>
2689 in summary , filtering the sentences whose polarities opposite to the <rand>
2690 in summary , filtering the sentences whose polarities opposite to the overall <rand>
2691 in summary , filtering the sentences whose polarities opposite to the overall orientation <rand>
2692 in summary , filtering the sentences whose polarities opposite to the overall orientation is <rand>
2693 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant <rand>
2694 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant for <rand>
2695 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant for constructing <rand>
2696 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant for constructing a <rand>
2697 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant for constructing a high <rand>
2698 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant for constructing a high quality <rand>
2699 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant for constructing a high quality training <rand>
2700 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant for constructing a high quality training set <rand>
2701 <rand>
2702 that <rand>
2703 that is <rand>
2704 that is to <rand>
2705 that is to say <rand>
2706 that is to say , <rand>
2707 that is to say , the <rand>
2708 that is to say , the classification <rand>
2709 that is to say , the classification model <rand>
2710 that is to say , the classification model might <rand>
2711 that is to say , the classification model might not <rand>
2712 that is to say , the classification model might not accurate <rand>
2713 that is to say , the classification model might not accurate when <rand>
2714 that is to say , the classification model might not accurate when applying <rand>
2715 that is to say , the classification model might not accurate when applying these <rand>
2716 that is to say , the classification model might not accurate when applying these samples <rand>
2717 that is to say , the classification model might not accurate when applying these samples for <rand>
2718 that is to say , the classification model might not accurate when applying these samples for training <rand>
2719 that is to say , the classification model might not accurate when applying these samples for training directly <rand>
2720 <rand>
2721 by <rand>
2722 by ss-lda <rand>
2723 by ss-lda filtering <rand>
2724 by ss-lda filtering , <rand>
2725 by ss-lda filtering , we <rand>
2726 by ss-lda filtering , we move <rand>
2727 by ss-lda filtering , we move the <rand>
2728 by ss-lda filtering , we move the sentences <rand>
2729 by ss-lda filtering , we move the sentences whose <rand>
2730 by ss-lda filtering , we move the sentences whose sentimental <rand>
2731 by ss-lda filtering , we move the sentences whose sentimental polarity <rand>
2732 by ss-lda filtering , we move the sentences whose sentimental polarity strongly <rand>
2733 by ss-lda filtering , we move the sentences whose sentimental polarity strongly opposites <rand>
2734 by ss-lda filtering , we move the sentences whose sentimental polarity strongly opposites the <rand>
2735 by ss-lda filtering , we move the sentences whose sentimental polarity strongly opposites the overall <rand>
2736 by ss-lda filtering , we move the sentences whose sentimental polarity strongly opposites the overall orientation <rand>
2737 <rand>
2738 this <rand>
2739 this article <rand>
2740 this article firstly <rand>
2741 this article firstly defined <rand>
2742 this article firstly defined the <rand>
2743 this article firstly defined the phenomenon <rand>
2744 this article firstly defined the phenomenon and <rand>
2745 this article firstly defined the phenomenon and explained <rand>
2746 this article firstly defined the phenomenon and explained related <rand>
2747 this article firstly defined the phenomenon and explained related concepts <rand>
2748 this article firstly defined the phenomenon and explained related concepts , <rand>
2749 this article firstly defined the phenomenon and explained related concepts , than <rand>
2750 this article firstly defined the phenomenon and explained related concepts , than demonstrated <rand>
2751 this article firstly defined the phenomenon and explained related concepts , than demonstrated this <rand>
2752 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon <rand>
2753 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by <rand>
2754 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting <rand>
2755 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling <rand>
2756 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style <rand>
2757 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of <rand>
2758 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example <rand>
2759 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word <rand>
2760 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , <rand>
2761 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw <rand>
2762 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus <rand>
2763 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus statistics <rand>
2764 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus statistics and <rand>
2765 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus statistics and whole <rand>
2766 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus statistics and whole text <rand>
2767 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus statistics and whole text annotation <rand>
2768 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus statistics and whole text annotation statistics <rand>
2769 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus statistics and whole text annotation statistics etc <rand>
2770 <rand>
2771 finally <rand>
2772 finally , <rand>
2773 finally , we <rand>
2774 finally , we promote <rand>
2775 finally , we promote serious <rand>
2776 finally , we promote serious of <rand>
2777 finally , we promote serious of methods <rand>
2778 finally , we promote serious of methods to <rand>
2779 finally , we promote serious of methods to solve <rand>
2780 finally , we promote serious of methods to solve msdp <rand>
2781 finally , we promote serious of methods to solve msdp , <rand>
2782 finally , we promote serious of methods to solve msdp , it <rand>
2783 finally , we promote serious of methods to solve msdp , it includes <rand>
2784 finally , we promote serious of methods to solve msdp , it includes popularization <rand>
2785 finally , we promote serious of methods to solve msdp , it includes popularization of <rand>
2786 finally , we promote serious of methods to solve msdp , it includes popularization of standardized <rand>
2787 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input <rand>
2788 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by <rand>
2789 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise <rand>
2790 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness <rand>
2791 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of <rand>
2792 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the <rand>
2793 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user <rand>
2794 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , <rand>
2795 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using <rand>
2796 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent <rand>
2797 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime <rand>
2798 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to <rand>
2799 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid <rand>
2800 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input <rand>
2801 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake <rand>
2802 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , <rand>
2803 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using <rand>
2804 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the <rand>
2805 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling <rand>
2806 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error <rand>
2807 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction <rand>
2808 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools <rand>
2809 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , <rand>
2810 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration <rand>
2811 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of <rand>
2812 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of raw <rand>
2813 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of raw corpus <rand>
2814 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of raw corpus based <rand>
2815 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of raw corpus based statistical <rand>
2816 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of raw corpus based statistical learning <rand>
2817 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of raw corpus based statistical learning methods <rand>
2818 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of raw corpus based statistical learning methods etc <rand>
