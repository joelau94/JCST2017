0 it
1 it shows
2 it shows that
3 it shows that the
4 it shows that the performance
5 it shows that the performance of
6 it shows that the performance of models
7 it shows that the performance of models that
8 it shows that the performance of models that consider
9 it shows that the performance of models that consider the
10 it shows that the performance of models that consider the relation
11 it shows that the performance of models that consider the relation between
12 it shows that the performance of models that consider the relation between labels
13 it shows that the performance of models that consider the relation between labels is
14 it shows that the performance of models that consider the relation between labels is better
15 it shows that the performance of models that consider the relation between labels is better than
16 it shows that the performance of models that consider the relation between labels is better than that
17 it shows that the performance of models that consider the relation between labels is better than that without
18 it shows that the performance of models that consider the relation between labels is better than that without considering
19 it shows that the performance of models that consider the relation between labels is better than that without considering the
20 it shows that the performance of models that consider the relation between labels is better than that without considering the relations
21 it shows that the performance of models that consider the relation between labels is better than that without considering the relations between
22 it shows that the performance of models that consider the relation between labels is better than that without considering the relations between labels
23 it shows that the performance of models that consider the relation between labels is better than that without considering the relations between labels .
24 furthermore
25 furthermore ,
26 furthermore , we
27 furthermore , we can
28 furthermore , we can also
29 furthermore , we can also see
30 furthermore , we can also see that
31 furthermore , we can also see that the
32 furthermore , we can also see that the perplexities
33 furthermore , we can also see that the perplexities of
34 furthermore , we can also see that the perplexities of ehllda
35 furthermore , we can also see that the perplexities of ehllda are
36 furthermore , we can also see that the perplexities of ehllda are lower
37 furthermore , we can also see that the perplexities of ehllda are lower than
38 furthermore , we can also see that the perplexities of ehllda are lower than that
39 furthermore , we can also see that the perplexities of ehllda are lower than that of
40 furthermore , we can also see that the perplexities of ehllda are lower than that of hllda
41 furthermore , we can also see that the perplexities of ehllda are lower than that of hllda over
42 furthermore , we can also see that the perplexities of ehllda are lower than that of hllda over all
43 furthermore , we can also see that the perplexities of ehllda are lower than that of hllda over all four
44 furthermore , we can also see that the perplexities of ehllda are lower than that of hllda over all four datasets
45 furthermore , we can also see that the perplexities of ehllda are lower than that of hllda over all four datasets .
46 ehllda
47 ehllda ,
48 ehllda , which
49 ehllda , which incorporated
50 ehllda , which incorporated prior
51 ehllda , which incorporated prior information
52 ehllda , which incorporated prior information of
53 ehllda , which incorporated prior information of paths
54 ehllda , which incorporated prior information of paths and
55 ehllda , which incorporated prior information of paths and relaxed
56 ehllda , which incorporated prior information of paths and relaxed the
57 ehllda , which incorporated prior information of paths and relaxed the assumption
58 ehllda , which incorporated prior information of paths and relaxed the assumption of
59 ehllda , which incorporated prior information of paths and relaxed the assumption of hllda
60 ehllda , which incorporated prior information of paths and relaxed the assumption of hllda .
61 this
62 this paper
63 this paper presents
64 this paper presents a
65 this paper presents a construction
66 this paper presents a construction method
67 this paper presents a construction method that
68 this paper presents a construction method that utilize
69 this paper presents a construction method that utilize transition
70 this paper presents a construction method that utilize transition words
71 this paper presents a construction method that utilize transition words and
72 this paper presents a construction method that utilize transition words and negative
73 this paper presents a construction method that utilize transition words and negative words
74 this paper presents a construction method that utilize transition words and negative words to
75 this paper presents a construction method that utilize transition words and negative words to classify
76 this paper presents a construction method that utilize transition words and negative words to classify the
77 this paper presents a construction method that utilize transition words and negative words to classify the sentiment
78 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words
79 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in
80 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus
81 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus ,
82 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as
83 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these
84 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these words
85 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these words can
86 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these words can reverse
87 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these words can reverse the
88 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these words can reverse the polarity
89 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these words can reverse the polarity of
90 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these words can reverse the polarity of text
91 this paper presents a construction method that utilize transition words and negative words to classify the sentiment words in corpus , as these words can reverse the polarity of text .
92 parameters
93 parameters and
94 parameters and policy
95 parameters and policy of
96 parameters and policy of pomdp-based
97 parameters and policy of pomdp-based dms
98 parameters and policy of pomdp-based dms is
99 parameters and policy of pomdp-based dms is estimated
100 parameters and policy of pomdp-based dms is estimated from
101 parameters and policy of pomdp-based dms is estimated from data
102 parameters and policy of pomdp-based dms is estimated from data .
103 taking
104 taking the
105 taking the teach-and-learn
106 taking the teach-and-learn task
107 taking the teach-and-learn task for
108 taking the teach-and-learn task for example
109 taking the teach-and-learn task for example ,
110 taking the teach-and-learn task for example , even
111 taking the teach-and-learn task for example , even if
112 taking the teach-and-learn task for example , even if objects
113 taking the teach-and-learn task for example , even if objects mentioned
114 taking the teach-and-learn task for example , even if objects mentioned in
115 taking the teach-and-learn task for example , even if objects mentioned in this
116 taking the teach-and-learn task for example , even if objects mentioned in this task
117 taking the teach-and-learn task for example , even if objects mentioned in this task have
118 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized
119 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots
120 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots ,
121 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the
122 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible
123 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values
124 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of
125 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots
126 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots are
127 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots are infinite
128 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots are infinite because
129 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots are infinite because of
130 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots are infinite because of infinity
131 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots are infinite because of infinity of
132 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots are infinite because of infinity of objects
133 taking the teach-and-learn task for example , even if objects mentioned in this task have fixsized slots , the possible values of slots are infinite because of infinity of objects .
134 in
135 in this
136 in this way
137 in this way ,
138 in this way , dm
139 in this way , dm obtains
140 in this way , dm obtains more
141 in this way , dm obtains more information
142 in this way , dm obtains more information with
143 in this way , dm obtains more information with which
144 in this way , dm obtains more information with which it
145 in this way , dm obtains more information with which it can
146 in this way , dm obtains more information with which it can guide
147 in this way , dm obtains more information with which it can guide the
148 in this way , dm obtains more information with which it can guide the dialogue
149 in this way , dm obtains more information with which it can guide the dialogue in
150 in this way , dm obtains more information with which it can guide the dialogue in a
151 in this way , dm obtains more information with which it can guide the dialogue in a more
152 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate
153 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction
154 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when
155 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when 1-best
156 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when 1-best result
157 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when 1-best result of
158 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when 1-best result of speech
159 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when 1-best result of speech recognition
160 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when 1-best result of speech recognition is
161 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when 1-best result of speech recognition is inaccurate
162 in this way , dm obtains more information with which it can guide the dialogue in a more appropriate direction when 1-best result of speech recognition is inaccurate .
163 in
164 in slot-filling
165 in slot-filling tasks
166 in slot-filling tasks based
167 in slot-filling tasks based on
168 in slot-filling tasks based on sds-pomdp
169 in slot-filling tasks based on sds-pomdp ,
170 in slot-filling tasks based on sds-pomdp , set
171 in slot-filling tasks based on sds-pomdp , set of
172 in slot-filling tasks based on sds-pomdp , set of user
173 in slot-filling tasks based on sds-pomdp , set of user goals
174 in slot-filling tasks based on sds-pomdp , set of user goals su
175 in slot-filling tasks based on sds-pomdp , set of user goals su is
176 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian
177 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product
178 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of
179 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all
180 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots
181 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots ,
182 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots , which
183 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots , which needs
184 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots , which needs to
185 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots , which needs to be
186 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots , which needs to be defined
187 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots , which needs to be defined in
188 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots , which needs to be defined in advance
189 in slot-filling tasks based on sds-pomdp , set of user goals su is cartesian product of all slots , which needs to be defined in advance .
190 system
191 system performs
192 system performs language
193 system performs language understanding
194 system performs language understanding when
195 system performs language understanding when user
196 system performs language understanding when user inputs
197 system performs language understanding when user inputs an
198 system performs language understanding when user inputs an utterance
199 system performs language understanding when user inputs an utterance ,
200 system performs language understanding when user inputs an utterance , which
201 system performs language understanding when user inputs an utterance , which includes
202 system performs language understanding when user inputs an utterance , which includes intention
203 system performs language understanding when user inputs an utterance , which includes intention recognition
204 system performs language understanding when user inputs an utterance , which includes intention recognition and
205 system performs language understanding when user inputs an utterance , which includes intention recognition and slot
206 system performs language understanding when user inputs an utterance , which includes intention recognition and slot value
207 system performs language understanding when user inputs an utterance , which includes intention recognition and slot value extraction
208 system performs language understanding when user inputs an utterance , which includes intention recognition and slot value extraction .
209 in
210 in algorithm
211 in algorithm 1
212 in algorithm 1 ,
213 in algorithm 1 , we
214 in algorithm 1 , we give
215 in algorithm 1 , we give out
216 in algorithm 1 , we give out algorithm
217 in algorithm 1 , we give out algorithm for
218 in algorithm 1 , we give out algorithm for the
219 in algorithm 1 , we give out algorithm for the improved
220 in algorithm 1 , we give out algorithm for the improved sdspomdp
221 in algorithm 1 , we give out algorithm for the improved sdspomdp ,
222 in algorithm 1 , we give out algorithm for the improved sdspomdp , which
223 in algorithm 1 , we give out algorithm for the improved sdspomdp , which is
224 in algorithm 1 , we give out algorithm for the improved sdspomdp , which is consistent
225 in algorithm 1 , we give out algorithm for the improved sdspomdp , which is consistent to
226 in algorithm 1 , we give out algorithm for the improved sdspomdp , which is consistent to algorithm
227 in algorithm 1 , we give out algorithm for the improved sdspomdp , which is consistent to algorithm for
228 in algorithm 1 , we give out algorithm for the improved sdspomdp , which is consistent to algorithm for sds-pomdp
229 in algorithm 1 , we give out algorithm for the improved sdspomdp , which is consistent to algorithm for sds-pomdp .
230 our
231 our modification
232 our modification to
233 our modification to observation
234 our modification to observation model
235 our modification to observation model is
236 our modification to observation model is mainly
237 our modification to observation model is mainly reflected
238 our modification to observation model is mainly reflected in
239 our modification to observation model is mainly reflected in line
240 our modification to observation model is mainly reflected in line 6
241 our modification to observation model is mainly reflected in line 6 and
242 our modification to observation model is mainly reflected in line 6 and line
243 our modification to observation model is mainly reflected in line 6 and line 17
244 our modification to observation model is mainly reflected in line 6 and line 17 to
245 our modification to observation model is mainly reflected in line 6 and line 17 to 20
246 our modification to observation model is mainly reflected in line 6 and line 17 to 20 .
247 the
248 the procedure
249 the procedure of
250 the procedure of dynamic
251 the procedure of dynamic binding
252 the procedure of dynamic binding is
253 the procedure of dynamic binding is shown
254 the procedure of dynamic binding is shown between
255 the procedure of dynamic binding is shown between lines
256 the procedure of dynamic binding is shown between lines 7
257 the procedure of dynamic binding is shown between lines 7 to
258 the procedure of dynamic binding is shown between lines 7 to 16
259 the procedure of dynamic binding is shown between lines 7 to 16 .
260 results
261 results of
262 results of experiments
263 results of experiments indicate
264 results of experiments indicate that
265 results of experiments indicate that our
266 results of experiments indicate that our method
267 results of experiments indicate that our method can
268 results of experiments indicate that our method can effectively
269 results of experiments indicate that our method can effectively overcome
270 results of experiments indicate that our method can effectively overcome the
271 results of experiments indicate that our method can effectively overcome the infinite
272 results of experiments indicate that our method can effectively overcome the infinite teaching
273 results of experiments indicate that our method can effectively overcome the infinite teaching objects
274 results of experiments indicate that our method can effectively overcome the infinite teaching objects problem
275 results of experiments indicate that our method can effectively overcome the infinite teaching objects problem in
276 results of experiments indicate that our method can effectively overcome the infinite teaching objects problem in the
277 results of experiments indicate that our method can effectively overcome the infinite teaching objects problem in the teach-andlearn
278 results of experiments indicate that our method can effectively overcome the infinite teaching objects problem in the teach-andlearn task
279 results of experiments indicate that our method can effectively overcome the infinite teaching objects problem in the teach-andlearn task .
280 mongolian
281 mongolian is
282 mongolian is one
283 mongolian is one of
284 mongolian is one of the
285 mongolian is one of the less
286 mongolian is one of the less studied
287 mongolian is one of the less studied languages
288 mongolian is one of the less studied languages for
289 mongolian is one of the less studied languages for speech
290 mongolian is one of the less studied languages for speech recognition
291 mongolian is one of the less studied languages for speech recognition .
292 and
293 and the
294 and the feature
295 and the feature is
296 and the feature is same
297 and the feature is same as
298 and the feature is same as the
299 and the feature is same as the mono1
300 and the feature is same as the mono1 .
301 first
302 first ,
303 first , according
304 first , according to
305 first , according to how
306 first , according to how two
307 first , according to how two morpheme
308 first , according to how two morpheme meanings
309 first , according to how two morpheme meanings turn
310 first , according to how two morpheme meanings turn into
311 first , according to how two morpheme meanings turn into word
312 first , according to how two morpheme meanings turn into word meanings
313 first , according to how two morpheme meanings turn into word meanings by
314 first , according to how two morpheme meanings turn into word meanings by metonymy
315 first , according to how two morpheme meanings turn into word meanings by metonymy or
316 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor
317 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor ,
318 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the
319 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the undirected
320 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the undirected word
321 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the undirected word was
322 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the undirected word was classified
323 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the undirected word was classified into
324 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the undirected word was classified into 8
325 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the undirected word was classified into 8 categories
326 first , according to how two morpheme meanings turn into word meanings by metonymy or metaphor , the undirected word was classified into 8 categories .
327 digital
328 digital libraries
329 digital libraries suffer
330 digital libraries suffer from
331 digital libraries suffer from the
332 digital libraries suffer from the overload
333 digital libraries suffer from the overload problem
334 digital libraries suffer from the overload problem ,
335 digital libraries suffer from the overload problem , which
336 digital libraries suffer from the overload problem , which makes
337 digital libraries suffer from the overload problem , which makes the
338 digital libraries suffer from the overload problem , which makes the researchers
339 digital libraries suffer from the overload problem , which makes the researchers have
340 digital libraries suffer from the overload problem , which makes the researchers have to
341 digital libraries suffer from the overload problem , which makes the researchers have to spend
342 digital libraries suffer from the overload problem , which makes the researchers have to spend much
343 digital libraries suffer from the overload problem , which makes the researchers have to spend much time
344 digital libraries suffer from the overload problem , which makes the researchers have to spend much time to
345 digital libraries suffer from the overload problem , which makes the researchers have to spend much time to find
346 digital libraries suffer from the overload problem , which makes the researchers have to spend much time to find relevant
347 digital libraries suffer from the overload problem , which makes the researchers have to spend much time to find relevant papers
348 digital libraries suffer from the overload problem , which makes the researchers have to spend much time to find relevant papers .
349 previous
350 previous paper
351 previous paper recommendation
352 previous paper recommendation methods
353 previous paper recommendation methods are
354 previous paper recommendation methods are either
355 previous paper recommendation methods are either citation-based
356 previous paper recommendation methods are either citation-based or
357 previous paper recommendation methods are either citation-based or contentbased
358 previous paper recommendation methods are either citation-based or contentbased .
359 co-coupling
360 co-coupling occurred
361 co-coupling occurred when
362 co-coupling occurred when two
363 co-coupling occurred when two papers
364 co-coupling occurred when two papers referred
365 co-coupling occurred when two papers referred a
366 co-coupling occurred when two papers referred a common
367 co-coupling occurred when two papers referred a common third
368 co-coupling occurred when two papers referred a common third paper
369 co-coupling occurred when two papers referred a common third paper ,
370 co-coupling occurred when two papers referred a common third paper , then
371 co-coupling occurred when two papers referred a common third paper , then that
372 co-coupling occurred when two papers referred a common third paper , then that two
373 co-coupling occurred when two papers referred a common third paper , then that two papers
374 co-coupling occurred when two papers referred a common third paper , then that two papers were
375 co-coupling occurred when two papers referred a common third paper , then that two papers were co-coupling
376 co-coupling occurred when two papers referred a common third paper , then that two papers were co-coupling .
377 the
378 the co-coupling
379 the co-coupling strength
380 the co-coupling strength of
381 the co-coupling strength of two
382 the co-coupling strength of two given
383 the co-coupling strength of two given papers
384 the co-coupling strength of two given papers was
385 the co-coupling strength of two given papers was higher
386 the co-coupling strength of two given papers was higher if
387 the co-coupling strength of two given papers was higher if they
388 the co-coupling strength of two given papers was higher if they referred
389 the co-coupling strength of two given papers was higher if they referred more
390 the co-coupling strength of two given papers was higher if they referred more common
391 the co-coupling strength of two given papers was higher if they referred more common papers
392 the co-coupling strength of two given papers was higher if they referred more common papers .
393 then
394 then ,
395 then , they
396 then , they applied
397 then , they applied the
398 then , they applied the hits
399 then , they applied the hits algorithm
400 then , they applied the hits algorithm to
401 then , they applied the hits algorithm to assign
402 then , they applied the hits algorithm to assign each
403 then , they applied the hits algorithm to assign each candidate
404 then , they applied the hits algorithm to assign each candidate paper
405 then , they applied the hits algorithm to assign each candidate paper a
406 then , they applied the hits algorithm to assign each candidate paper a hub
407 then , they applied the hits algorithm to assign each candidate paper a hub score
408 then , they applied the hits algorithm to assign each candidate paper a hub score ,
409 then , they applied the hits algorithm to assign each candidate paper a hub score , and
410 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally
411 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according
412 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to
413 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the
414 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub
415 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores
416 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores ,
417 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores , the
418 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores , the top-n
419 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores , the top-n papers
420 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores , the top-n papers would
421 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores , the top-n papers would be
422 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores , the top-n papers would be recommended
423 then , they applied the hits algorithm to assign each candidate paper a hub score , and finally according to the hub scores , the top-n papers would be recommended .
424 and
425 and if
426 and if one
427 and if one paper
428 and if one paper had
429 and if one paper had citation
430 and if one paper had citation relation
431 and if one paper had citation relation to
432 and if one paper had citation relation to another
433 and if one paper had citation relation to another paper
434 and if one paper had citation relation to another paper ,
435 and if one paper had citation relation to another paper , the
436 and if one paper had citation relation to another paper , the two
437 and if one paper had citation relation to another paper , the two papers
438 and if one paper had citation relation to another paper , the two papers were
439 and if one paper had citation relation to another paper , the two papers were also
440 and if one paper had citation relation to another paper , the two papers were also considered
441 and if one paper had citation relation to another paper , the two papers were also considered similar
442 and if one paper had citation relation to another paper , the two papers were also considered similar .
443 the
444 the key-terms
445 the key-terms are
446 the key-terms are extracted
447 the key-terms are extracted based
448 the key-terms are extracted based on
449 the key-terms are extracted based on the
450 the key-terms are extracted based on the tfidf
451 the key-terms are extracted based on the tfidf score
452 the key-terms are extracted based on the tfidf score ,
453 the key-terms are extracted based on the tfidf score , the
454 the key-terms are extracted based on the tfidf score , the weights
455 the key-terms are extracted based on the tfidf score , the weights of
456 the key-terms are extracted based on the tfidf score , the weights of these
457 the key-terms are extracted based on the tfidf score , the weights of these key-terms
458 the key-terms are extracted based on the tfidf score , the weights of these key-terms are
459 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed
460 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by
461 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet
462 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet ,
463 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and
464 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and if
465 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and if key-terms
466 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and if key-terms tj
467 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and if key-terms tj appears
468 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and if key-terms tj appears in
469 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and if key-terms tj appears in paper
470 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and if key-terms tj appears in paper pi
471 the key-terms are extracted based on the tfidf score , the weights of these key-terms are computed by wordnet , and if key-terms tj appears in paper pi .
472 the
473 the main
474 the main idea
475 the main idea of
476 the main idea of graph-based
477 the main idea of graph-based semi-supervised
478 the main idea of graph-based semi-supervised classification
479 the main idea of graph-based semi-supervised classification was
480 the main idea of graph-based semi-supervised classification was that
481 the main idea of graph-based semi-supervised classification was that similar
482 the main idea of graph-based semi-supervised classification was that similar instances
483 the main idea of graph-based semi-supervised classification was that similar instances tended
484 the main idea of graph-based semi-supervised classification was that similar instances tended to
485 the main idea of graph-based semi-supervised classification was that similar instances tended to have
486 the main idea of graph-based semi-supervised classification was that similar instances tended to have similar
487 the main idea of graph-based semi-supervised classification was that similar instances tended to have similar categories
488 the main idea of graph-based semi-supervised classification was that similar instances tended to have similar categories .
489 general
490 general recommendation
491 general recommendation system
492 general recommendation system will
493 general recommendation system will recommend
494 general recommendation system will recommend some
495 general recommendation system will recommend some items
496 general recommendation system will recommend some items for
497 general recommendation system will recommend some items for the
498 general recommendation system will recommend some items for the user
499 general recommendation system will recommend some items for the user .
500 mean
501 mean reciprocal
502 mean reciprocal rank
503 mean reciprocal rank is
504 mean reciprocal rank is only
505 mean reciprocal rank is only concerned
506 mean reciprocal rank is only concerned about
507 mean reciprocal rank is only concerned about the
508 mean reciprocal rank is only concerned about the ranking
509 mean reciprocal rank is only concerned about the ranking of
510 mean reciprocal rank is only concerned about the ranking of the
511 mean reciprocal rank is only concerned about the ranking of the first
512 mean reciprocal rank is only concerned about the ranking of the first relevant
513 mean reciprocal rank is only concerned about the ranking of the first relevant term
514 mean reciprocal rank is only concerned about the ranking of the first relevant term which
515 mean reciprocal rank is only concerned about the ranking of the first relevant term which is
516 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned
517 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by
518 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the
519 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the system
520 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the system ,
521 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the system , average
522 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the system , average over
523 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the system , average over all
524 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the system , average over all target
525 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the system , average over all target papers
526 mean reciprocal rank is only concerned about the ranking of the first relevant term which is returned by the system , average over all target papers .
527 in
528 in order
529 in order to
530 in order to examine
531 in order to examine the
532 in order to examine the sensitive
533 in order to examine the sensitive of
534 in order to examine the sensitive of the
535 in order to examine the sensitive of the initial
536 in order to examine the sensitive of the initial weights
537 in order to examine the sensitive of the initial weights of
538 in order to examine the sensitive of the initial weights of the
539 in order to examine the sensitive of the initial weights of the edges
540 in order to examine the sensitive of the initial weights of the edges ,
541 in order to examine the sensitive of the initial weights of the edges , we
542 in order to examine the sensitive of the initial weights of the edges , we do
543 in order to examine the sensitive of the initial weights of the edges , we do the
544 in order to examine the sensitive of the initial weights of the edges , we do the other
545 in order to examine the sensitive of the initial weights of the edges , we do the other experiment
546 in order to examine the sensitive of the initial weights of the edges , we do the other experiment .
547 table
548 table 2
549 table 2 is
550 table 2 is the
551 table 2 is the results
552 table 2 is the results of
553 table 2 is the results of five
554 table 2 is the results of five different
555 table 2 is the results of five different weights
556 table 2 is the results of five different weights of
557 table 2 is the results of five different weights of edges
558 table 2 is the results of five different weights of edges combinations
559 table 2 is the results of five different weights of edges combinations in
560 table 2 is the results of five different weights of edges combinations in the
561 table 2 is the results of five different weights of edges combinations in the heterogeneous
562 table 2 is the results of five different weights of edges combinations in the heterogeneous graph
563 table 2 is the results of five different weights of edges combinations in the heterogeneous graph .
564 frame
565 frame semantics
566 frame semantics has
567 frame semantics has a
568 frame semantics has a complete
569 frame semantics has a complete system
570 frame semantics has a complete system ,
571 frame semantics has a complete system , is
572 frame semantics has a complete system , is an
573 frame semantics has a complete system , is an effective
574 frame semantics has a complete system , is an effective knowledge
575 frame semantics has a complete system , is an effective knowledge representation
576 frame semantics has a complete system , is an effective knowledge representation with
577 frame semantics has a complete system , is an effective knowledge representation with empirical
578 frame semantics has a complete system , is an effective knowledge representation with empirical semantic
579 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties
580 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against
581 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the
582 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background
583 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of
584 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive
585 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism
586 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism ,
587 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism , and
588 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism , and directly
589 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism , and directly oriented
590 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism , and directly oriented to
591 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism , and directly oriented to the
592 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism , and directly oriented to the application
593 frame semantics has a complete system , is an effective knowledge representation with empirical semantic properties against the background of cognitive mechanism , and directly oriented to the application .
594 it
595 it is
596 it is of
597 it is of great
598 it is of great importance
599 it is of great importance to
600 it is of great importance to frame
601 it is of great importance to frame construction
602 it is of great importance to frame construction ,
603 it is of great importance to frame construction , sentence
604 it is of great importance to frame construction , sentence annotation
605 it is of great importance to frame construction , sentence annotation and
606 it is of great importance to frame construction , sentence annotation and the
607 it is of great importance to frame construction , sentence annotation and the valence
608 it is of great importance to frame construction , sentence annotation and the valence patterns
609 it is of great importance to frame construction , sentence annotation and the valence patterns statistics
610 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of
611 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of lexical
612 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of lexical unit
613 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of lexical unit ,
614 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of lexical unit , providing
615 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of lexical unit , providing precondition
616 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of lexical unit , providing precondition for
617 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of lexical unit , providing precondition for domlfsr
618 it is of great importance to frame construction , sentence annotation and the valence patterns statistics of lexical unit , providing precondition for domlfsr .
619 the
620 the tagset
621 the tagset we
622 the tagset we used
623 the tagset we used in
624 the tagset we used in annotating
625 the tagset we used in annotating divided
626 the tagset we used in annotating divided into
627 the tagset we used in annotating divided into two
628 the tagset we used in annotating divided into two kinds
629 the tagset we used in annotating divided into two kinds of
630 the tagset we used in annotating divided into two kinds of english
631 the tagset we used in annotating divided into two kinds of english and
632 the tagset we used in annotating divided into two kinds of english and chinese
633 the tagset we used in annotating divided into two kinds of english and chinese .
634 mapping
635 mapping the
636 mapping the structure
637 mapping the structure and
638 mapping the structure and meaning
639 mapping the structure and meaning of
640 mapping the structure and meaning of language
641 mapping the structure and meaning of language has
642 mapping the structure and meaning of language has been
643 mapping the structure and meaning of language has been considered
644 mapping the structure and meaning of language has been considered as
645 mapping the structure and meaning of language has been considered as one
646 mapping the structure and meaning of language has been considered as one of
647 mapping the structure and meaning of language has been considered as one of the
648 mapping the structure and meaning of language has been considered as one of the basic
649 mapping the structure and meaning of language has been considered as one of the basic principles
650 mapping the structure and meaning of language has been considered as one of the basic principles of
651 mapping the structure and meaning of language has been considered as one of the basic principles of reearches
652 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in
653 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational
654 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics
655 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and
656 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language
657 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information
658 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing
659 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing ,
660 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting
661 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from
662 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the
663 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom
664 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of
665 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the
666 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language
667 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law
668 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law ,
669 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the
670 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching
671 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event
672 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction
673 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method
674 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has
675 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important
676 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important significance
677 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important significance for
678 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important significance for domain-oriented
679 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important significance for domain-oriented multi-language
680 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important significance for domain-oriented multi-language news
681 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important significance for domain-oriented multi-language news events
682 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important significance for domain-oriented multi-language news events extraction
683 mapping the structure and meaning of language has been considered as one of the basic principles of reearches in computational linguistics and language information processing , starting from the bottom of the language law , the pattern-matching event extraction method has important significance for domain-oriented multi-language news events extraction .
684 in
685 in addition
686 in addition ,
687 in addition , we
688 in addition , we will
689 in addition , we will transfer
690 in addition , we will transfer above
691 in addition , we will transfer above research
692 in addition , we will transfer above research productions
693 in addition , we will transfer above research productions to
694 in addition , we will transfer above research productions to other
695 in addition , we will transfer above research productions to other suitable
696 in addition , we will transfer above research productions to other suitable oriental
697 in addition , we will transfer above research productions to other suitable oriental languages
698 in addition , we will transfer above research productions to other suitable oriental languages like
699 in addition , we will transfer above research productions to other suitable oriental languages like malay
700 in addition , we will transfer above research productions to other suitable oriental languages like malay ,
701 in addition , we will transfer above research productions to other suitable oriental languages like malay , thai
702 in addition , we will transfer above research productions to other suitable oriental languages like malay , thai ,
703 in addition , we will transfer above research productions to other suitable oriental languages like malay , thai , japanese
704 in addition , we will transfer above research productions to other suitable oriental languages like malay , thai , japanese ,
705 in addition , we will transfer above research productions to other suitable oriental languages like malay , thai , japanese , and
706 in addition , we will transfer above research productions to other suitable oriental languages like malay , thai , japanese , and so
707 in addition , we will transfer above research productions to other suitable oriental languages like malay , thai , japanese , and so on
708 in addition , we will transfer above research productions to other suitable oriental languages like malay , thai , japanese , and so on .
709 by
710 by observing
711 by observing the
712 by observing the sentimental
713 by observing the sentimental trend
714 by observing the sentimental trend of
715 by observing the sentimental trend of the
716 by observing the sentimental trend of the different
717 by observing the sentimental trend of the different topics
718 by observing the sentimental trend of the different topics beneath
719 by observing the sentimental trend of the different topics beneath this
720 by observing the sentimental trend of the different topics beneath this event
721 by observing the sentimental trend of the different topics beneath this event ,
722 by observing the sentimental trend of the different topics beneath this event , we
723 by observing the sentimental trend of the different topics beneath this event , we attempt
724 by observing the sentimental trend of the different topics beneath this event , we attempt to
725 by observing the sentimental trend of the different topics beneath this event , we attempt to offer
726 by observing the sentimental trend of the different topics beneath this event , we attempt to offer feasible
727 by observing the sentimental trend of the different topics beneath this event , we attempt to offer feasible suggestions
728 by observing the sentimental trend of the different topics beneath this event , we attempt to offer feasible suggestions for
729 by observing the sentimental trend of the different topics beneath this event , we attempt to offer feasible suggestions for public
730 by observing the sentimental trend of the different topics beneath this event , we attempt to offer feasible suggestions for public sentiment
731 by observing the sentimental trend of the different topics beneath this event , we attempt to offer feasible suggestions for public sentiment monitoring
732 by observing the sentimental trend of the different topics beneath this event , we attempt to offer feasible suggestions for public sentiment monitoring .
733 dependency
734 dependency and
735 dependency and relational
736 dependency and relational structure
737 dependency and relational structure in
738 dependency and relational structure in treebank
739 dependency and relational structure in treebank annotation
740 dependency and relational structure in treebank annotation .
741 the
742 the rules
743 the rules of
744 the rules of hedges
745 the rules of hedges scope
746 the rules of hedges scope were
747 the rules of hedges scope were mainly
748 the rules of hedges scope were mainly due
749 the rules of hedges scope were mainly due to
750 the rules of hedges scope were mainly due to the
751 the rules of hedges scope were mainly due to the different
752 the rules of hedges scope were mainly due to the different syntactic
753 the rules of hedges scope were mainly due to the different syntactic constituents
754 the rules of hedges scope were mainly due to the different syntactic constituents which
755 the rules of hedges scope were mainly due to the different syntactic constituents which caused
756 the rules of hedges scope were mainly due to the different syntactic constituents which caused by
757 the rules of hedges scope were mainly due to the different syntactic constituents which caused by its
758 the rules of hedges scope were mainly due to the different syntactic constituents which caused by its parts
759 the rules of hedges scope were mainly due to the different syntactic constituents which caused by its parts of
760 the rules of hedges scope were mainly due to the different syntactic constituents which caused by its parts of speech
761 the rules of hedges scope were mainly due to the different syntactic constituents which caused by its parts of speech .
762 the
763 the paper
764 the paper proposed
765 the paper proposed a
766 the paper proposed a local
767 the paper proposed a local community
768 the paper proposed a local community detection
769 the paper proposed a local community detection method
770 the paper proposed a local community detection method ,
771 the paper proposed a local community detection method , this
772 the paper proposed a local community detection method , this method
773 the paper proposed a local community detection method , this method does
774 the paper proposed a local community detection method , this method does not
775 the paper proposed a local community detection method , this method does not need
776 the paper proposed a local community detection method , this method does not need to
777 the paper proposed a local community detection method , this method does not need to know
778 the paper proposed a local community detection method , this method does not need to know the
779 the paper proposed a local community detection method , this method does not need to know the whole
780 the paper proposed a local community detection method , this method does not need to know the whole complex
781 the paper proposed a local community detection method , this method does not need to know the whole complex network
782 the paper proposed a local community detection method , this method does not need to know the whole complex network information
783 the paper proposed a local community detection method , this method does not need to know the whole complex network information ,
784 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it
785 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just
786 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting
787 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from
788 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an
789 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial
790 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node
791 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and
792 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating
793 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the
794 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness
795 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between
796 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between the
797 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between the initial
798 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between the initial node
799 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between the initial node and
800 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between the initial node and the
801 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between the initial node and the adjacent
802 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between the initial node and the adjacent nodes
803 the paper proposed a local community detection method , this method does not need to know the whole complex network information , it just starting from an initial node and calculating the tightness between the initial node and the adjacent nodes .
804 and
805 and gradually
806 and gradually add
807 and gradually add the
808 and gradually add the adjacent
809 and gradually add the adjacent nodes
810 and gradually add the adjacent nodes to
811 and gradually add the adjacent nodes to the
812 and gradually add the adjacent nodes to the community
813 and gradually add the adjacent nodes to the community ;
814 and gradually add the adjacent nodes to the community ; finally
815 and gradually add the adjacent nodes to the community ; finally get
816 and gradually add the adjacent nodes to the community ; finally get this
817 and gradually add the adjacent nodes to the community ; finally get this node
818 and gradually add the adjacent nodes to the community ; finally get this node community
819 and gradually add the adjacent nodes to the community ; finally get this node community structure
820 and gradually add the adjacent nodes to the community ; finally get this node community structure .
821 identifying
822 identifying the
823 identifying the role
824 identifying the role that
825 identifying the role that animals
826 identifying the role that animals play
827 identifying the role that animals play in
828 identifying the role that animals play in their
829 identifying the role that animals play in their social
830 identifying the role that animals play in their social networks
831 identifying the role that animals play in their social networks .
832 and
833 and then
834 and then the
835 and then the authors
836 and then the authors compare
837 and then the authors compare several
838 and then the authors compare several pos
839 and then the authors compare several pos tagging
840 and then the authors compare several pos tagging methods
841 and then the authors compare several pos tagging methods ,
842 and then the authors compare several pos tagging methods , the
843 and then the authors compare several pos tagging methods , the results
844 and then the authors compare several pos tagging methods , the results prove
845 and then the authors compare several pos tagging methods , the results prove that
846 and then the authors compare several pos tagging methods , the results prove that train
847 and then the authors compare several pos tagging methods , the results prove that train data
848 and then the authors compare several pos tagging methods , the results prove that train data with
849 and then the authors compare several pos tagging methods , the results prove that train data with the
850 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level
851 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation
852 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation can
853 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation can enhance
854 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation can enhance the
855 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation can enhance the effects
856 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation can enhance the effects of
857 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation can enhance the effects of pos
858 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation can enhance the effects of pos tagging
859 and then the authors compare several pos tagging methods , the results prove that train data with the multi-level annotation can enhance the effects of pos tagging .
860 by
861 by considering
862 by considering the
863 by considering the characteristics
864 by considering the characteristics of
865 by considering the characteristics of both
866 by considering the characteristics of both chinese
867 by considering the characteristics of both chinese language
868 by considering the characteristics of both chinese language and
869 by considering the characteristics of both chinese language and social
870 by considering the characteristics of both chinese language and social networks
871 by considering the characteristics of both chinese language and social networks ,
872 by considering the characteristics of both chinese language and social networks , we
873 by considering the characteristics of both chinese language and social networks , we build
874 by considering the characteristics of both chinese language and social networks , we build a
875 by considering the characteristics of both chinese language and social networks , we build a set
876 by considering the characteristics of both chinese language and social networks , we build a set of
877 by considering the characteristics of both chinese language and social networks , we build a set of discriminating
878 by considering the characteristics of both chinese language and social networks , we build a set of discriminating features
879 by considering the characteristics of both chinese language and social networks , we build a set of discriminating features for
880 by considering the characteristics of both chinese language and social networks , we build a set of discriminating features for chinese
881 by considering the characteristics of both chinese language and social networks , we build a set of discriminating features for chinese irony
882 by considering the characteristics of both chinese language and social networks , we build a set of discriminating features for chinese irony detection
883 by considering the characteristics of both chinese language and social networks , we build a set of discriminating features for chinese irony detection .
884 for
885 for these
886 for these features
887 for these features ,
888 for these features , information
889 for these features , information gain
890 for these features , information gain is
891 for these features , information gain is applied
892 for these features , information gain is applied to
893 for these features , information gain is applied to compare
894 for these features , information gain is applied to compare their
895 for these features , information gain is applied to compare their efficiency
896 for these features , information gain is applied to compare their efficiency ,
897 for these features , information gain is applied to compare their efficiency , and
898 for these features , information gain is applied to compare their efficiency , and several
899 for these features , information gain is applied to compare their efficiency , and several classifiers
900 for these features , information gain is applied to compare their efficiency , and several classifiers are
901 for these features , information gain is applied to compare their efficiency , and several classifiers are also
902 for these features , information gain is applied to compare their efficiency , and several classifiers are also applied
903 for these features , information gain is applied to compare their efficiency , and several classifiers are also applied to
904 for these features , information gain is applied to compare their efficiency , and several classifiers are also applied to test
905 for these features , information gain is applied to compare their efficiency , and several classifiers are also applied to test their
906 for these features , information gain is applied to compare their efficiency , and several classifiers are also applied to test their stability
907 for these features , information gain is applied to compare their efficiency , and several classifiers are also applied to test their stability .
908 in
909 in our
910 in our work
911 in our work ,
912 in our work , lexical
913 in our work , lexical level
914 in our work , lexical level features
915 in our work , lexical level features include
916 in our work , lexical level features include the
917 in our work , lexical level features include the two
918 in our work , lexical level features include the two entities
919 in our work , lexical level features include the two entities ,
920 in our work , lexical level features include the two entities , their
921 in our work , lexical level features include the two entities , their ner
922 in our work , lexical level features include the two entities , their ner tags
923 in our work , lexical level features include the two entities , their ner tags ,
924 in our work , lexical level features include the two entities , their ner tags , and
925 in our work , lexical level features include the two entities , their ner tags , and the
926 in our work , lexical level features include the two entities , their ner tags , and the neighbor
927 in our work , lexical level features include the two entities , their ner tags , and the neighbor tokens
928 in our work , lexical level features include the two entities , their ner tags , and the neighbor tokens of
929 in our work , lexical level features include the two entities , their ner tags , and the neighbor tokens of these
930 in our work , lexical level features include the two entities , their ner tags , and the neighbor tokens of these two
931 in our work , lexical level features include the two entities , their ner tags , and the neighbor tokens of these two entities
932 in our work , lexical level features include the two entities , their ner tags , and the neighbor tokens of these two entities .
933 in
934 in our
935 in our work
936 in our work ,
937 in our work , we
938 in our work , we adopt
939 in our work , we adopt neural
940 in our work , we adopt neural tensor
941 in our work , we adopt neural tensor layer
942 in our work , we adopt neural tensor layer to
943 in our work , we adopt neural tensor layer to model
944 in our work , we adopt neural tensor layer to model the
945 in our work , we adopt neural tensor layer to model the interactions
946 in our work , we adopt neural tensor layer to model the interactions of
947 in our work , we adopt neural tensor layer to model the interactions of above
948 in our work , we adopt neural tensor layer to model the interactions of above extracted
949 in our work , we adopt neural tensor layer to model the interactions of above extracted features
950 in our work , we adopt neural tensor layer to model the interactions of above extracted features in
951 in our work , we adopt neural tensor layer to model the interactions of above extracted features in a
952 in our work , we adopt neural tensor layer to model the interactions of above extracted features in a mention
953 in our work , we adopt neural tensor layer to model the interactions of above extracted features in a mention .
954 we
955 we can
956 we can think
957 we can think of
958 we can think of this
959 we can think of this procedure
960 we can think of this procedure from
961 we can think of this procedure from a
962 we can think of this procedure from a joint
963 we can think of this procedure from a joint learning
964 we can think of this procedure from a joint learning perspective
965 we can think of this procedure from a joint learning perspective .
966 in
967 in another
968 in another word
969 in another word ,
970 in another word , the
971 in another word , the representation
972 in another word , the representation learned
973 in another word , the representation learned contains
974 in another word , the representation learned contains information
975 in another word , the representation learned contains information from
976 in another word , the representation learned contains information from all
977 in another word , the representation learned contains information from all of
978 in another word , the representation learned contains information from all of these
979 in another word , the representation learned contains information from all of these relations
980 in another word , the representation learned contains information from all of these relations ,
981 in another word , the representation learned contains information from all of these relations , hence
982 in another word , the representation learned contains information from all of these relations , hence it
983 in another word , the representation learned contains information from all of these relations , hence it can
984 in another word , the representation learned contains information from all of these relations , hence it can capture
985 in another word , the representation learned contains information from all of these relations , hence it can capture the
986 in another word , the representation learned contains information from all of these relations , hence it can capture the correlations
987 in another word , the representation learned contains information from all of these relations , hence it can capture the correlations among
988 in another word , the representation learned contains information from all of these relations , hence it can capture the correlations among different
989 in another word , the representation learned contains information from all of these relations , hence it can capture the correlations among different relations
990 in another word , the representation learned contains information from all of these relations , hence it can capture the correlations among different relations .
991 on
992 on the
993 on the one
994 on the one hand
995 on the one hand ,
996 on the one hand , comparing
997 on the one hand , comparing to
998 on the one hand , comparing to traditional
999 on the one hand , comparing to traditional shallow
1000 on the one hand , comparing to traditional shallow models
1001 on the one hand , comparing to traditional shallow models ,
1002 on the one hand , comparing to traditional shallow models , our
1003 on the one hand , comparing to traditional shallow models , our model
1004 on the one hand , comparing to traditional shallow models , our model does
1005 on the one hand , comparing to traditional shallow models , our model does not
1006 on the one hand , comparing to traditional shallow models , our model does not dependent
1007 on the one hand , comparing to traditional shallow models , our model does not dependent on
1008 on the one hand , comparing to traditional shallow models , our model does not dependent on lots
1009 on the one hand , comparing to traditional shallow models , our model does not dependent on lots of
1010 on the one hand , comparing to traditional shallow models , our model does not dependent on lots of rich
1011 on the one hand , comparing to traditional shallow models , our model does not dependent on lots of rich handdesigned
1012 on the one hand , comparing to traditional shallow models , our model does not dependent on lots of rich handdesigned features
1013 on the one hand , comparing to traditional shallow models , our model does not dependent on lots of rich handdesigned features .
1014 further
1015 further more
1016 further more ,
1017 further more , our
1018 further more , our model
1019 further more , our model uses
1020 further more , our model uses global
1021 further more , our model uses global structure
1022 further more , our model uses global structure information
1023 further more , our model uses global structure information of
1024 further more , our model uses global structure information of parse
1025 further more , our model uses global structure information of parse trees
1026 further more , our model uses global structure information of parse trees .
1027 one
1028 one of
1029 one of drawbacks
1030 one of drawbacks of
1031 one of drawbacks of these
1032 one of drawbacks of these traditional
1033 one of drawbacks of these traditional models
1034 one of drawbacks of these traditional models is
1035 one of drawbacks of these traditional models is that
1036 one of drawbacks of these traditional models is that they
1037 one of drawbacks of these traditional models is that they need
1038 one of drawbacks of these traditional models is that they need a
1039 one of drawbacks of these traditional models is that they need a lot
1040 one of drawbacks of these traditional models is that they need a lot of
1041 one of drawbacks of these traditional models is that they need a lot of hand-designed
1042 one of drawbacks of these traditional models is that they need a lot of hand-designed features
1043 one of drawbacks of these traditional models is that they need a lot of hand-designed features and
1044 one of drawbacks of these traditional models is that they need a lot of hand-designed features and rich
1045 one of drawbacks of these traditional models is that they need a lot of hand-designed features and rich resources
1046 one of drawbacks of these traditional models is that they need a lot of hand-designed features and rich resources to
1047 one of drawbacks of these traditional models is that they need a lot of hand-designed features and rich resources to reach
1048 one of drawbacks of these traditional models is that they need a lot of hand-designed features and rich resources to reach high
1049 one of drawbacks of these traditional models is that they need a lot of hand-designed features and rich resources to reach high performance
1050 one of drawbacks of these traditional models is that they need a lot of hand-designed features and rich resources to reach high performance .
1051 they
1052 they model
1053 they model srl
1054 they model srl task
1055 they model srl task using
1056 they model srl task using time
1057 they model srl task using time delay
1058 they model srl task using time delay neural
1059 they model srl task using time delay neural networks
1060 they model srl task using time delay neural networks and
1061 they model srl task using time delay neural networks and got
1062 they model srl task using time delay neural networks and got competitive
1063 they model srl task using time delay neural networks and got competitive performance
1064 they model srl task using time delay neural networks and got competitive performance compared
1065 they model srl task using time delay neural networks and got competitive performance compared to
1066 they model srl task using time delay neural networks and got competitive performance compared to the
1067 they model srl task using time delay neural networks and got competitive performance compared to the state-of-art
1068 they model srl task using time delay neural networks and got competitive performance compared to the state-of-art traditional
1069 they model srl task using time delay neural networks and got competitive performance compared to the state-of-art traditional model
1070 they model srl task using time delay neural networks and got competitive performance compared to the state-of-art traditional model .
1071 consider
1072 consider the
1073 consider the expandability
1074 consider the expandability of
1075 consider the expandability of the
1076 consider the expandability of the network
1077 consider the expandability of the network ,
1078 consider the expandability of the network , we
1079 consider the expandability of the network , we also
1080 consider the expandability of the network , we also add
1081 consider the expandability of the network , we also add a
1082 consider the expandability of the network , we also add a hidden
1083 consider the expandability of the network , we also add a hidden layer
1084 consider the expandability of the network , we also add a hidden layer below
1085 consider the expandability of the network , we also add a hidden layer below output
1086 consider the expandability of the network , we also add a hidden layer below output layer
1087 consider the expandability of the network , we also add a hidden layer below output layer .
1088 so
1089 so we
1090 so we need
1091 so we need not
1092 so we need not take
1093 so we need not take too
1094 so we need not take too much
1095 so we need not take too much attention
1096 so we need not take too much attention on
1097 so we need not take too much attention on the
1098 so we need not take too much attention on the roles
1099 so we need not take too much attention on the roles of
1100 so we need not take too much attention on the roles of other
1101 so we need not take too much attention on the roles of other clauses
1102 so we need not take too much attention on the roles of other clauses .
1103 although
1104 although the
1105 although the purpose
1106 although the purpose of
1107 although the purpose of sharing
1108 although the purpose of sharing weight
1109 although the purpose of sharing weight is
1110 although the purpose of sharing weight is that
1111 although the purpose of sharing weight is that the
1112 although the purpose of sharing weight is that the network
1113 although the purpose of sharing weight is that the network can
1114 although the purpose of sharing weight is that the network can learning
1115 although the purpose of sharing weight is that the network can learning some
1116 although the purpose of sharing weight is that the network can learning some general
1117 although the purpose of sharing weight is that the network can learning some general features
1118 although the purpose of sharing weight is that the network can learning some general features from
1119 although the purpose of sharing weight is that the network can learning some general features from each
1120 although the purpose of sharing weight is that the network can learning some general features from each recursive
1121 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure
1122 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure ,
1123 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our
1124 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal
1125 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is
1126 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is predict
1127 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is predict different
1128 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is predict different semantic
1129 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is predict different semantic roles
1130 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is predict different semantic roles in
1131 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is predict different semantic roles in one
1132 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is predict different semantic roles in one sentence
1133 although the purpose of sharing weight is that the network can learning some general features from each recursive substructure , our goal is predict different semantic roles in one sentence .
1134 so
1135 so our
1136 so our model
1137 so our model not
1138 so our model not only
1139 so our model not only needs
1140 so our model not only needs the
1141 so our model not only needs the global
1142 so our model not only needs the global features
1143 so our model not only needs the global features but
1144 so our model not only needs the global features but local
1145 so our model not only needs the global features but local features
1146 so our model not only needs the global features but local features as
1147 so our model not only needs the global features but local features as well
1148 so our model not only needs the global features but local features as well .
1149 the
1150 the loss
1151 the loss is
1152 the loss is proportional
1153 the loss is proportional to
1154 the loss is proportional to the
1155 the loss is proportional to the number
1156 the loss is proportional to the number of
1157 the loss is proportional to the number of nodes
1158 the loss is proportional to the number of nodes with
1159 the loss is proportional to the number of nodes with an
1160 the loss is proportional to the number of nodes with an incorrect
1161 the loss is proportional to the number of nodes with an incorrect role
1162 the loss is proportional to the number of nodes with an incorrect role in
1163 the loss is proportional to the number of nodes with an incorrect role in the
1164 the loss is proportional to the number of nodes with an incorrect role in the proposed
1165 the loss is proportional to the number of nodes with an incorrect role in the proposed role
1166 the loss is proportional to the number of nodes with an incorrect role in the proposed role set
1167 the loss is proportional to the number of nodes with an incorrect role in the proposed role set ,
1168 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which
1169 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases
1170 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the
1171 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the more
1172 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the more incorrect
1173 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the more incorrect the
1174 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the more incorrect the proposed
1175 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the more incorrect the proposed role
1176 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the more incorrect the proposed role set
1177 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the more incorrect the proposed role set is
1178 the loss is proportional to the number of nodes with an incorrect role in the proposed role set , which increases the more incorrect the proposed role set is .
1179 although
1180 although our
1181 although our system
1182 although our system is
1183 although our system is a
1184 although our system is a recursive
1185 although our system is a recursive network
1186 although our system is a recursive network based
1187 although our system is a recursive network based on
1188 although our system is a recursive network based on parse
1189 although our system is a recursive network based on parse tree
1190 although our system is a recursive network based on parse tree ,
1191 although our system is a recursive network based on parse tree , an
1192 although our system is a recursive network based on parse tree , an average
1193 although our system is a recursive network based on parse tree , an average parse
1194 although our system is a recursive network based on parse tree , an average parse tree
1195 although our system is a recursive network based on parse tree , an average parse tree level
1196 although our system is a recursive network based on parse tree , an average parse tree level is
1197 although our system is a recursive network based on parse tree , an average parse tree level is about
1198 although our system is a recursive network based on parse tree , an average parse tree level is about ten
1199 although our system is a recursive network based on parse tree , an average parse tree level is about ten and
1200 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the
1201 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length
1202 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of
1203 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence
1204 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is
1205 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about
1206 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen
1207 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen ,
1208 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the
1209 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost
1210 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of
1211 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing
1212 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on
1213 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the
1214 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive
1215 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse
1216 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree
1217 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is
1218 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar
1219 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with
1220 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with the
1221 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with the cost
1222 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with the cost of
1223 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with the cost of convolution
1224 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with the cost of convolution layer
1225 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with the cost of convolution layer in
1226 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with the cost of convolution layer in cnn
1227 although our system is a recursive network based on parse tree , an average parse tree level is about ten and the length of sentence is about fifteen , the cost of computing on the recursive parse tree is similar with the cost of convolution layer in cnn .
1228 our
1229 our model
1230 our model is
1231 our model is also
1232 our model is also extensible
1233 our model is also extensible for
1234 our model is also extensible for add
1235 our model is also extensible for add more
1236 our model is also extensible for add more features
1237 our model is also extensible for add more features and
1238 our model is also extensible for add more features and resources
1239 our model is also extensible for add more features and resources as
1240 our model is also extensible for add more features and resources as talked
1241 our model is also extensible for add more features and resources as talked in
1242 our model is also extensible for add more features and resources as talked in section
1243 our model is also extensible for add more features and resources as talked in section 3
1244 our model is also extensible for add more features and resources as talked in section 3 .
1245 svm-crf
1246 svm-crf modeled
1247 svm-crf modeled the
1248 svm-crf modeled the task
1249 svm-crf modeled the task by
1250 svm-crf modeled the task by sequence
1251 svm-crf modeled the task by sequence labeling
1252 svm-crf modeled the task by sequence labeling model
1253 svm-crf modeled the task by sequence labeling model ,
1254 svm-crf modeled the task by sequence labeling model , however
1255 svm-crf modeled the task by sequence labeling model , however this
1256 svm-crf modeled the task by sequence labeling model , however this method
1257 svm-crf modeled the task by sequence labeling model , however this method performs
1258 svm-crf modeled the task by sequence labeling model , however this method performs badly
1259 svm-crf modeled the task by sequence labeling model , however this method performs badly and
1260 svm-crf modeled the task by sequence labeling model , however this method performs badly and may
1261 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be
1262 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because
1263 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the
1264 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf
1265 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model
1266 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs
1267 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a
1268 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large
1269 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number
1270 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of
1271 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data
1272 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to
1273 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn
1274 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the
1275 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model
1276 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters
1277 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and
1278 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and the
1279 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and the imbalanced
1280 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and the imbalanced learning
1281 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and the imbalanced learning problem
1282 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and the imbalanced learning problem is
1283 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and the imbalanced learning problem is not
1284 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and the imbalanced learning problem is not concerned
1285 svm-crf modeled the task by sequence labeling model , however this method performs badly and may be because the crf model needs a large number of data to learn the model parameters and the imbalanced learning problem is not concerned .
1286 lcr
1287 lcr in
1288 lcr in the
1289 lcr in the right
1290 lcr in the right section
1291 lcr in the right section shares
1292 lcr in the right section shares lookup
1293 lcr in the right section shares lookup table
1294 lcr in the right section shares lookup table layer
1295 lcr in the right section shares lookup table layer and
1296 lcr in the right section shares lookup table layer and convolutional
1297 lcr in the right section shares lookup table layer and convolutional layer
1298 lcr in the right section shares lookup table layer and convolutional layer with
1299 lcr in the right section shares lookup table layer and convolutional layer with ldr
1300 lcr in the right section shares lookup table layer and convolutional layer with ldr the
1301 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn
1302 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture
1303 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically
1304 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns
1305 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features
1306 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for
1307 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification
1308 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task
1309 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task in
1310 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task in the
1311 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task in the deep
1312 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task in the deep layers
1313 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task in the deep layers of
1314 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task in the deep layers of its
1315 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task in the deep layers of its architecture
1316 lcr in the right section shares lookup table layer and convolutional layer with ldr the cnn architecture automatically learns features for classification task in the deep layers of its architecture .
1317 learning
1318 learning continuous
1319 learning continuous representation
1320 learning continuous representation of
1321 learning continuous representation of the
1322 learning continuous representation of the context
1323 learning continuous representation of the context and
1324 learning continuous representation of the context and its
1325 learning continuous representation of the context and its category
1326 learning continuous representation of the context and its category are
1327 learning continuous representation of the context and its category are two
1328 learning continuous representation of the context and its category are two highly
1329 learning continuous representation of the context and its category are two highly related
1330 learning continuous representation of the context and its category are two highly related task
1331 learning continuous representation of the context and its category are two highly related task ,
1332 learning continuous representation of the context and its category are two highly related task , and
1333 learning continuous representation of the context and its category are two highly related task , and it
1334 learning continuous representation of the context and its category are two highly related task , and it make
1335 learning continuous representation of the context and its category are two highly related task , and it make sense
1336 learning continuous representation of the context and its category are two highly related task , and it make sense that
1337 learning continuous representation of the context and its category are two highly related task , and it make sense that features
1338 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful
1339 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for
1340 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one
1341 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one task
1342 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one task might
1343 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one task might be
1344 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one task might be useful
1345 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one task might be useful for
1346 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one task might be useful for another
1347 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one task might be useful for another one
1348 learning continuous representation of the context and its category are two highly related task , and it make sense that features useful for one task might be useful for another one .
1349 compared
1350 compared to
1351 compared to traditional
1352 compared to traditional models
1353 compared to traditional models ,
1354 compared to traditional models , we
1355 compared to traditional models , we incorporates
1356 compared to traditional models , we incorporates much
1357 compared to traditional models , we incorporates much larger
1358 compared to traditional models , we incorporates much larger context
1359 compared to traditional models , we incorporates much larger context and
1360 compared to traditional models , we incorporates much larger context and more
1361 compared to traditional models , we incorporates much larger context and more patterns
1362 compared to traditional models , we incorporates much larger context and more patterns of
1363 compared to traditional models , we incorporates much larger context and more patterns of interactions
1364 compared to traditional models , we incorporates much larger context and more patterns of interactions .
1365 we
1366 we adapt
1367 we adapt this
1368 we adapt this network
1369 we adapt this network for
1370 we adapt this network for dependency
1371 we adapt this network for dependency trees
1372 we adapt this network for dependency trees and
1373 we adapt this network for dependency trees and propose
1374 we adapt this network for dependency trees and propose the
1375 we adapt this network for dependency trees and propose the content-context
1376 we adapt this network for dependency trees and propose the content-context vectorial
1377 we adapt this network for dependency trees and propose the content-context vectorial representation
1378 we adapt this network for dependency trees and propose the content-context vectorial representation for
1379 we adapt this network for dependency trees and propose the content-context vectorial representation for each
1380 we adapt this network for dependency trees and propose the content-context vectorial representation for each node
1381 we adapt this network for dependency trees and propose the content-context vectorial representation for each node ,
1382 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this
1383 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node
1384 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation
1385 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses
1386 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses rich
1387 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses rich context
1388 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses rich context of
1389 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses rich context of the
1390 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses rich context of the whole
1391 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses rich context of the whole sub-tree
1392 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses rich context of the whole sub-tree information
1393 we adapt this network for dependency trees and propose the content-context vectorial representation for each node , this node representation compresses rich context of the whole sub-tree information .
1394 the
1395 the only
1396 the only difference
1397 the only difference is
1398 the only difference is that
1399 the only difference is that in
1400 the only difference is that in each
1401 the only difference is that in each chart
1402 the only difference is that in each chart cell
1403 the only difference is that in each chart cell ,
1404 the only difference is that in each chart cell , we
1405 the only difference is that in each chart cell , we keep
1406 the only difference is that in each chart cell , we keep b
1407 the only difference is that in each chart cell , we keep b highest
1408 the only difference is that in each chart cell , we keep b highest scored
1409 the only difference is that in each chart cell , we keep b highest scored candidates
1410 the only difference is that in each chart cell , we keep b highest scored candidates instead
1411 the only difference is that in each chart cell , we keep b highest scored candidates instead of
1412 the only difference is that in each chart cell , we keep b highest scored candidates instead of just
1413 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one
1414 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one ,
1415 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one , and
1416 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one , and call
1417 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one , and call 7
1418 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one , and call 7 this
1419 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one , and call 7 this list
1420 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one , and call 7 this list an
1421 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one , and call 7 this list an agenda
1422 the only difference is that in each chart cell , we keep b highest scored candidates instead of just one , and call 7 this list an agenda .
1423 this
1424 this will
1425 this will be
1426 this will be time-consuming
1427 this will be time-consuming in
1428 this will be time-consuming in practise
1429 this will be time-consuming in practise .
1430 the
1431 the minibatch
1432 the minibatch size
1433 the minibatch size was
1434 the minibatch size was set
1435 the minibatch size was set as
1436 the minibatch size was set as 20
1437 the minibatch size was set as 20 .
1438 we
1439 we then
1440 we then directly
1441 we then directly applies
1442 we then directly applies this
1443 we then directly applies this trained
1444 we then directly applies this trained model
1445 we then directly applies this trained model in
1446 we then directly applies this trained model in a
1447 we then directly applies this trained model in a reranking
1448 we then directly applies this trained model in a reranking framework
1449 we then directly applies this trained model in a reranking framework .
1450 compared
1451 compared with
1452 compared with these
1453 compared with these models
1454 compared with these models ,
1455 compared with these models , our
1456 compared with these models , our system
1457 compared with these models , our system considers
1458 compared with these models , our system considers richer
1459 compared with these models , our system considers richer more
1460 compared with these models , our system considers richer more structural
1461 compared with these models , our system considers richer more structural context
1462 compared with these models , our system considers richer more structural context information
1463 compared with these models , our system considers richer more structural context information ,
1464 compared with these models , our system considers richer more structural context information , and
1465 compared with these models , our system considers richer more structural context information , and we
1466 compared with these models , our system considers richer more structural context information , and we do
1467 compared with these models , our system considers richer more structural context information , and we do not
1468 compared with these models , our system considers richer more structural context information , and we do not rely
1469 compared with these models , our system considers richer more structural context information , and we do not rely on
1470 compared with these models , our system considers richer more structural context information , and we do not rely on hand-crafted
1471 compared with these models , our system considers richer more structural context information , and we do not rely on hand-crafted feature
1472 compared with these models , our system considers richer more structural context information , and we do not rely on hand-crafted feature templates
1473 compared with these models , our system considers richer more structural context information , and we do not rely on hand-crafted feature templates .
1474 such
1475 such as
1476 such as treatment
1477 such as treatment customization
1478 such as treatment customization based
1479 such as treatment customization based on
1480 such as treatment customization based on the
1481 such as treatment customization based on the risk
1482 such as treatment customization based on the risk level
1483 such as treatment customization based on the risk level of
1484 such as treatment customization based on the risk level of the
1485 such as treatment customization based on the risk level of the specific
1486 such as treatment customization based on the risk level of the specific patient
1487 such as treatment customization based on the risk level of the specific patient .
1488 in
1489 in this
1490 in this way
1491 in this way ,
1492 in this way , each
1493 in this way , each report
1494 in this way , each report can
1495 in this way , each report can be
1496 in this way , each report can be represented
1497 in this way , each report can be represented by
1498 in this way , each report can be represented by a
1499 in this way , each report can be represented by a vector
1500 in this way , each report can be represented by a vector containing
1501 in this way , each report can be represented by a vector containing values
1502 in this way , each report can be represented by a vector containing values of
1503 in this way , each report can be represented by a vector containing values of each
1504 in this way , each report can be represented by a vector containing values of each attributes
1505 in this way , each report can be represented by a vector containing values of each attributes .
1506 while
1507 while the
1508 while the classifier
1509 while the classifier trained
1510 while the classifier trained by
1511 while the classifier trained by machine
1512 while the classifier trained by machine learning
1513 while the classifier trained by machine learning is
1514 while the classifier trained by machine learning is validated
1515 while the classifier trained by machine learning is validated through
1516 while the classifier trained by machine learning is validated through a
1517 while the classifier trained by machine learning is validated through a 10-fold
1518 while the classifier trained by machine learning is validated through a 10-fold cross-validation
1519 while the classifier trained by machine learning is validated through a 10-fold cross-validation approach
1520 while the classifier trained by machine learning is validated through a 10-fold cross-validation approach .
1521 after
1522 after the
1523 after the tagging
1524 after the tagging process
1525 after the tagging process was
1526 after the tagging process was done
1527 after the tagging process was done ,
1528 after the tagging process was done , adopting
1529 after the tagging process was done , adopting maximum
1530 after the tagging process was done , adopting maximum entropy
1531 after the tagging process was done , adopting maximum entropy model
1532 after the tagging process was done , adopting maximum entropy model three
1533 after the tagging process was done , adopting maximum entropy model three subtasks
1534 after the tagging process was done , adopting maximum entropy model three subtasks of
1535 after the tagging process was done , adopting maximum entropy model three subtasks of chinese
1536 after the tagging process was done , adopting maximum entropy model three subtasks of chinese lexical
1537 after the tagging process was done , adopting maximum entropy model three subtasks of chinese lexical analysis
1538 after the tagging process was done , adopting maximum entropy model three subtasks of chinese lexical analysis could
1539 after the tagging process was done , adopting maximum entropy model three subtasks of chinese lexical analysis could be
1540 after the tagging process was done , adopting maximum entropy model three subtasks of chinese lexical analysis could be completed
1541 after the tagging process was done , adopting maximum entropy model three subtasks of chinese lexical analysis could be completed at
1542 after the tagging process was done , adopting maximum entropy model three subtasks of chinese lexical analysis could be completed at once
1543 after the tagging process was done , adopting maximum entropy model three subtasks of chinese lexical analysis could be completed at once .
1544 the
1545 the raising
1546 the raising performance
1547 the raising performance indicates
1548 the raising performance indicates that
1549 the raising performance indicates that the
1550 the raising performance indicates that the trinity
1551 the raising performance indicates that the trinity character-based
1552 the raising performance indicates that the trinity character-based tagging
1553 the raising performance indicates that the trinity character-based tagging method
1554 the raising performance indicates that the trinity character-based tagging method is
1555 the raising performance indicates that the trinity character-based tagging method is much
1556 the raising performance indicates that the trinity character-based tagging method is much better
1557 the raising performance indicates that the trinity character-based tagging method is much better than
1558 the raising performance indicates that the trinity character-based tagging method is much better than traditional
1559 the raising performance indicates that the trinity character-based tagging method is much better than traditional method
1560 the raising performance indicates that the trinity character-based tagging method is much better than traditional method .
1561 the
1562 the corpus
1563 the corpus is
1564 the corpus is based
1565 the corpus is based on
1566 the corpus is based on connective-driven
1567 the corpus is based on connective-driven dependence
1568 the corpus is based on connective-driven dependence tree
1569 the corpus is based on connective-driven dependence tree representation
1570 the corpus is based on connective-driven dependence tree representation system
1571 the corpus is based on connective-driven dependence tree representation system ,
1572 the corpus is based on connective-driven dependence tree representation system , which
1573 the corpus is based on connective-driven dependence tree representation system , which contains
1574 the corpus is based on connective-driven dependence tree representation system , which contains clause
1575 the corpus is based on connective-driven dependence tree representation system , which contains clause ,
1576 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective
1577 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective ,
1578 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective , discourse
1579 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective , discourse relation
1580 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective , discourse relation ,
1581 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective , discourse relation , discourse
1582 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective , discourse relation , discourse structure
1583 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective , discourse relation , discourse structure et
1584 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective , discourse relation , discourse structure et al
1585 the corpus is based on connective-driven dependence tree representation system , which contains clause , connective , discourse relation , discourse structure et al .
1586 firstly
1587 firstly ,
1588 firstly , answer
1589 firstly , answer features
1590 firstly , answer features bind
1591 firstly , answer features bind question
1592 firstly , answer features bind question features
1593 firstly , answer features bind question features to
1594 firstly , answer features bind question features to realize
1595 firstly , answer features bind question features to realize samples
1596 firstly , answer features bind question features to realize samples said
1597 firstly , answer features bind question features to realize samples said ;
1598 firstly , answer features bind question features to realize samples said ; secondly
1599 firstly , answer features bind question features to realize samples said ; secondly ,
1600 firstly , answer features bind question features to realize samples said ; secondly , a
1601 firstly , answer features bind question features to realize samples said ; secondly , a question
1602 firstly , answer features bind question features to realize samples said ; secondly , a question classifier
1603 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will
1604 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be
1605 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained
1606 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on
1607 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled
1608 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions
1609 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using
1610 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label
1611 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation
1612 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm
1613 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to
1614 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to annotate
1615 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to annotate the
1616 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to annotate the category
1617 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to annotate the category of
1618 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to annotate the category of unlabeled
1619 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to annotate the category of unlabeled questions
1620 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to annotate the category of unlabeled questions automatically
1621 firstly , answer features bind question features to realize samples said ; secondly , a question classifier will be trained on labeled questions using label propagation algorithm to annotate the category of unlabeled questions automatically .
1622 topics
1623 topics are
1624 topics are then
1625 topics are then represented
1626 topics are then represented with
1627 topics are then represented with multi-word
1628 topics are then represented with multi-word expression
1629 topics are then represented with multi-word expression .
1630 label
1631 label examples
1632 label examples for
1633 label examples for news
1634 label examples for news ,
1635 label examples for news , twitter
1636 label examples for news , twitter and
1637 label examples for news , twitter and nips
1638 label examples for news , twitter and nips topics
1639 label examples for news , twitter and nips topics .
1640 unbiased
1641 unbiased ,
1642 unbiased , which
1643 unbiased , which means
1644 unbiased , which means that
1645 unbiased , which means that either
1646 unbiased , which means that either of
1647 unbiased , which means that either of two
1648 unbiased , which means that either of two characters
1649 unbiased , which means that either of two characters semantic
1650 unbiased , which means that either of two characters semantic distance
1651 unbiased , which means that either of two characters semantic distance to
1652 unbiased , which means that either of two characters semantic distance to the
1653 unbiased , which means that either of two characters semantic distance to the word
1654 unbiased , which means that either of two characters semantic distance to the word is
1655 unbiased , which means that either of two characters semantic distance to the word is approximate
1656 unbiased , which means that either of two characters semantic distance to the word is approximate or
1657 unbiased , which means that either of two characters semantic distance to the word is approximate or the
1658 unbiased , which means that either of two characters semantic distance to the word is approximate or the word
1659 unbiased , which means that either of two characters semantic distance to the word is approximate or the word is
1660 unbiased , which means that either of two characters semantic distance to the word is approximate or the word is non-compositional
1661 unbiased , which means that either of two characters semantic distance to the word is approximate or the word is non-compositional .
1662 generally
1663 generally speaking
1664 generally speaking ,
1665 generally speaking , when
1666 generally speaking , when machine
1667 generally speaking , when machine learning
1668 generally speaking , when machine learning models
1669 generally speaking , when machine learning models such
1670 generally speaking , when machine learning models such as
1671 generally speaking , when machine learning models such as me
1672 generally speaking , when machine learning models such as me and
1673 generally speaking , when machine learning models such as me and crfs
1674 generally speaking , when machine learning models such as me and crfs are
1675 generally speaking , when machine learning models such as me and crfs are used
1676 generally speaking , when machine learning models such as me and crfs are used to
1677 generally speaking , when machine learning models such as me and crfs are used to train
1678 generally speaking , when machine learning models such as me and crfs are used to train a
1679 generally speaking , when machine learning models such as me and crfs are used to train a tibetan
1680 generally speaking , when machine learning models such as me and crfs are used to train a tibetan word
1681 generally speaking , when machine learning models such as me and crfs are used to train a tibetan word segmenter
1682 generally speaking , when machine learning models such as me and crfs are used to train a tibetan word segmenter .
1683 these
1684 these features
1685 these features are
1686 these features are usually
1687 these features are usually acquired
1688 these features are usually acquired by
1689 these features are usually acquired by the
1690 these features are usually acquired by the help
1691 these features are usually acquired by the help of
1692 these features are usually acquired by the help of external
1693 these features are usually acquired by the help of external linguistic
1694 these features are usually acquired by the help of external linguistic resources
1695 these features are usually acquired by the help of external linguistic resources .
1696 the
1697 the traditional
1698 the traditional methods
1699 the traditional methods introduced
1700 the traditional methods introduced above
1701 the traditional methods introduced above for
1702 the traditional methods introduced above for implicit
1703 the traditional methods introduced above for implicit relation
1704 the traditional methods introduced above for implicit relation classification
1705 the traditional methods introduced above for implicit relation classification face
1706 the traditional methods introduced above for implicit relation classification face two
1707 the traditional methods introduced above for implicit relation classification face two major
1708 the traditional methods introduced above for implicit relation classification face two major challenges
1709 the traditional methods introduced above for implicit relation classification face two major challenges .
1710 since
1711 since sentences
1712 since sentences in
1713 since sentences in different
1714 since sentences in different length
1715 since sentences in different length contain
1716 since sentences in different length contain different
1717 since sentences in different length contain different number
1718 since sentences in different length contain different number of
1719 since sentences in different length contain different number of word
1720 since sentences in different length contain different number of word pairs
1721 since sentences in different length contain different number of word pairs ,
1722 since sentences in different length contain different number of word pairs , concatenation
1723 since sentences in different length contain different number of word pairs , concatenation of
1724 since sentences in different length contain different number of word pairs , concatenation of word
1725 since sentences in different length contain different number of word pairs , concatenation of word embeddings
1726 since sentences in different length contain different number of word pairs , concatenation of word embeddings can
1727 since sentences in different length contain different number of word pairs , concatenation of word embeddings can not
1728 since sentences in different length contain different number of word pairs , concatenation of word embeddings can not lead
1729 since sentences in different length contain different number of word pairs , concatenation of word embeddings can not lead to
1730 since sentences in different length contain different number of word pairs , concatenation of word embeddings can not lead to a
1731 since sentences in different length contain different number of word pairs , concatenation of word embeddings can not lead to a fixed-length
1732 since sentences in different length contain different number of word pairs , concatenation of word embeddings can not lead to a fixed-length vector
1733 since sentences in different length contain different number of word pairs , concatenation of word embeddings can not lead to a fixed-length vector representation
1734 since sentences in different length contain different number of word pairs , concatenation of word embeddings can not lead to a fixed-length vector representation .
1735 additionally
1736 additionally ,
1737 additionally , when
1738 additionally , when we
1739 additionally , when we are
1740 additionally , when we are training
1741 additionally , when we are training with
1742 additionally , when we are training with discrete
1743 additionally , when we are training with discrete features
1744 additionally , when we are training with discrete features ,
1745 additionally , when we are training with discrete features , we
1746 additionally , when we are training with discrete features , we follow
1747 additionally , when we are training with discrete features , we follow the
1748 additionally , when we are training with discrete features , we follow the previous
1749 additionally , when we are training with discrete features , we follow the previous methods
1750 additionally , when we are training with discrete features , we follow the previous methods and
1751 additionally , when we are training with discrete features , we follow the previous methods and remove
1752 additionally , when we are training with discrete features , we follow the previous methods and remove the
1753 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete
1754 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features
1755 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose
1756 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence
1757 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number
1758 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower
1759 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than
1760 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a
1761 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off
1762 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off ,
1763 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which
1764 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is
1765 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3
1766 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 ,
1767 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5
1768 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 ,
1769 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and
1770 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5
1771 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for
1772 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3
1773 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3 ,
1774 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3 , words
1775 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3 , words pairs
1776 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3 , words pairs and
1777 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3 , words pairs and production
1778 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3 , words pairs and production rules
1779 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3 , words pairs and production rules respectively
1780 additionally , when we are training with discrete features , we follow the previous methods and remove the discrete features whose occurrence number lower than a cut-off , which is 3 , 5 , and 5 for first-last-first3 , words pairs and production rules respectively .
1781 our
1782 our final
1783 our final system
1784 our final system obtain
1785 our final system obtain the
1786 our final system obtain the best
1787 our final system obtain the best performance
1788 our final system obtain the best performance for
1789 our final system obtain the best performance for contingency
1790 our final system obtain the best performance for contingency ,
1791 our final system obtain the best performance for contingency , expansion
1792 our final system obtain the best performance for contingency , expansion and
1793 our final system obtain the best performance for contingency , expansion and temporal
1794 our final system obtain the best performance for contingency , expansion and temporal relations
1795 our final system obtain the best performance for contingency , expansion and temporal relations ,
1796 our final system obtain the best performance for contingency , expansion and temporal relations , while
1797 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps
1798 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive
1799 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in
1800 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in comparison
1801 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in comparison relation
1802 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in comparison relation compared
1803 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in comparison relation compared to
1804 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in comparison relation compared to the
1805 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in comparison relation compared to the state-of-the-art
1806 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in comparison relation compared to the state-of-the-art methods
1807 our final system obtain the best performance for contingency , expansion and temporal relations , while keeps competitive in comparison relation compared to the state-of-the-art methods .
1808 in
1809 in the
1810 in the future
1811 in the future ,
1812 in the future , we
1813 in the future , we will
1814 in the future , we will devote
1815 in the future , we will devote ourselves
1816 in the future , we will devote ourselves to
1817 in the future , we will devote ourselves to encode
1818 in the future , we will devote ourselves to encode more
1819 in the future , we will devote ourselves to encode more linguistically
1820 in the future , we will devote ourselves to encode more linguistically features
1821 in the future , we will devote ourselves to encode more linguistically features into
1822 in the future , we will devote ourselves to encode more linguistically features into distributed
1823 in the future , we will devote ourselves to encode more linguistically features into distributed representation
1824 in the future , we will devote ourselves to encode more linguistically features into distributed representation and
1825 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore
1826 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more
1827 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective
1828 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method
1829 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to
1830 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn
1831 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn long-distance
1832 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn long-distance dependency
1833 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn long-distance dependency of
1834 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn long-distance dependency of the
1835 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn long-distance dependency of the syntax
1836 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn long-distance dependency of the syntax and
1837 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn long-distance dependency of the syntax and semantics
1838 in the future , we will devote ourselves to encode more linguistically features into distributed representation and explore more effective method to learn long-distance dependency of the syntax and semantics .
1839 we
1840 we add
1841 we add this
1842 we add this ratio
1843 we add this ratio in
1844 we add this ratio in a
1845 we add this ratio in a logarithm
1846 we add this ratio in a logarithm function
1847 we add this ratio in a logarithm function as
1848 we add this ratio in a logarithm function as the
1849 we add this ratio in a logarithm function as the weight
1850 we add this ratio in a logarithm function as the weight of
1851 we add this ratio in a logarithm function as the weight of the
1852 we add this ratio in a logarithm function as the weight of the matching
1853 we add this ratio in a logarithm function as the weight of the matching word
1854 we add this ratio in a logarithm function as the weight of the matching word .
1855 performances
1856 performances of
1857 performances of meteor
1858 performances of meteor based
1859 performances of meteor based on
1860 performances of meteor based on different
1861 performances of meteor based on different score
1862 performances of meteor based on different score selections
1863 performances of meteor based on different score selections on
1864 performances of meteor based on different score selections on fluency
1865 performances of meteor based on different score selections on fluency and
1866 performances of meteor based on different score selections on fluency and accuracy
1867 performances of meteor based on different score selections on fluency and accuracy with
1868 performances of meteor based on different score selections on fluency and accuracy with and
1869 performances of meteor based on different score selections on fluency and accuracy with and without
1870 performances of meteor based on different score selections on fluency and accuracy with and without synonym
1871 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching
1872 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at
1873 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level
1874 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in
1875 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms
1876 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of
1877 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of pearson
1878 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of pearson correlation
1879 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of pearson correlation are
1880 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of pearson correlation are shown
1881 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of pearson correlation are shown in
1882 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of pearson correlation are shown in tables
1883 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of pearson correlation are shown in tables 1-2
1884 performances of meteor based on different score selections on fluency and accuracy with and without synonym matching at system-level in terms of pearson correlation are shown in tables 1-2 .
1885 as
1886 as the
1887 as the architecture
1888 as the architecture is
1889 as the architecture is given
1890 as the architecture is given in
1891 as the architecture is given in the
1892 as the architecture is given in the figure
1893 as the architecture is given in the figure 2
1894 as the architecture is given in the figure 2 ,
1895 as the architecture is given in the figure 2 , scnn
1896 as the architecture is given in the figure 2 , scnn model
1897 as the architecture is given in the figure 2 , scnn model consists
1898 as the architecture is given in the figure 2 , scnn model consists of
1899 as the architecture is given in the figure 2 , scnn model consists of two
1900 as the architecture is given in the figure 2 , scnn model consists of two convolutional
1901 as the architecture is given in the figure 2 , scnn model consists of two convolutional layers
1902 as the architecture is given in the figure 2 , scnn model consists of two convolutional layers to
1903 as the architecture is given in the figure 2 , scnn model consists of two convolutional layers to do
1904 as the architecture is given in the figure 2 , scnn model consists of two convolutional layers to do the
1905 as the architecture is given in the figure 2 , scnn model consists of two convolutional layers to do the composition
1906 as the architecture is given in the figure 2 , scnn model consists of two convolutional layers to do the composition .
1907 the
1908 the our
1909 the our scnn
1910 the our scnn model
1911 the our scnn model for
1912 the our scnn model for learning
1913 the our scnn model for learning sentence
1914 the our scnn model for learning sentence representation
1915 the our scnn model for learning sentence representation .
1916 through
1917 through the
1918 through the pooling
1919 through the pooling layer
1920 through the pooling layer ,
1921 through the pooling layer , the
1922 through the pooling layer , the sentence
1923 through the pooling layer , the sentence vectors
1924 through the pooling layer , the sentence vectors transform
1925 through the pooling layer , the sentence vectors transform into
1926 through the pooling layer , the sentence vectors transform into a
1927 through the pooling layer , the sentence vectors transform into a document
1928 through the pooling layer , the sentence vectors transform into a document vector
1929 through the pooling layer , the sentence vectors transform into a document vector by
1930 through the pooling layer , the sentence vectors transform into a document vector by a
1931 through the pooling layer , the sentence vectors transform into a document vector by a weighted-average
1932 through the pooling layer , the sentence vectors transform into a document vector by a weighted-average operation
1933 through the pooling layer , the sentence vectors transform into a document vector by a weighted-average operation .
1934 we
1935 we do
1936 we do two
1937 we do two comparison
1938 we do two comparison experiments
1939 we do two comparison experiments to
1940 we do two comparison experiments to show
1941 we do two comparison experiments to show the
1942 we do two comparison experiments to show the effectiveness
1943 we do two comparison experiments to show the effectiveness our
1944 we do two comparison experiments to show the effectiveness our model
1945 we do two comparison experiments to show the effectiveness our model .
1946 the
1947 the basic
1948 the basic cnn
1949 the basic cnn is
1950 the basic cnn is the
1951 the basic cnn is the basic
1952 the basic cnn is the basic convolutional
1953 the basic cnn is the basic convolutional neural
1954 the basic cnn is the basic convolutional neural network
1955 the basic cnn is the basic convolutional neural network model
1956 the basic cnn is the basic convolutional neural network model which
1957 the basic cnn is the basic convolutional neural network model which sentences
1958 the basic cnn is the basic convolutional neural network model which sentences are
1959 the basic cnn is the basic convolutional neural network model which sentences are representing
1960 the basic cnn is the basic convolutional neural network model which sentences are representing through
1961 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional
1962 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer
1963 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and
1964 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform
1965 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into
1966 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into a
1967 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into a document
1968 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into a document vector
1969 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into a document vector by
1970 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into a document vector by the
1971 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into a document vector by the average
1972 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into a document vector by the average operation
1973 the basic cnn is the basic convolutional neural network model which sentences are representing through convolutional layer and transform into a document vector by the average operation .
1974 the
1975 the parameters
1976 the parameters of
1977 the parameters of swnnmodel
1978 the parameters of swnnmodel used
1979 the parameters of swnnmodel used in
1980 the parameters of swnnmodel used in the
1981 the parameters of swnnmodel used in the deceptive
1982 the parameters of swnnmodel used in the deceptive opinion
1983 the parameters of swnnmodel used in the deceptive opinion spam
1984 the parameters of swnnmodel used in the deceptive opinion spam detection
1985 the parameters of swnnmodel used in the deceptive opinion spam detection experiment
1986 the parameters of swnnmodel used in the deceptive opinion spam detection experiment is
1987 the parameters of swnnmodel used in the deceptive opinion spam detection experiment is listing
1988 the parameters of swnnmodel used in the deceptive opinion spam detection experiment is listing followed
1989 the parameters of swnnmodel used in the deceptive opinion spam detection experiment is listing followed .
1990 this
1991 this phenomenon
1992 this phenomenon attracted
1993 this phenomenon attracted researchers
1994 this phenomenon attracted researchers attention
1995 this phenomenon attracted researchers attention .
1996 sentences
1997 sentences play
1998 sentences play different
1999 sentences play different important
2000 sentences play different important role
2001 sentences play different important role in
2002 sentences play different important role in the
2003 sentences play different important role in the document
2004 sentences play different important role in the document .
2005 we
2006 we extend
2007 we extend it
2008 we extend it based
2009 we extend it based tree
2010 we extend it based tree structure
2011 we extend it based tree structure to
2012 we extend it based tree structure to recursive
2013 we extend it based tree structure to recursive neural
2014 we extend it based tree structure to recursive neural network
2015 we extend it based tree structure to recursive neural network to
2016 we extend it based tree structure to recursive neural network to capture
2017 we extend it based tree structure to recursive neural network to capture more
2018 we extend it based tree structure to recursive neural network to capture more syntactic
2019 we extend it based tree structure to recursive neural network to capture more syntactic and
2020 we extend it based tree structure to recursive neural network to capture more syntactic and semantic
2021 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information
2022 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information ,
2023 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information , and
2024 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information , and sentiment
2025 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information , and sentiment polarity
2026 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information , and sentiment polarity shifting
2027 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information , and sentiment polarity shifting model
2028 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information , and sentiment polarity shifting model is
2029 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information , and sentiment polarity shifting model is introduced
2030 we extend it based tree structure to recursive neural network to capture more syntactic and semantic information , and sentiment polarity shifting model is introduced .
2031 since
2032 since the
2033 since the professional
2034 since the professional technical
2035 since the professional technical literature
2036 since the professional technical literature include
2037 since the professional technical literature include amounts
2038 since the professional technical literature include amounts of
2039 since the professional technical literature include amounts of complex
2040 since the professional technical literature include amounts of complex noun
2041 since the professional technical literature include amounts of complex noun phrases
2042 since the professional technical literature include amounts of complex noun phrases ,
2043 since the professional technical literature include amounts of complex noun phrases , identifying
2044 since the professional technical literature include amounts of complex noun phrases , identifying those
2045 since the professional technical literature include amounts of complex noun phrases , identifying those phrases
2046 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has
2047 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an
2048 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important
2049 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical
2050 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical value
2051 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical value for
2052 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical value for such
2053 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical value for such tasks
2054 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical value for such tasks as
2055 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical value for such tasks as machine
2056 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical value for such tasks as machine translation
2057 since the professional technical literature include amounts of complex noun phrases , identifying those phrases has an important practical value for such tasks as machine translation .
2058 as
2059 as the
2060 as the development
2061 as the development of
2062 as the development of digital
2063 as the development of digital libraries
2064 as the development of digital libraries and
2065 as the development of digital libraries and publication
2066 as the development of digital libraries and publication ,
2067 as the development of digital libraries and publication , there
2068 as the development of digital libraries and publication , there is
2069 as the development of digital libraries and publication , there is a
2070 as the development of digital libraries and publication , there is a need
2071 as the development of digital libraries and publication , there is a need to
2072 as the development of digital libraries and publication , there is a need to assemble
2073 as the development of digital libraries and publication , there is a need to assemble new
2074 as the development of digital libraries and publication , there is a need to assemble new books
2075 as the development of digital libraries and publication , there is a need to assemble new books or
2076 as the development of digital libraries and publication , there is a need to assemble new books or resources
2077 as the development of digital libraries and publication , there is a need to assemble new books or resources by
2078 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking
2079 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage
2080 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of
2081 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books
2082 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which
2083 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having
2084 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having been
2085 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having been published
2086 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having been published and
2087 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having been published and stored
2088 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having been published and stored in
2089 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having been published and stored in digital
2090 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having been published and stored in digital libraries
2091 as the development of digital libraries and publication , there is a need to assemble new books or resources by taking advantage of books which having been published and stored in digital libraries .
2092 so
2093 so an
2094 so an automatic
2095 so an automatic keyword
2096 so an automatic keyword recommendation
2097 so an automatic keyword recommendation mechanism
2098 so an automatic keyword recommendation mechanism is
2099 so an automatic keyword recommendation mechanism is needed
2100 so an automatic keyword recommendation mechanism is needed to
2101 so an automatic keyword recommendation mechanism is needed to faster
2102 so an automatic keyword recommendation mechanism is needed to faster the
2103 so an automatic keyword recommendation mechanism is needed to faster the process
2104 so an automatic keyword recommendation mechanism is needed to faster the process of
2105 so an automatic keyword recommendation mechanism is needed to faster the process of making
2106 so an automatic keyword recommendation mechanism is needed to faster the process of making items
2107 so an automatic keyword recommendation mechanism is needed to faster the process of making items of
2108 so an automatic keyword recommendation mechanism is needed to faster the process of making items of books
2109 so an automatic keyword recommendation mechanism is needed to faster the process of making items of books .
2110 we
2111 we did
2112 we did not
2113 we did not use
2114 we did not use the
2115 we did not use the user
2116 we did not use the user study
2117 we did not use the user study valuation
2118 we did not use the user study valuation method
2119 we did not use the user study valuation method for
2120 we did not use the user study valuation method for that
2121 we did not use the user study valuation method for that we
2122 we did not use the user study valuation method for that we have
2123 we did not use the user study valuation method for that we have enough
2124 we did not use the user study valuation method for that we have enough annotated
2125 we did not use the user study valuation method for that we have enough annotated items
2126 we did not use the user study valuation method for that we have enough annotated items to
2127 we did not use the user study valuation method for that we have enough annotated items to test
2128 we did not use the user study valuation method for that we have enough annotated items to test and
2129 we did not use the user study valuation method for that we have enough annotated items to test and the
2130 we did not use the user study valuation method for that we have enough annotated items to test and the annotated
2131 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items
2132 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were
2133 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated
2134 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by
2135 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert
2136 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors
2137 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who
2138 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have
2139 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough
2140 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority
2141 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in
2142 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging
2143 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work
2144 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work ,
2145 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work , and
2146 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work , and it
2147 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work , and it also
2148 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work , and it also saves
2149 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work , and it also saves lots
2150 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work , and it also saves lots of
2151 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work , and it also saves lots of time
2152 we did not use the user study valuation method for that we have enough annotated items to test and the annotated items were annotated by expert editors who have enough authority in tagging work , and it also saves lots of time .
2153 but
2154 but it
2155 but it does
2156 but it does not
2157 but it does not means
2158 but it does not means that
2159 but it does not means that we
2160 but it does not means that we would
2161 but it does not means that we would abandon
2162 but it does not means that we would abandon the
2163 but it does not means that we would abandon the extraction
2164 but it does not means that we would abandon the extraction method
2165 but it does not means that we would abandon the extraction method because
2166 but it does not means that we would abandon the extraction method because there
2167 but it does not means that we would abandon the extraction method because there are
2168 but it does not means that we would abandon the extraction method because there are cases
2169 but it does not means that we would abandon the extraction method because there are cases that
2170 but it does not means that we would abandon the extraction method because there are cases that the
2171 but it does not means that we would abandon the extraction method because there are cases that the coming
2172 but it does not means that we would abandon the extraction method because there are cases that the coming new
2173 but it does not means that we would abandon the extraction method because there are cases that the coming new item
2174 but it does not means that we would abandon the extraction method because there are cases that the coming new item is
2175 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite
2176 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different
2177 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from
2178 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the
2179 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training
2180 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set
2181 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and
2182 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the
2183 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended
2184 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords
2185 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from
2186 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical
2187 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information
2188 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may
2189 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not
2190 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not cover
2191 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not cover the
2192 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not cover the main
2193 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not cover the main idea
2194 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not cover the main idea of
2195 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not cover the main idea of the
2196 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not cover the main idea of the item
2197 but it does not means that we would abandon the extraction method because there are cases that the coming new item is quite different from the training set and the recommended keywords from statistical information may not cover the main idea of the item .
2198 however
2199 however ,
2200 however , traditional
2201 however , traditional ner
2202 however , traditional ner systems
2203 however , traditional ner systems are
2204 however , traditional ner systems are mainly
2205 however , traditional ner systems are mainly based
2206 however , traditional ner systems are mainly based on
2207 however , traditional ner systems are mainly based on complex
2208 however , traditional ner systems are mainly based on complex hand-designed
2209 however , traditional ner systems are mainly based on complex hand-designed features
2210 however , traditional ner systems are mainly based on complex hand-designed features which
2211 however , traditional ner systems are mainly based on complex hand-designed features which are
2212 however , traditional ner systems are mainly based on complex hand-designed features which are derived
2213 however , traditional ner systems are mainly based on complex hand-designed features which are derived from
2214 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various
2215 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic
2216 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses
2217 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses and
2218 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses and maybe
2219 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses and maybe only
2220 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses and maybe only adapted
2221 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses and maybe only adapted to
2222 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses and maybe only adapted to specified
2223 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses and maybe only adapted to specified area
2224 however , traditional ner systems are mainly based on complex hand-designed features which are derived from various linguistic analyses and maybe only adapted to specified area .
2225 as
2226 as the
2227 as the shallow
2228 as the shallow machine
2229 as the shallow machine learning
2230 as the shallow machine learning methods
2231 as the shallow machine learning methods described
2232 as the shallow machine learning methods described above
2233 as the shallow machine learning methods described above have
2234 as the shallow machine learning methods described above have strong
2235 as the shallow machine learning methods described above have strong dependency
2236 as the shallow machine learning methods described above have strong dependency on
2237 as the shallow machine learning methods described above have strong dependency on the
2238 as the shallow machine learning methods described above have strong dependency on the artificial
2239 as the shallow machine learning methods described above have strong dependency on the artificial features
2240 as the shallow machine learning methods described above have strong dependency on the artificial features and
2241 as the shallow machine learning methods described above have strong dependency on the artificial features and are
2242 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard
2243 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to
2244 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent
2245 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the
2246 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex
2247 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models
2248 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models ,
2249 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep
2250 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning
2251 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has
2252 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has been
2253 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has been applied
2254 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has been applied on
2255 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has been applied on ner
2256 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has been applied on ner in
2257 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has been applied on ner in recent
2258 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has been applied on ner in recent years
2259 as the shallow machine learning methods described above have strong dependency on the artificial features and are hard to represent the complex models , deep learning has been applied on ner in recent years .
2260 the
2261 the outputs
2262 the outputs of
2263 the outputs of hidden
2264 the outputs of hidden layer
2265 the outputs of hidden layer and
2266 the outputs of hidden layer and output
2267 the outputs of hidden layer and output layer
2268 the outputs of hidden layer and output layer are
2269 the outputs of hidden layer and output layer are saved
2270 the outputs of hidden layer and output layer are saved and
2271 the outputs of hidden layer and output layer are saved and inputted
2272 the outputs of hidden layer and output layer are saved and inputted into
2273 the outputs of hidden layer and output layer are saved and inputted into the
2274 the outputs of hidden layer and output layer are saved and inputted into the next
2275 the outputs of hidden layer and output layer are saved and inputted into the next node
2276 the outputs of hidden layer and output layer are saved and inputted into the next node .
2277 secondly
2278 secondly ,
2279 secondly , the
2280 secondly , the affective
2281 secondly , the affective features
2282 secondly , the affective features ,
2283 secondly , the affective features , which
2284 secondly , the affective features , which are
2285 secondly , the affective features , which are extracted
2286 secondly , the affective features , which are extracted with
2287 secondly , the affective features , which are extracted with the
2288 secondly , the affective features , which are extracted with the sentiment
2289 secondly , the affective features , which are extracted with the sentiment lexicon
2290 secondly , the affective features , which are extracted with the sentiment lexicon ,
2291 secondly , the affective features , which are extracted with the sentiment lexicon , are
2292 secondly , the affective features , which are extracted with the sentiment lexicon , are merge
2293 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into
2294 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature
2295 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters
2296 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based
2297 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on
2298 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the
2299 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method
2300 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of
2301 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing
2302 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the
2303 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word
2304 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity
2305 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with
2306 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the
2307 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term
2308 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector
2309 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector ,
2310 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and
2311 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then
2312 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these
2313 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature
2314 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters
2315 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are
2316 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are used
2317 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are used construct
2318 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are used construct the
2319 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are used construct the text
2320 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are used construct the text vector
2321 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are used construct the text vector with
2322 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are used construct the text vector with low-dimension
2323 secondly , the affective features , which are extracted with the sentiment lexicon , are merge into feature clusters based on the method of computing the word similarity with the term vector , and then these feature clusters are used construct the text vector with low-dimension .
2324 we
2325 we build
2326 we build sentence
2327 we build sentence weights
2328 we build sentence weights model
2329 we build sentence weights model by
2330 we build sentence weights model by word
2331 we build sentence weights model by word distribution
2332 we build sentence weights model by word distribution which
2333 we build sentence weights model by word distribution which make
2334 we build sentence weights model by word distribution which make the
2335 we build sentence weights model by word distribution which make the calculation
2336 we build sentence weights model by word distribution which make the calculation efficient
2337 we build sentence weights model by word distribution which make the calculation efficient and
2338 we build sentence weights model by word distribution which make the calculation efficient and simple
2339 we build sentence weights model by word distribution which make the calculation efficient and simple .
2340 our
2341 our likelihood
2342 our likelihood estimation
2343 our likelihood estimation of
2344 our likelihood estimation of weights
2345 our likelihood estimation of weights shows
2346 our likelihood estimation of weights shows as
2347 our likelihood estimation of weights shows as formula
2348 our likelihood estimation of weights shows as formula 2
2349 our likelihood estimation of weights shows as formula 2 .
2350 and
2351 and the
2352 and the meaning
2353 and the meaning of
2354 and the meaning of the
2355 and the meaning of the translation
2356 and the meaning of the translation of
2357 and the meaning of the translation of our
2358 and the meaning of the translation of our system
2359 and the meaning of the translation of our system is
2360 and the meaning of the translation of our system is the
2361 and the meaning of the translation of our system is the same
2362 and the meaning of the translation of our system is the same to
2363 and the meaning of the translation of our system is the same to that
2364 and the meaning of the translation of our system is the same to that of
2365 and the meaning of the translation of our system is the same to that of the
2366 and the meaning of the translation of our system is the same to that of the news
2367 and the meaning of the translation of our system is the same to that of the news domain
2368 and the meaning of the translation of our system is the same to that of the news domain translation
2369 and the meaning of the translation of our system is the same to that of the news domain translation system
2370 and the meaning of the translation of our system is the same to that of the news domain translation system .
2371 in
2372 in the
2373 in the sentence
2374 in the sentence 1
2375 in the sentence 1 ,
2376 in the sentence 1 , the
2377 in the sentence 1 , the meaning
2378 in the sentence 1 , the meaning of
2379 in the sentence 1 , the meaning of source
2380 in the sentence 1 , the meaning of source sentence
2381 in the sentence 1 , the meaning of source sentence is
2382 in the sentence 1 , the meaning of source sentence is that
2383 in the sentence 1 , the meaning of source sentence is that the
2384 in the sentence 1 , the meaning of source sentence is that the economic
2385 in the sentence 1 , the meaning of source sentence is that the economic growth
2386 in the sentence 1 , the meaning of source sentence is that the economic growth may
2387 in the sentence 1 , the meaning of source sentence is that the economic growth may slow
2388 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down
2389 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down ,
2390 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so
2391 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so ,
2392 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the
2393 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation
2394 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of
2395 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of baseline
2396 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of baseline is
2397 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of baseline is wrong
2398 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of baseline is wrong and
2399 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of baseline is wrong and ours
2400 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of baseline is wrong and ours is
2401 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of baseline is wrong and ours is accurately
2402 in the sentence 1 , the meaning of source sentence is that the economic growth may slow down , so , the translation of baseline is wrong and ours is accurately .
2403 in
2404 in order
2405 in order to
2406 in order to better
2407 in order to better describe
2408 in order to better describe the
2409 in order to better describe the corresponding
2410 in order to better describe the corresponding relationship
2411 in order to better describe the corresponding relationship between
2412 in order to better describe the corresponding relationship between the
2413 in order to better describe the corresponding relationship between the source
2414 in order to better describe the corresponding relationship between the source domain
2415 in order to better describe the corresponding relationship between the source domain and
2416 in order to better describe the corresponding relationship between the source domain and the
2417 in order to better describe the corresponding relationship between the source domain and the attributes
2418 in order to better describe the corresponding relationship between the source domain and the attributes ,
2419 in order to better describe the corresponding relationship between the source domain and the attributes , the
2420 in order to better describe the corresponding relationship between the source domain and the attributes , the paper
2421 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists
2422 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all
2423 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the
2424 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source
2425 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source domains
2426 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source domains ,
2427 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source domains , mapped
2428 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source domains , mapped more
2429 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source domains , mapped more than
2430 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source domains , mapped more than 10
2431 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source domains , mapped more than 10 times
2432 in order to better describe the corresponding relationship between the source domain and the attributes , the paper lists all the source domains , mapped more than 10 times .
2433 meanwhile
2434 meanwhile ,
2435 meanwhile , the
2436 meanwhile , the ranking
2437 meanwhile , the ranking of
2438 meanwhile , the ranking of the
2439 meanwhile , the ranking of the 5
2440 meanwhile , the ranking of the 5 categories
2441 meanwhile , the ranking of the 5 categories has
2442 meanwhile , the ranking of the 5 categories has also
2443 meanwhile , the ranking of the 5 categories has also changed
2444 meanwhile , the ranking of the 5 categories has also changed ,
2445 meanwhile , the ranking of the 5 categories has also changed , the
2446 meanwhile , the ranking of the 5 categories has also changed , the source
2447 meanwhile , the ranking of the 5 categories has also changed , the source domain
2448 meanwhile , the ranking of the 5 categories has also changed , the source domain of
2449 meanwhile , the ranking of the 5 categories has also changed , the source domain of the
2450 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second
2451 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place
2452 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_
2453 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people
2454 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply
2455 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined
2456 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined ,
2457 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing
2458 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only
2459 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14
2460 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words
2461 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and
2462 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked
2463 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third
2464 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third ,
2465 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while
2466 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the
2467 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number
2468 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of
2469 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of d_
2470 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of d_ abstract
2471 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of d_ abstract things
2472 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of d_ abstract things arranged
2473 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of d_ abstract things arranged in
2474 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of d_ abstract things arranged in second
2475 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of d_ abstract things arranged in second place
2476 meanwhile , the ranking of the 5 categories has also changed , the source domain of the second place a_ people sharply declined , containing only 14 words and ranked third , while the number of d_ abstract things arranged in second place .
2477 im
2478 im reminded
2479 im reminded of
2480 im reminded of a
2481 im reminded of a counselor
2482 im reminded of a counselor who
2483 im reminded of a counselor who would
2484 im reminded of a counselor who would often
2485 im reminded of a counselor who would often state
2486 im reminded of a counselor who would often state no
2487 im reminded of a counselor who would often state no one
2488 im reminded of a counselor who would often state no one can
2489 im reminded of a counselor who would often state no one can drive
2490 im reminded of a counselor who would often state no one can drive your
2491 im reminded of a counselor who would often state no one can drive your car
2492 im reminded of a counselor who would often state no one can drive your car unless
2493 im reminded of a counselor who would often state no one can drive your car unless you
2494 im reminded of a counselor who would often state no one can drive your car unless you give
2495 im reminded of a counselor who would often state no one can drive your car unless you give them
2496 im reminded of a counselor who would often state no one can drive your car unless you give them the
2497 im reminded of a counselor who would often state no one can drive your car unless you give them the keys
2498 im reminded of a counselor who would often state no one can drive your car unless you give them the keys .
2499 you
2500 you can
2501 you can not
2502 you can not control
2503 you can not control others
2504 you can not control others actions
2505 you can not control others actions ,
2506 you can not control others actions , but
2507 you can not control others actions , but you
2508 you can not control others actions , but you can
2509 you can not control others actions , but you can be
2510 you can not control others actions , but you can be responsible
2511 you can not control others actions , but you can be responsible for
2512 you can not control others actions , but you can be responsible for your
2513 you can not control others actions , but you can be responsible for your reactions
2514 you can not control others actions , but you can be responsible for your reactions .
2515 for
2516 for every
2517 for every candidate
2518 for every candidate answer
2519 for every candidate answer of
2520 for every candidate answer of the
2521 for every candidate answer of the question
2522 for every candidate answer of the question we
2523 for every candidate answer of the question we are
2524 for every candidate answer of the question we are considering
2525 for every candidate answer of the question we are considering ,
2526 for every candidate answer of the question we are considering , we
2527 for every candidate answer of the question we are considering , we score
2528 for every candidate answer of the question we are considering , we score it
2529 for every candidate answer of the question we are considering , we score it base
2530 for every candidate answer of the question we are considering , we score it base on
2531 for every candidate answer of the question we are considering , we score it base on its
2532 for every candidate answer of the question we are considering , we score it base on its content
2533 for every candidate answer of the question we are considering , we score it base on its content and
2534 for every candidate answer of the question we are considering , we score it base on its content and structure
2535 for every candidate answer of the question we are considering , we score it base on its content and structure similarity
2536 for every candidate answer of the question we are considering , we score it base on its content and structure similarity with
2537 for every candidate answer of the question we are considering , we score it base on its content and structure similarity with the
2538 for every candidate answer of the question we are considering , we score it base on its content and structure similarity with the support
2539 for every candidate answer of the question we are considering , we score it base on its content and structure similarity with the support sets
2540 for every candidate answer of the question we are considering , we score it base on its content and structure similarity with the support sets .
2541 notice
2542 notice that
2543 notice that ,
2544 notice that , a
2545 notice that , a smaller
2546 notice that , a smaller distance
2547 notice that , a smaller distance means
2548 notice that , a smaller distance means the
2549 notice that , a smaller distance means the feature
2550 notice that , a smaller distance means the feature of
2551 notice that , a smaller distance means the feature of the
2552 notice that , a smaller distance means the feature of the candidate
2553 notice that , a smaller distance means the feature of the candidate answer
2554 notice that , a smaller distance means the feature of the candidate answer is
2555 notice that , a smaller distance means the feature of the candidate answer is more
2556 notice that , a smaller distance means the feature of the candidate answer is more close
2557 notice that , a smaller distance means the feature of the candidate answer is more close with
2558 notice that , a smaller distance means the feature of the candidate answer is more close with the
2559 notice that , a smaller distance means the feature of the candidate answer is more close with the representative
2560 notice that , a smaller distance means the feature of the candidate answer is more close with the representative .
2561 so
2562 so far
2563 so far ,
2564 so far , we
2565 so far , we made
2566 so far , we made some
2567 so far , we made some rules
2568 so far , we made some rules solve
2569 so far , we made some rules solve this
2570 so far , we made some rules solve this reasoning
2571 so far , we made some rules solve this reasoning problem
2572 so far , we made some rules solve this reasoning problem .
2573 the
2574 the evaluation
2575 the evaluation of
2576 the evaluation of the
2577 the evaluation of the system
2578 the evaluation of the system is
2579 the evaluation of the system is performed
2580 the evaluation of the system is performed by
2581 the evaluation of the system is performed by applying
2582 the evaluation of the system is performed by applying it
2583 the evaluation of the system is performed by applying it on
2584 the evaluation of the system is performed by applying it on recognizing
2585 the evaluation of the system is performed by applying it on recognizing entailment
2586 the evaluation of the system is performed by applying it on recognizing entailment relation
2587 the evaluation of the system is performed by applying it on recognizing entailment relation of
2588 the evaluation of the system is performed by applying it on recognizing entailment relation of test
2589 the evaluation of the system is performed by applying it on recognizing entailment relation of test text
2590 the evaluation of the system is performed by applying it on recognizing entailment relation of test text pairs
2591 the evaluation of the system is performed by applying it on recognizing entailment relation of test text pairs .
2592 it
2593 it seeks
2594 it seeks to
2595 it seeks to generalize
2596 it seeks to generalize a
2597 it seeks to generalize a model
2598 it seeks to generalize a model ,
2599 it seeks to generalize a model , which
2600 it seeks to generalize a model , which is
2601 it seeks to generalize a model , which is trained
2602 it seeks to generalize a model , which is trained on
2603 it seeks to generalize a model , which is trained on a
2604 it seeks to generalize a model , which is trained on a source
2605 it seeks to generalize a model , which is trained on a source domain
2606 it seeks to generalize a model , which is trained on a source domain and
2607 it seeks to generalize a model , which is trained on a source domain and using
2608 it seeks to generalize a model , which is trained on a source domain and using it
2609 it seeks to generalize a model , which is trained on a source domain and using it to
2610 it seeks to generalize a model , which is trained on a source domain and using it to label
2611 it seeks to generalize a model , which is trained on a source domain and using it to label samples
2612 it seeks to generalize a model , which is trained on a source domain and using it to label samples in
2613 it seeks to generalize a model , which is trained on a source domain and using it to label samples in the
2614 it seeks to generalize a model , which is trained on a source domain and using it to label samples in the target
2615 it seeks to generalize a model , which is trained on a source domain and using it to label samples in the target domain
2616 it seeks to generalize a model , which is trained on a source domain and using it to label samples in the target domain .
2617 moreover
2618 moreover ,
2619 moreover , ss-lda
2620 moreover , ss-lda bases
2621 moreover , ss-lda bases on
2622 moreover , ss-lda bases on sentence
2623 moreover , ss-lda bases on sentence level
2624 moreover , ss-lda bases on sentence level while
2625 moreover , ss-lda bases on sentence level while jst
2626 moreover , ss-lda bases on sentence level while jst depends
2627 moreover , ss-lda bases on sentence level while jst depends on
2628 moreover , ss-lda bases on sentence level while jst depends on document
2629 moreover , ss-lda bases on sentence level while jst depends on document level
2630 moreover , ss-lda bases on sentence level while jst depends on document level .
2631 furthermore
2632 furthermore ,
2633 furthermore , prior
2634 furthermore , prior knowledge
2635 furthermore , prior knowledge that
2636 furthermore , prior knowledge that obtained
2637 furthermore , prior knowledge that obtained from
2638 furthermore , prior knowledge that obtained from sentiment
2639 furthermore , prior knowledge that obtained from sentiment lexicons
2640 furthermore , prior knowledge that obtained from sentiment lexicons is
2641 furthermore , prior knowledge that obtained from sentiment lexicons is utilized
2642 furthermore , prior knowledge that obtained from sentiment lexicons is utilized during
2643 furthermore , prior knowledge that obtained from sentiment lexicons is utilized during the
2644 furthermore , prior knowledge that obtained from sentiment lexicons is utilized during the initialization
2645 furthermore , prior knowledge that obtained from sentiment lexicons is utilized during the initialization for
2646 furthermore , prior knowledge that obtained from sentiment lexicons is utilized during the initialization for gibbs
2647 furthermore , prior knowledge that obtained from sentiment lexicons is utilized during the initialization for gibbs sampling
2648 furthermore , prior knowledge that obtained from sentiment lexicons is utilized during the initialization for gibbs sampling .
2649 my
2650 my friends
2651 my friends were
2652 my friends were disappointed
2653 my friends were disappointed to
2654 my friends were disappointed to the
2655 my friends were disappointed to the sound
2656 my friends were disappointed to the sound .
2657 besides
2658 besides ,
2659 besides , in
2660 besides , in order
2661 besides , in order to
2662 besides , in order to improving
2663 besides , in order to improving the
2664 besides , in order to improving the sentiment
2665 besides , in order to improving the sentiment detection
2666 besides , in order to improving the sentiment detection ,
2667 besides , in order to improving the sentiment detection , prior
2668 besides , in order to improving the sentiment detection , prior knowledge
2669 besides , in order to improving the sentiment detection , prior knowledge is
2670 besides , in order to improving the sentiment detection , prior knowledge is incorporated
2671 besides , in order to improving the sentiment detection , prior knowledge is incorporated during
2672 besides , in order to improving the sentiment detection , prior knowledge is incorporated during the
2673 besides , in order to improving the sentiment detection , prior knowledge is incorporated during the initialization
2674 besides , in order to improving the sentiment detection , prior knowledge is incorporated during the initialization of
2675 besides , in order to improving the sentiment detection , prior knowledge is incorporated during the initialization of gibbs
2676 besides , in order to improving the sentiment detection , prior knowledge is incorporated during the initialization of gibbs sampling
2677 besides , in order to improving the sentiment detection , prior knowledge is incorporated during the initialization of gibbs sampling .
2678 in
2679 in summary
2680 in summary ,
2681 in summary , filtering
2682 in summary , filtering the
2683 in summary , filtering the sentences
2684 in summary , filtering the sentences whose
2685 in summary , filtering the sentences whose polarities
2686 in summary , filtering the sentences whose polarities opposite
2687 in summary , filtering the sentences whose polarities opposite to
2688 in summary , filtering the sentences whose polarities opposite to the
2689 in summary , filtering the sentences whose polarities opposite to the overall
2690 in summary , filtering the sentences whose polarities opposite to the overall orientation
2691 in summary , filtering the sentences whose polarities opposite to the overall orientation is
2692 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant
2693 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant for
2694 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant for constructing
2695 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant for constructing a
2696 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant for constructing a high
2697 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant for constructing a high quality
2698 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant for constructing a high quality training
2699 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant for constructing a high quality training set
2700 in summary , filtering the sentences whose polarities opposite to the overall orientation is significant for constructing a high quality training set .
2701 that
2702 that is
2703 that is to
2704 that is to say
2705 that is to say ,
2706 that is to say , the
2707 that is to say , the classification
2708 that is to say , the classification model
2709 that is to say , the classification model might
2710 that is to say , the classification model might not
2711 that is to say , the classification model might not accurate
2712 that is to say , the classification model might not accurate when
2713 that is to say , the classification model might not accurate when applying
2714 that is to say , the classification model might not accurate when applying these
2715 that is to say , the classification model might not accurate when applying these samples
2716 that is to say , the classification model might not accurate when applying these samples for
2717 that is to say , the classification model might not accurate when applying these samples for training
2718 that is to say , the classification model might not accurate when applying these samples for training directly
2719 that is to say , the classification model might not accurate when applying these samples for training directly .
2720 by
2721 by ss-lda
2722 by ss-lda filtering
2723 by ss-lda filtering ,
2724 by ss-lda filtering , we
2725 by ss-lda filtering , we move
2726 by ss-lda filtering , we move the
2727 by ss-lda filtering , we move the sentences
2728 by ss-lda filtering , we move the sentences whose
2729 by ss-lda filtering , we move the sentences whose sentimental
2730 by ss-lda filtering , we move the sentences whose sentimental polarity
2731 by ss-lda filtering , we move the sentences whose sentimental polarity strongly
2732 by ss-lda filtering , we move the sentences whose sentimental polarity strongly opposites
2733 by ss-lda filtering , we move the sentences whose sentimental polarity strongly opposites the
2734 by ss-lda filtering , we move the sentences whose sentimental polarity strongly opposites the overall
2735 by ss-lda filtering , we move the sentences whose sentimental polarity strongly opposites the overall orientation
2736 by ss-lda filtering , we move the sentences whose sentimental polarity strongly opposites the overall orientation .
2737 this
2738 this article
2739 this article firstly
2740 this article firstly defined
2741 this article firstly defined the
2742 this article firstly defined the phenomenon
2743 this article firstly defined the phenomenon and
2744 this article firstly defined the phenomenon and explained
2745 this article firstly defined the phenomenon and explained related
2746 this article firstly defined the phenomenon and explained related concepts
2747 this article firstly defined the phenomenon and explained related concepts ,
2748 this article firstly defined the phenomenon and explained related concepts , than
2749 this article firstly defined the phenomenon and explained related concepts , than demonstrated
2750 this article firstly defined the phenomenon and explained related concepts , than demonstrated this
2751 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon
2752 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by
2753 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting
2754 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling
2755 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style
2756 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of
2757 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example
2758 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word
2759 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word ,
2760 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw
2761 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus
2762 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus statistics
2763 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus statistics and
2764 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus statistics and whole
2765 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus statistics and whole text
2766 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus statistics and whole text annotation
2767 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus statistics and whole text annotation statistics
2768 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus statistics and whole text annotation statistics etc
2769 this article firstly defined the phenomenon and explained related concepts , than demonstrated this phenomenon by exhausting spelling style of example word , raw corpus statistics and whole text annotation statistics etc .
2770 finally
2771 finally ,
2772 finally , we
2773 finally , we promote
2774 finally , we promote serious
2775 finally , we promote serious of
2776 finally , we promote serious of methods
2777 finally , we promote serious of methods to
2778 finally , we promote serious of methods to solve
2779 finally , we promote serious of methods to solve msdp
2780 finally , we promote serious of methods to solve msdp ,
2781 finally , we promote serious of methods to solve msdp , it
2782 finally , we promote serious of methods to solve msdp , it includes
2783 finally , we promote serious of methods to solve msdp , it includes popularization
2784 finally , we promote serious of methods to solve msdp , it includes popularization of
2785 finally , we promote serious of methods to solve msdp , it includes popularization of standardized
2786 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input
2787 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by
2788 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise
2789 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness
2790 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of
2791 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the
2792 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user
2793 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user ,
2794 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using
2795 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent
2796 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime
2797 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to
2798 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid
2799 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input
2800 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake
2801 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake ,
2802 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using
2803 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the
2804 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling
2805 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error
2806 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction
2807 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools
2808 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools ,
2809 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration
2810 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of
2811 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of raw
2812 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of raw corpus
2813 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of raw corpus based
2814 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of raw corpus based statistical
2815 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of raw corpus based statistical learning
2816 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of raw corpus based statistical learning methods
2817 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of raw corpus based statistical learning methods etc
2818 finally , we promote serious of methods to solve msdp , it includes popularization of standardized input by raise awareness of the user , using intelligent ime to avoid input mistake , using the spelling error correction tools , exploration of raw corpus based statistical learning methods etc .
